# INGESTION DU PROJET LLMBASEDOS (Racine: /home/iluxu/llmbasedos/llmbasedos)

==================================================

Répertoire: ./
  Fichier: .env
  Fichier: .gitignore
  Fichier: Dockerfile
  Fichier: README.md
    --- Début Contenu (ascii) ---
    | # llmbasedos
    | 
    | `llmbasedos` is a system designed to expose local capabilities (files, mail, sync, agents) to various "host" applications (LLM frontends, VS Code plugins, etc.) via the **Model Context Protocol (MCP)**. It serves as a secure and standardized bridge between Large Language Models and your personal data and tools.
    | 
    | Primarily deployed via **Docker**, `llmbasedos` can also be built as a minimal Arch Linux based ISO for dedicated appliances.
    | 
    | ## Core Architecture (Docker Deployment)
    | 
    | The system is composed of several key Python components, typically running within a single Docker container managed by **Supervisord**:
    | 
    | 1.  **Gateway (`llmbasedos_pkg/gateway/`)**:
    |     *   Central MCP router (FastAPI + WebSockets/UNIX Sockets).
    |     *   Handles authentication (licence key from `/etc/llmbasedos/lic.key`, tiers from `/etc/llmbasedos/licence_tiers.yaml`), authorization, and rate limiting.
    |     *   Dynamically discovers backend server capabilities by reading `/run/mcp/*.cap.json` files.
    |     *   Proxies `mcp.llm.chat` to configured LLMs (OpenAI, llama.cpp, etc., defined in `AVAILABLE_LLM_MODELS` in gateway config), applying quotas.
    | 
    | 2.  **MCP Servers (`llmbasedos_pkg/servers/*/`)**:
    |     *   Python daemons, each providing specific MCP capabilities over a UNIX socket.
    |     *   Built using a common `llmbasedos.mcp_server_framework.MCPServer` base class.
    |     *   Each server publishes its `SERVICE_NAME.cap.json` to `/run/mcp/` for discovery by the gateway.
    |     *   **FS Server (`servers/fs/`)**: File system operations (list, read, write, delete, semantic embed/search via SentenceTransformers/FAISS). Path access is confined within a configurable "virtual root" (e.g., `/mnt/user_data` in Docker). FAISS index stored in a persistent volume.
    |     *   **Sync Server (`servers/sync/`)**: Wrapper for `rclone` for file synchronization tasks. Requires `rclone.conf`.
    |     *   **Mail Server (`servers/mail/`)**: IMAP client for email access and iCalendar parsing. Accounts configured in `/etc/llmbasedos/mail_accounts.yaml`.
    |     *   **Agent Server (`servers/agent/`)**: Executes agentic workflows defined in YAML files (from `/etc/llmbasedos/workflows`), potentially interacting with Docker (if Docker-in-Docker setup or socket passthrough) or HTTP services.
    | 
    | 3.  **Shell (`llmbasedos_pkg/shell/`)**:
    |     *   `luca-shell`: An interactive Python REPL (using `prompt_toolkit`) that runs on your **host machine** (or wherever you need a client).
    |     *   Acts as an MCP client, connecting to the gateway's WebSocket endpoint.
    |     *   Translates shell commands (built-in aliases like `ls`, `cat`, or direct MCP calls) to the gateway.
    |     *   Supports command history, basic autocompletion, and LLM chat streaming.
    | 
    | ## Communication Protocol
    | 
    | *   All inter-component communication uses **Model Context Protocol (MCP)**.
    | *   MCP is implemented as JSON-RPC 2.0 messages.
    | *   Transport:
    |     *   External hosts (like `luca-shell`) to Gateway: WebSocket (e.g., `ws://localhost:8000/ws`).
    |     *   Gateway to Backend Servers (within Docker): UNIX domain sockets (e.g., `/run/mcp/fs.sock`) with JSON messages delimited by `\0`.
    | 
    | ## Security Considerations
    | 
    | *   **Path Validation**: FS server operations are restricted by a "virtual root" to prevent arbitrary file system access.
    | *   **Licence & Auth**: Gateway enforces access based on a licence key and configured tiers.
    | *   **Secrets**: API keys (OpenAI, etc.) and email passwords **must be provided via environment variables** (e.g., through an `.env` file with `docker-compose`) and are not part of the image.
    | *   **Docker Volumes**: Sensitive configuration files (`lic.key`, `mail_accounts.yaml`, `rclone.conf`) are mounted as read-only volumes into the container.
    | 
    | ## Deployment (Docker - Recommended)
    | 
    | 1.  **Prerequisites**: Docker and Docker Compose (or `docker compose` CLI v2).
    | 2.  **Clone the repository.**
    | 3.  **Project Structure**: Ensure your Python application code (`gateway/`, `servers/`, `shell/`, `mcp_server_framework.py`, `common_utils.py`) is inside a top-level directory (e.g., `llmbasedos_src/`) within your project root. This `llmbasedos_src/` directory will be treated as the `llmbasedos` Python package inside the Docker image.
    | 4.  **Configuration**:
    |     *   At the project root (next to `docker-compose.yml`):
    |         *   Create/Edit `.env`: Define `OPENAI_API_KEY` and other environment variables (e.g., `LLMBDO_LOG_LEVEL`).
    |         *   Create/Edit `lic.key`: Example: `FREE:youruser:2025-12-31`
    |         *   Create/Edit `mail_accounts.yaml`: For mail server accounts.
    |         *   Create/Edit `gateway/licence_tiers.yaml`: To define licence tiers (if you want to override defaults that might be in `gateway/config.py`).
    |         *   Create `./workflows/` directory and add your agent workflow YAML files.
    |         *   Create `./user_files/` directory and add any files you want the FS server to access.
    |         *   Ensure `supervisord.conf` is present and correctly configured (especially the `directory` for each program).
    | 5.  **Build the Docker Image**:
    |     ```bash
    |     docker compose build
    |     ```
    | 6.  **Run the Services**:
    |     ```bash
    |     docker compose up
    |     ```
    | 7.  **Interact**:
    |     *   The MCP Gateway will be accessible on `ws://localhost:8000/ws` (or the port configured via `LLMBDO_GATEWAY_EXPOSED_PORT` in `.env`).
    |     *   Run `luca-shell` from your host machine (ensure its Python environment has dependencies from `llmbasedos_src/shell/requirements.txt` installed):
    |         ```bash
    |         # From project root, assuming venv is activated
    |         python -m llmbasedos_src.shell.luca
    |         ```
    |     *   Inside `luca-shell`, type `connect` (if not auto-connected), then `mcp.hello`.
    | 
    | ## Development Cycle (with Docker)
    | 
    | *   **Initial Build**: `docker compose build` (needed if `Dockerfile` or `requirements.txt` files change).
    | *   **Code Changes**: Modify Python code in your local `llmbasedos_src/` directory.
    | *   **Apply Changes**:
    |     *   The `docker-compose.yml` is set up to mount `./llmbasedos_src` into `/opt/app/llmbasedos` in the container.
    |     *   Restart services to pick up Python code changes:
    |         ```bash
    |         docker compose restart llmbasedos_instance 
    |         # OR, for specific service restart:
    |         # docker exec -it llmbasedos_instance supervisorctl restart mcp-gateway 
    |         ```
    | *   **Configuration Changes**: If you modify mounted config files (`supervisord.conf`, `licence_tiers.yaml`, etc.), a `docker-compose restart llmbasedos_instance` is also sufficient.
    | 
    | ## ISO Build (Alternative/Legacy)
    | 
    | The `iso/` directory contains scripts for building a bootable Arch Linux ISO. This is a more complex deployment method, with Docker being the preferred route for most use cases. (Refer to older README versions or `iso/build.sh` for details if needed).
    | 
    | ## Changelog (Recent Major Changes)
    | 
    | *   **[2025-05-22] - Dockerization & Framework Refactor**
    |     *   Primary deployment model shifted to Docker using a single image managed by Supervisord.
    |     *   Introduced `MCPServer` framework in `llmbasedos_pkg/mcp_server_framework.py` for all backend servers (`fs`, `sync`, `mail`, `agent`), standardizing initialization, MCP method registration, socket handling, and capability publishing.
    |     *   Project source code refactored into a main Python package (e.g., `llmbasedos_src/` on host, becoming `llmbasedos` package in Docker) for cleaner imports and module management.
    |     *   Gateway (`gateway/main.py`) updated to use FastAPI's `lifespan` manager for startup/shutdown events.
    |     *   Shell (`shell/luca.py`) refactored into `ShellApp` class for better state and connection management.
    |     *   Corrected numerous import errors and runtime issues related to module discovery, Python path, and library API changes (e.g., `websockets`, `logging.config`).
    |     *   Configuration for licence tiers (`gateway/licence_tiers.yaml`) and mail accounts (`mail_accounts.yaml`) externalized.
    |     *   Hugging Face cache directory configured via `HF_HOME` for `fs_server` to resolve permission issues.
    |     *   Added `jsonschema` dependency for MCP parameter validation within `MCPServer` framework.
    |     *   `supervisord.conf` now correctly sets working directories and includes sections for `supervisorctl` interaction.
    |     *   `Dockerfile` optimized with multi-stage builds and correct user/permission setup.
    |     *   `docker-compose.yml` configured for easy launch, volume mounting (including live code mounting for development), and environment variable setup.
    | 
    | ## Future Improvements & TODOs
    | 
    | *   Robust OAuth2 support for mail server.
    | *   Secure credential management (Vault, system keyring integration).
    | *   Advanced shell features (path/argument tab completion, job control).
    | *   More sophisticated workflow engine and step types for the agent server.
    | *   Web UI for management.
    | *   Comprehensive test suite.
    | *   Security hardening.
    | *   (Consider removing or clearly marking the ISO build ���������� as legacy/advanced if Docker is the main focus).
    --- Fin Contenu ---
  Fichier: docker-compose.yml
    --- Début Contenu (utf-8) ---
    | # llmbasedos/docker-compose.yml
    | version: '3.8'
    | 
    | services:
    |   llmbasedos:
    |     # Pour le développement rapide du code Python, utilisez l'image pré-construite
    |     # et montez votre code source local par-dessus celui de l'image.
    |     image: llmbasedos/appliance:latest # Assurez-vous que ce tag existe localement
    |     build: 
    |       context: .
    |       dockerfile: Dockerfile # Décommentez et utilisez `docker-compose build` 
    |                              # SEULEMENT si le Dockerfile ou les requirements.txt changent.
    |     
    |     container_name: llmbasedos_instance
    |     ports:
    |       - "${LLMBDO_GATEWAY_EXPOSED_PORT:-8000}:8000"
    |     volumes:
    |       # 1. MONTAGE DU CODE SOURCE LOCAL (pour le développement)
    |       #    Le code dans ./llmbasedos_src/ sur votre hôte masquera celui
    |       #    copié dans l'image à /opt/app/llmbasedos/
    |       - ./llmbasedos_src:/opt/app/llmbasedos 
    | 
    |       # 2. Fichiers de configuration montés depuis l'hôte
    |       - ./llmbasedos_src/gateway/licence_tiers.yaml:/etc/llmbasedos/licence_tiers.yaml:ro
    |       - ./lic.key:/etc/llmbasedos/lic.key:ro 
    |       - ./mail_accounts.yaml:/etc/llmbasedos/mail_accounts.yaml:ro 
    |       - ./workflows:/etc/llmbasedos/workflows:ro 
    |       - ./supervisord.conf:/etc/supervisor/conf.d/llmbasedos_supervisor.conf:ro # Utilise le nom exact du fichier config copié dans le Dockerfile
    | 
    |       # 3. Données persistantes (volumes nommés Docker)
    |       - llmbasedos_faiss_index:/var/lib/llmbasedos/faiss_index
    |       - llmbasedos_app_logs:/var/log/llmbasedos
    |       - llmbasedos_supervisor_logs:/var/log/supervisor
    | 
    |       # 4. Données utilisateur pour mcp.fs
    |       - ./user_files:/mnt/user_data 
    |     
    |     environment:
    |       - PYTHONUNBUFFERED=1
    |       - LLMBDO_LOG_LEVEL=${LLMBDO_LOG_LEVEL:-INFO}
    |       - OPENAI_API_KEY=${OPENAI_API_KEY:?err_openai_api_key_not_set_in_env_file} # Message d'erreur plus clair
    |       # Exemple pour llama.cpp sur l'hôte (si Docker Desktop avec WSL2)
    |       # - LLAMA_CPP_URL=http://host.docker.internal:8080 
    |       - LLMBDO_FS_DATA_ROOT=/mnt/user_data
    |       # Les variables d'environnement définies dans le Dockerfile (comme LLMBDO_GATEWAY_WEB_PORT, APP_ROOT_DIR, etc.)
    |       # seront utilisées sauf si surchargées ici ou dans un fichier .env.
    |       # Il n'est généralement pas nécessaire de les redéfinir ici si les valeurs par défaut du Dockerfile sont bonnes.
    |     
    |     restart: unless-stopped
    |     stop_grace_period: 1m
    | 
    | volumes:
    |   llmbasedos_faiss_index:
    |   llmbasedos_app_logs:
    |   llmbasedos_supervisor_logs:
    | 
    | # --- Optionnel : Service llama.cpp ---
    | # services:
    | #   llama_cpp_service:
    | #     image: ghcr.io/ggerganov/llama.cpp:server 
    | #     # Ou une image spécifique comme `your-repo/llama-cpp-cpu-server:latest`
    | #     # Si vous avez besoin de la version CPU uniquement.
    | #     # Vous pouvez la construire vous-même ou chercher des images publiques.
    | #     # L'image officielle ggerganov/llama.cpp:server peut nécessiter des options de build
    | #     # ou des variables d'environnement pour forcer le mode CPU si CUDA est détecté.
    | #     # Pour forcer CPU avec l'image officielle, il n'y a pas d'option simple, 
    | #     # il faut généralement la compiler sans support GPU ou s'assurer qu'aucun GPU n'est passé.
    | #     ports:
    | #       - "${LLAMA_CPP_EXPOSED_PORT:-8081}:8080"
    | #     volumes:
    | #       - ./models:/models:ro # Montez vos modèles GGUF
    | #     # Adaptez la commande pour votre modèle et pour s'assurer qu'il tourne en mode CPU si c'est ce que vous voulez.
    | #     # L'option --n-gpu-layers 0 force l'utilisation du CPU.
    | #     command: -m /models/your-model-name.Q5_K_M.gguf -c 2048 --host 0.0.0.0 --port 8080 --n-gpu-layers 0
    | #     restart: unless-stopped
    --- Fin Contenu ---
  Fichier: ingest.py
    --- Début Contenu (utf-8) ---
    | import os
    | from pathlib import Path
    | import chardet # Pour détecter l'encodage des fichiers de manière plus robuste
    | from typing import Optional, TypeVar
    | # --- Configuration ---
    | PROJECT_ROOT_PATH_STR = "."  # Chemin vers la racine de votre projet llmbasedos
    | # (laisser "." si vous exécutez le script depuis la racine du projet)
    | 
    | # Extensions de fichiers dont on veut lire le contenu
    | # Ajouter/supprimer des extensions selon les besoins
    | CONTENT_EXTENSIONS = ['.py', '.json', '.yaml', '.yml', '.md', '.txt', '.sh', '.conf', '.service', '.env', '.dockerignore']
    | 
    | # Répertoires à ignorer (ex: environnements virtuels, caches, builds Docker non pertinents)
    | IGNORE_DIRS = ['.git', '.venv', '.vscode', '.idea', '__pycache__', 'work', 'out', 'build', 'dist', 'node_modules']
    | # Fichiers spécifiques à ignorer
    | IGNORE_FILES = [] # ex: ['.DS_Store']
    | 
    | MAX_FILE_SIZE_BYTES = 1 * 1024 * 1024  # 1MB : Ne pas lire les fichiers trop gros
    | 
    | # --- Fonctions Utilitaires ---
    | def detect_encoding(file_path: Path) -> Optional[str]:
    |     """Tente de détecter l'encodage d'un fichier."""
    |     try:
    |         with file_path.open('rb') as f:
    |             raw_data = f.read(1024 * 4) # Lire les premiers 4KB pour la détection
    |             if not raw_data:
    |                 return 'utf-8' # Fichier vide, assumer utf-8
    |             result = chardet.detect(raw_data)
    |             return result['encoding'] if result['encoding'] else 'utf-8'
    |     except Exception:
    |         return 'utf-8' # Fallback
    | 
    | def should_ignore(path: Path, root_path: Path) -> bool:
    |     """Vérifie si un chemin doit être ignoré."""
    |     relative_path_parts = path.relative_to(root_path).parts
    |     for part in relative_path_parts:
    |         if part in IGNORE_DIRS:
    |             return True
    |     if path.name in IGNORE_FILES:
    |         return True
    |     return False
    | 
    | # --- Script Principal ---
    | def ingest_project_structure(project_root: str) -> str:
    |     """
    |     Parcourt le projet et génère une représentation textuelle de sa structure et du contenu
    |     des fichiers pertinents.
    |     """
    |     root_path = Path(project_root).resolve()
    |     if not root_path.is_dir():
    |         return f"ERREUR: Le chemin du projet '{project_root}' n'est pas un répertoire valide."
    | 
    |     output_lines = []
    |     output_lines.append(f"# INGESTION DU PROJET LLMBASEDOS (Racine: {root_path})\n")
    |     output_lines.append("=" * 50 + "\n")
    | 
    |     for current_dir, dirnames, filenames in os.walk(root_path, topdown=True):
    |         current_path = Path(current_dir)
    | 
    |         # Ignorer les répertoires spécifiés
    |         dirnames[:] = [d for d in dirnames if not should_ignore(current_path / d, root_path)]
    |         
    |         if should_ignore(current_path, root_path) and current_path != root_path:
    |             continue
    | 
    |         relative_dir_path = current_path.relative_to(root_path)
    |         depth = len(relative_dir_path.parts)
    |         indent = "  " * depth
    | 
    |         output_lines.append(f"{indent}Répertoire: ./{relative_dir_path if str(relative_dir_path) != '.' else ''}")
    | 
    |         for filename in sorted(filenames):
    |             file_path = current_path / filename
    |             if should_ignore(file_path, root_path):
    |                 continue
    | 
    |             file_indent = "  " * (depth + 1)
    |             output_lines.append(f"{file_indent}Fichier: {filename}")
    | 
    |             if file_path.suffix.lower() in CONTENT_EXTENSIONS:
    |                 try:
    |                     if file_path.stat().st_size > MAX_FILE_SIZE_BYTES:
    |                         output_lines.append(f"{file_indent}  (Contenu > {MAX_FILE_SIZE_BYTES // (1024*1024)}MB, ignoré)")
    |                         continue
    |                     if file_path.stat().st_size == 0:
    |                         output_lines.append(f"{file_indent}  (Fichier vide)")
    |                         continue
    | 
    |                     encoding = detect_encoding(file_path)
    |                     with file_path.open('r', encoding=encoding, errors='replace') as f_content:
    |                         content = f_content.read()
    |                     output_lines.append(f"{file_indent}  --- Début Contenu ({encoding}) ---")
    |                     # Indenter chaque ligne du contenu
    |                     for line in content.splitlines():
    |                         output_lines.append(f"{file_indent}  | {line}")
    |                     output_lines.append(f"{file_indent}  --- Fin Contenu ---")
    |                 except UnicodeDecodeError as ude:
    |                     output_lines.append(f"{file_indent}  (Erreur de décodage avec {encoding}: {ude}, fichier binaire ?)")
    |                 except Exception as e:
    |                     output_lines.append(f"{file_indent}  (Erreur de lecture du contenu: {e})")
    |         output_lines.append("") # Ligne vide entre les répertoires
    | 
    |     return "\n".join(output_lines)
    | 
    | if __name__ == "__main__":
    |     print("Ce script va ingérer la structure et le contenu du projet.")
    |     print(f"Racine du projet configurée : {Path(PROJECT_ROOT_PATH_STR).resolve()}")
    |     print(f"Extensions de contenu lues : {CONTENT_EXTENSIONS}")
    |     print(f"Répertoires ignorés : {IGNORE_DIRS}")
    |     
    |     confirmation = input("Continuer ? (o/N) : ")
    |     if confirmation.lower() == 'o':
    |         project_data = ingest_project_structure(PROJECT_ROOT_PATH_STR)
    |         output_filename = "llmbasedos_project_ingestion.txt"
    |         with open(output_filename, "w", encoding="utf-8") as f_out:
    |             f_out.write(project_data)
    |         print(f"\nL'ingestion du projet est terminée. Les données ont été sauvegardées dans : {output_filename}")
    |         print(f"Vous pouvez maintenant copier le contenu de ce fichier dans une nouvelle fenêtre de chat.")
    |     else:
    |         print("Ingestion annulée.")
    --- Fin Contenu ---
  Fichier: lic.key
  Fichier: mail_accounts.yaml
    --- Début Contenu (ascii) ---
    | # ./mail_accounts.yaml
    | # Vide pour l'instant, ou exemple :
    | # my_gmail_account:
    | #   email: "votre_email@gmail.com"
    | #   host: "imap.gmail.com"
    | #   user: "votre_email@gmail.com"
    | #   password: "VOTRE_MOT_DE_PASSE_D_APPLICATION_GMAIL" # Utilisez un mot de passe d'application !
    | #   port: 993
    | #   ssl: true
    --- Fin Contenu ---
  Fichier: supervisord.conf
    --- Début Contenu (utf-8) ---
    | ; llmbasedos/supervisord.conf 
    | ; (Sera monté dans le conteneur à /etc/supervisor/conf.d/llmbasedos_supervisor.conf,
    | ;  et utilisé par la commande CMD du Dockerfile)
    | 
    | [supervisord]
    | nodaemon=true                     ; Exécute supervisord au premier plan (essentiel pour Docker)
    | logfile=/var/log/supervisor/supervisord.log ; Chemin vers le fichier log principal de supervisord
    | pidfile=/var/run/supervisord.pid  ; Chemin vers le fichier PID de supervisord (peut aussi être /run/supervisord.pid)
    | loglevel=info                     ; Niveau de log pour supervisord lui-même
    | user=root                         ; Supervisord tourne en tant que root pour gérer les processus
    | childlogdir=/var/log/supervisor   ; Répertoire par défaut pour les logs des enfants si non spécifié par programme
    | 
    | ; --- Configuration du serveur pour supervisorctl ---
    | [unix_http_server]
    | file=/var/run/supervisor.sock   ; Chemin vers le fichier socket UNIX pour la communication RPC
    | chmod=0700                       ; Permissions du fichier socket (seul root peut y accéder par défaut)
    | ;chown=root:root                 ; Propriétaire du socket (root par défaut si supervisord est root)
    | 
    | [rpcinterface:supervisor]
    | supervisor.rpcinterface_factory = supervisor.rpcinterface:make_main_rpcinterface
    | 
    | [supervisorctl]
    | serverurl=unix:///var/run/supervisor.sock ; Indique à supervisorctl comment se connecter au démon
    | ;username=quelquun                  ; Décommentez si vous activez l'authentification
    | ;password=unmotdepasse              ; Décommentez si vous activez l'authentification
    | ;prompt=llmbasedos>                ; Personnaliser le prompt de supervisorctl
    | 
    | ; --- Définitions des Programmes à Gérer ---
    | 
    | [program:mcp-gateway]
    | command=/usr/local/bin/python -m llmbasedos.gateway.main
    | directory=/opt/app                 ; Répertoire de travail, parent du package 'llmbasedos'
    | user=llmuser                       ; Exécuter le processus en tant que llmuser
    | autostart=true                     ; Démarrer automatiquement avec supervisord
    | autorestart=true                   ; Redémarrer automatiquement si le processus s'arrête
    | stopwaitsecs=10                    ; Temps d'attente (secondes) pour un arrêt propre avant SIGKILL
    | stopsignal=TERM                    ; Signal à envoyer pour arrêter le processus (TERM, puis KILL si stopwaitsecs est dépassé)
    | stdout_logfile=/var/log/supervisor/mcp-gateway.stdout.log ; Log de la sortie standard
    | stderr_logfile=/var/log/supervisor/mcp-gateway.stderr.log ; Log de la sortie d'erreur
    | environment=PYTHONUNBUFFERED=1     ; Assure que la sortie Python n'est pas mise en mémoire tampon
    | 
    | [program:mcp-fs-server]
    | command=/usr/local/bin/python -m llmbasedos.servers.fs.server
    | directory=/opt/app
    | user=llmuser
    | autostart=true
    | autorestart=true
    | stopwaitsecs=10
    | stopsignal=TERM
    | stdout_logfile=/var/log/supervisor/mcp-fs-server.stdout.log
    | stderr_logfile=/var/log/supervisor/mcp-fs-server.stderr.log
    | environment=PYTHONUNBUFFERED=1,HF_HOME="/opt/app/llmbasedos_cache/huggingface",TRANSFORMERS_CACHE="/opt/app/llmbasedos_cache/huggingface/hub"
    | 
    | [program:mcp-sync-server]
    | command=/usr/local/bin/python -m llmbasedos.servers.sync.server
    | directory=/opt/app
    | user=llmuser
    | autostart=true
    | autorestart=true
    | stopwaitsecs=10
    | stopsignal=TERM
    | stdout_logfile=/var/log/supervisor/mcp-sync-server.stdout.log
    | stderr_logfile=/var/log/supervisor/mcp-sync-server.stderr.log
    | environment=PYTHONUNBUFFERED=1
    | 
    | [program:mcp-mail-server]
    | command=/usr/local/bin/python -m llmbasedos.servers.mail.server
    | directory=/opt/app
    | user=llmuser
    | autostart=true
    | autorestart=true
    | stopwaitsecs=10
    | stopsignal=TERM
    | stdout_logfile=/var/log/supervisor/mcp-mail-server.stdout.log
    | stderr_logfile=/var/log/supervisor/mcp-mail-server.stderr.log
    | environment=PYTHONUNBUFFERED=1
    | 
    | [program:mcp-agent-server]
    | command=/usr/local/bin/python -m llmbasedos.servers.agent.server
    | directory=/opt/app
    | user=llmuser
    | autostart=true
    | autorestart=true
    | stopwaitsecs=10
    | stopsignal=TERM
    | stdout_logfile=/var/log/supervisor/mcp-agent-server.stdout.log
    | stderr_logfile=/var/log/supervisor/mcp-agent-server.err.log
    | environment=PYTHONUNBUFFERED=1			
    --- Fin Contenu ---

  Répertoire: ./user_files

  Répertoire: ./workflows

  Répertoire: ./gateway

    Répertoire: ./gateway/licence_tiers.yaml

  Répertoire: ./llmbasedos_src
    Fichier: __init__.py
      (Fichier vide)
    Fichier: common_utils.py
      --- Début Contenu (utf-8) ---
      | # llmbasedos/common_utils.py
      | import os
      | from pathlib import Path
      | from typing import Optional, Tuple, Any # Added Any
      | import logging
      | 
      | # Logger for this module - will be configured if a server/app imports it and has logging set up
      | # Or, can set up a basic one here if run standalone (unlikely for utils)
      | logger = logging.getLogger("llmbasedos.common_utils")
      | 
      | # --- Path Validation (centralized and enhanced) ---
      | DEFAULT_VIRTUAL_ROOT_STR = os.getenv("LLMBDO_DEFAULT_VIRTUAL_ROOT", os.path.expanduser("~"))
      | DEFAULT_VIRTUAL_ROOT = Path(DEFAULT_VIRTUAL_ROOT_STR).resolve()
      | logger.info(f"Default virtual root for path validation: {DEFAULT_VIRTUAL_ROOT}")
      | 
      | def _is_path_within_virtual_root(path_to_check: Path, virtual_root: Path) -> bool:
      |     try:
      |         resolved_check = path_to_check.resolve()
      |         resolved_root = virtual_root.resolve() # Ensure virtual_root itself is resolved
      |         # Check if resolved_check is equal to or a subpath of resolved_root
      |         return resolved_check == resolved_root or resolved_root in resolved_check.parents
      |     except Exception as e: # Symlink loops, permissions on resolve()
      |         logger.warning(f"Path safety check failed for {path_to_check} against {virtual_root}: {e}")
      |         return False
      | 
      | 
      | # llmbasedos_pkg/common_utils.py
      | 
      | # llmbasedos_pkg/common_utils.py
      | # ... (logger, DEFAULT_VIRTUAL_ROOT_STR, _is_path_within_virtual_root) ...
      | 
      | def validate_mcp_path_param(
      |     path_param_relative_to_root: str, # Ex: "docs/file.txt" ou "" pour la racine virtuelle elle-même
      |     virtual_root_str: str,            # Ex: "/mnt/user_data" (doit être fourni et exister)
      |     check_exists: bool = False,
      |     must_be_dir: Optional[bool] = None,
      |     must_be_file: Optional[bool] = None
      | ) -> Tuple[Optional[Path], Optional[str]]:
      |     
      |     logger.debug(f"validate_mcp_path_param: Validating '{path_param_relative_to_root}' against virtual_root '{virtual_root_str}'")
      |     
      |     try:
      |         # La racine virtuelle doit exister et être un répertoire
      |         effective_virtual_root = Path(virtual_root_str).resolve()
      |         if not effective_virtual_root.is_dir():
      |             # Cet échec devrait être attrapé au démarrage du serveur fs, mais vérification ici aussi.
      |             msg = f"Virtual root '{effective_virtual_root}' is not an existing directory."
      |             logger.error(msg)
      |             return None, msg
      | 
      |         # path_param_relative_to_root est déjà nettoyé de son '/' initial.
      |         # Il représente un chemin relatif à la racine virtuelle.
      |         # Ex: path_param_relative_to_root = "docs/notes.txt", effective_virtual_root = Path("/mnt/user_data")
      |         # disk_path deviendra Path("/mnt/user_data/docs/notes.txt")
      |         # Si path_param_relative_to_root est "", disk_path deviendra Path("/mnt/user_data")
      |         disk_path = (effective_virtual_root / path_param_relative_to_root).resolve()
      | 
      |         # Sécurité : Vérifier que le chemin résolu `disk_path` est bien DANS ou ÉGAL à `effective_virtual_root`.
      |         if not _is_path_within_virtual_root(disk_path, effective_virtual_root):
      |             unconfined_msg = f"Access violation: Path '{path_param_relative_to_root}' (resolves to '{disk_path}') is outside virtual root '{effective_virtual_root}'."
      |             logger.warning(unconfined_msg)
      |             return None, f"Path '{path_param_relative_to_root}' is outside allowed access boundaries."
      | 
      |         if check_exists and not disk_path.exists():
      |             return None, f"Path '{path_param_relative_to_root}' (resolved to '{disk_path.relative_to(effective_virtual_root)}' within root) does not exist."
      |         
      |         if disk_path.exists(): # Vérifier le type seulement si le chemin existe
      |             if must_be_dir is True and not disk_path.is_dir():
      |                 return None, f"Path '{path_param_relative_to_root}' (resolved to '{disk_path.relative_to(effective_virtual_root)}') is not a directory."
      |             if must_be_file is True and not disk_path.is_file():
      |                 return None, f"Path '{path_param_relative_to_root}' (resolved to '{disk_path.relative_to(effective_virtual_root)}') is not a file."
      |             
      |         return disk_path, None # Retourne le chemin disque absolu et validé
      |     
      |     except ValueError as ve: 
      |         logger.warning(f"Path '{path_param_relative_to_root}' is malformed: {ve}")
      |         return None, f"Path '{path_param_relative_to_root}' is malformed."
      |     except Exception as e: 
      |         logger.error(f"Unexpected error validating path '{path_param_relative_to_root}' against '{virtual_root_str}': {e}", exc_info=True)
      |         return None, f"Error processing path '{path_param_relative_to_root}': {type(e).__name__}"
      --- Fin Contenu ---
    Fichier: mcp_server_framework.py
      --- Début Contenu (utf-8) ---
      | # llmbasedos/mcp_server_framework.py
      | import asyncio
      | import json
      | import logging
      | import os
      | from pathlib import Path
      | from typing import Any, Dict, List, Optional, Callable, Awaitable, Union, Tuple # Added 'Tuple'
      | from concurrent.futures import ThreadPoolExecutor
      | import jsonschema # For input validation
      | import shutil # <<< AJOUTER CET IMPORT
      | 
      | # --- JSON-RPC Constants (centralized) ---
      | JSONRPC_PARSE_ERROR = -32700
      | JSONRPC_INVALID_REQUEST = -32600
      | JSONRPC_METHOD_NOT_FOUND = -32601
      | JSONRPC_INVALID_PARAMS = -32602
      | JSONRPC_INTERNAL_ERROR = -32603
      | 
      | def create_mcp_response(id: Union[str, int, None], result: Optional[Any] = None) -> Dict[str, Any]:
      |     return {"jsonrpc": "2.0", "id": id, "result": result}
      | 
      | def create_mcp_error(id: Union[str, int, None], code: int, message: str, data: Optional[Any] = None) -> Dict[str, Any]:
      |     error_obj: Dict[str, Any] = {"code": code, "message": message}
      |     if data is not None:
      |         error_obj["data"] = data
      |     return {"jsonrpc": "2.0", "id": id, "error": error_obj}
      | 
      | # --- Base MCP Server Class ---
      | class MCPServer:
      |     def __init__(self, 
      |                  server_name: str, 
      |                  caps_file_path_str: str, 
      |                  custom_error_code_base: int = -32000,
      |                  socket_dir_str: str = "/run/mcp"):
      |         self.server_name = server_name
      |         self.socket_path = Path(socket_dir_str) / f"{self.server_name}.sock"
      |         self.caps_file_path = Path(caps_file_path_str)
      |         self.custom_error_code_base = custom_error_code_base
      | 
      |         log_level_str = os.getenv(f"LLMBDO_{self.server_name.upper()}_LOG_LEVEL", "INFO").upper()
      |         log_level_int = logging.getLevelName(log_level_str)
      |         self.logger = logging.getLogger(f"llmbasedos.servers.{self.server_name}")
      |         
      |         if not self.logger.hasHandlers():
      |             handler = logging.StreamHandler()
      |             formatter = logging.Formatter(f"%(asctime)s - {self.server_name} - %(levelname)s - %(message)s")
      |             handler.setFormatter(formatter)
      |             self.logger.addHandler(handler)
      |         self.logger.setLevel(log_level_int)
      | 
      |         self._method_handlers: Dict[str, Callable[..., Awaitable[Any]]] = {}
      |         self._method_schemas: Dict[str, Dict[str, Any]] = {}
      |         
      |         num_workers = int(os.getenv(f"LLMBDO_{self.server_name.upper()}_WORKERS", 
      |                                     os.getenv("LLMBDO_DEFAULT_SERVER_WORKERS", "2")))
      |         self.executor = ThreadPoolExecutor(max_workers=num_workers, thread_name_prefix=f"{self.server_name}_worker")
      |         self.logger.info(f"Initialized with {num_workers} worker threads.")
      | 
      |         self._load_capabilities_and_schemas()
      | 
      |         # Initialize hooks with default (no-op) implementations
      |         # User can override these by assigning a new callable to self.on_startup / self.on_shutdown
      |         # The type hint 'MCPServer' needs to be in quotes for forward reference if MCPServer is not fully defined yet.
      |         self._on_startup_hook: Optional[Callable[['MCPServer'], Awaitable[None]]] = self._default_on_startup
      |         self._on_shutdown_hook: Optional[Callable[['MCPServer'], Awaitable[None]]] = self._default_on_shutdown
      |     def _publish_capability_descriptor(self):
      |         """
      |         Copies the server's caps.json file to the discovery directory /run/mcp/
      |         so the gateway can find it.
      |         """
      |         if not self.caps_file_path.exists():
      |             self.logger.error(f"Cannot publish capabilities: Source file {self.caps_file_path} does not exist.")
      |             return
      | 
      |         discovery_dir = Path("/run/mcp") # Devrait correspondre à MCP_CAPS_DIR du gateway
      |         discovery_dir.mkdir(parents=True, exist_ok=True) # S'assurer que /run/mcp existe
      |         
      |         # Le nom du fichier dans le répertoire de découverte est server_name.cap.json
      |         destination_cap_file = discovery_dir / f"{self.server_name}.cap.json"
      |         
      |         try:
      |             shutil.copyfile(self.caps_file_path, destination_cap_file)
      |             # S'assurer que le fichier a les bonnes permissions pour que le gateway (llmuser) puisse le lire
      |             # et que le serveur lui-même (llmuser) puisse le supprimer au shutdown.
      |             # Le umask de llmuser devrait donner des perms raisonnables, sinon on peut chmod.
      |             os.chmod(destination_cap_file, 0o664) # rw-rw-r-- (llmuser, llmgroup, others read)
      |             self.logger.info(f"Successfully published capability descriptor to {destination_cap_file}")
      |         except Exception as e:
      |             self.logger.error(f"Failed to publish capability descriptor from {self.caps_file_path} to {destination_cap_file}: {e}", exc_info=True)
      | 
      |     def _unpublish_capability_descriptor(self):
      |         """
      |         Removes the server's caps.json file from the discovery directory /run/mcp/
      |         during shutdown.
      |         """
      |         discovery_dir = Path("/run/mcp")
      |         destination_cap_file = discovery_dir / f"{self.server_name}.cap.json"
      |         if destination_cap_file.exists():
      |             try:
      |                 destination_cap_file.unlink()
      |                 self.logger.info(f"Successfully unpublished capability descriptor from {destination_cap_file}")
      |             except Exception as e:
      |                 self.logger.error(f"Failed to unpublish capability descriptor {destination_cap_file}: {e}", exc_info=True)
      | 
      |     def _load_capabilities_and_schemas(self):
      |         if not self.caps_file_path.exists():
      |             self.logger.error(f"CRITICAL: Capability file {self.caps_file_path} missing for '{self.server_name}'.")
      |             return
      |         try:
      |             with self.caps_file_path.open('r') as f:
      |                 caps_data = json.load(f)
      |             for cap_item in caps_data.get("capabilities", []):
      |                 method_name = cap_item.get("method")
      |                 params_schema = cap_item.get("params_schema")
      |                 if method_name and isinstance(params_schema, dict):
      |                     self._method_schemas[method_name] = params_schema
      |                 elif method_name and params_schema is None:
      |                      self._method_schemas[method_name] = {"type": "array", "maxItems": 0}
      |                 elif method_name:
      |                     self.logger.warning(f"Method '{method_name}' in {self.caps_file_path.name} has invalid 'params_schema'.")
      |             self.logger.info(f"Loaded {len(self._method_schemas)} param schemas from {self.caps_file_path.name}")
      |         except Exception as e:
      |             self.logger.error(f"Error loading schemas from {self.caps_file_path}: {e}", exc_info=True)
      | 
      |     def register_method(self, method_name: str): # Renamed from 'register' for clarity
      |         def decorator(func: Callable[..., Awaitable[Any]]):
      |             if method_name in self._method_handlers:
      |                 self.logger.warning(f"Method '{method_name}' re-registered. Overwriting.")
      |             self.logger.debug(f"Registering method: {method_name} -> {func.__name__}")
      |             self._method_handlers[method_name] = func
      |             if method_name not in self._method_schemas:
      |                  self.logger.warning(f"Method '{method_name}' registered but no params_schema in {self.caps_file_path.name}.")
      |                  self._method_schemas.setdefault(method_name, {"type": "array", "maxItems": 0})
      |             return func
      |         return decorator
      | 
      | # Dans llmbasedos_pkg/mcp_server_framework.py
      |     async def _validate_params(self, method_name: str, params: Union[List[Any], Dict[str, Any]]) -> Optional[str]:
      |         schema = self._method_schemas.get(method_name)
      |         if not schema:
      |             self.logger.debug(f"No schema found for method '{method_name}', skipping validation.")
      |             return None
      | 
      |         self.logger.debug(f"Validating params for '{method_name}'. Schema: {schema}, Instance: {params}")
      |         try:
      |             jsonschema.validate(instance=params, schema=schema)
      |             self.logger.debug(f"Params for '{method_name}' are valid.")
      |             return None
      |         except jsonschema.exceptions.ValidationError as e_val_error:
      |             self.logger.warning(f"jsonschema.exceptions.ValidationError for '{method_name}': {e_val_error.message}. Path: {e_val_error.path}, Validator: {e_val_error.validator}, Schema: {e_val_error.schema}")
      |             error_path_str = " -> ".join(map(str, e_val_error.path)) if e_val_error.path else "params"
      |             return f"Invalid parameter '{error_path_str}': {e_val_error.message}"
      |         except Exception as e_other_val: # Capturer toute autre exception
      |             self.logger.error(f"UNEXPECTED validation error for '{method_name}': {type(e_other_val).__name__} - {e_other_val}", exc_info=True)
      |             return f"Internal error during parameter validation: {type(e_other_val).__name__}"
      | 
      |     async def _handle_single_request(self, request_data: Dict[str, Any]) -> Dict[str, Any]:
      |         request_id = request_data.get("id")
      |         method_name = request_data.get("method")
      |         params = request_data.get("params", [])
      | 
      |         if not isinstance(method_name, str):
      |             return create_mcp_error(request_id, JSONRPC_INVALID_REQUEST, "Method name must be a string.")
      |         
      |         # Basic type check for params if no schema is available or schema allows list/dict
      |         if not self._method_schemas.get(method_name) and not isinstance(params, list):
      |              return create_mcp_error(request_id, JSONRPC_INVALID_PARAMS, "Params must be an array if no schema defines object type.")
      |         elif self._method_schemas.get(method_name) and not isinstance(params, (list, dict)): # If schema exists, it will enforce type
      |              pass # jsonschema will handle this type check based on schema.type
      | 
      |         handler = self._method_handlers.get(method_name)
      |         if not handler:
      |             return create_mcp_error(request_id, JSONRPC_METHOD_NOT_FOUND, f"Method '{method_name}' not found on server '{self.server_name}'.")
      | 
      |         validation_error_msg = await self._validate_params(method_name, params)
      |         if validation_error_msg:
      |             return create_mcp_error(request_id, JSONRPC_INVALID_PARAMS, validation_error_msg)
      |         
      |         try:
      |             result_payload = await handler(self, request_id, params) # Pass self, request_id, params
      |             return create_mcp_response(request_id, result_payload)
      |         except ValueError as ve:
      |             self.logger.warning(f"Handler for '{method_name}' raised ValueError: {ve}")
      |             return create_mcp_error(request_id, self.custom_error_code_base - 1, str(ve))
      |         except PermissionError as pe:
      |             self.logger.warning(f"Handler for '{method_name}' raised PermissionError: {pe}")
      |             return create_mcp_error(request_id, self.custom_error_code_base - 2, str(pe))
      |         except FileNotFoundError as fnfe:
      |             self.logger.warning(f"Handler for '{method_name}' raised FileNotFoundError: {fnfe}")
      |             return create_mcp_error(request_id, self.custom_error_code_base - 3, str(fnfe))
      |         except NotImplementedError as nie:
      |             self.logger.warning(f"Handler for '{method_name}' raised NotImplementedError: {nie}")
      |             return create_mcp_error(request_id, JSONRPC_METHOD_NOT_FOUND, f"Method '{method_name}' action not fully implemented: {nie}")
      |         except Exception as e:
      |             self.logger.error(f"Error executing method '{method_name}' (ID {request_id}): {e}", exc_info=True)
      |             return create_mcp_error(request_id, JSONRPC_INTERNAL_ERROR, f"Internal server error during '{method_name}': {type(e).__name__}")
      | 
      |     async def _client_connection_handler(self, reader: asyncio.StreamReader, writer: asyncio.StreamWriter):
      |         client_addr_obj = writer.get_extra_info('socket').getsockname()
      |         client_desc = f"client_at_{client_addr_obj}" if isinstance(client_addr_obj, str) else f"client_pid_{client_addr_obj}"
      |         self.logger.info(f"Client connected: {client_desc}")
      |         message_buffer = bytearray()
      |         try:
      |             while True:
      |                 try:
      |                     chunk = await reader.read(4096)
      |                     if not chunk: self.logger.info(f"Client {client_desc} disconnected (EOF)."); break
      |                     message_buffer.extend(chunk)
      |                     
      |                     while b'\0' in message_buffer:
      |                         message_bytes, rest_of_buffer = message_buffer.split(b'\0', 1)
      |                         message_buffer = rest_of_buffer
      |                         message_str = message_bytes.decode('utf-8')
      |                         self.logger.debug(f"RCV from {client_desc}: {message_str[:200]}...")
      |                         
      |                         request_id_for_error = None
      |                         try:
      |                             request_data = json.loads(message_str)
      |                             request_id_for_error = request_data.get("id")
      |                             response = await self._handle_single_request(request_data)
      |                         except json.JSONDecodeError:
      |                             response = create_mcp_error(None, JSONRPC_PARSE_ERROR, "Failed to parse JSON.")
      |                         except Exception as e_handler:
      |                             self.logger.error(f"Critical error in _handle_single_request from {client_desc}: {e_handler}", exc_info=True)
      |                             response = create_mcp_error(request_id_for_error, JSONRPC_INTERNAL_ERROR, "Critical internal server error.")
      |                         
      |                         response_bytes = json.dumps(response).encode('utf-8') + b'\0'
      |                         self.logger.debug(f"SND to {client_desc}: {response_bytes.decode()[:200]}...")
      |                         writer.write(response_bytes)
      |                         await writer.drain()
      |                 
      |                 except (asyncio.IncompleteReadError, ConnectionResetError, BrokenPipeError):
      |                     self.logger.info(f"Client {client_desc} connection lost/reset/broken."); break
      |                 except UnicodeDecodeError:
      |                     err_resp = create_mcp_error(None, JSONRPC_PARSE_ERROR, "Invalid UTF-8 sequence.")
      |                     try: writer.write(json.dumps(err_resp).encode('utf-8') + b'\0'); await writer.drain()
      |                     except: pass
      |                     break
      |         
      |         except asyncio.CancelledError: self.logger.info(f"Client handler for {client_desc} cancelled.")
      |         except Exception as e_outer: self.logger.error(f"Unexpected error in client handler for {client_desc}: {e_outer}", exc_info=True)
      |         finally:
      |             self.logger.info(f"Closing connection for {client_desc}")
      |             if not writer.is_closing():
      |                 try: writer.close(); await writer.wait_closed()
      |                 except: pass
      | 
      |     async def start(self):
      |         self.socket_path.parent.mkdir(parents=True, exist_ok=True)
      |         if self.socket_path.exists():
      |             try: self.socket_path.unlink()
      |             except OSError as e: self.logger.error(f"Error removing old socket {self.socket_path}: {e}"); return
      | 
      |         asyncio_server_obj = await asyncio.start_unix_server(self._client_connection_handler, path=str(self.socket_path))
      |         addr = asyncio_server_obj.sockets[0].getsockname() if asyncio_server_obj.sockets else str(self.socket_path)
      |         self.logger.info(f"MCP Server '{self.server_name}' listening on UNIX socket: {addr}")
      |         
      |         try:
      |             os.chmod(str(self.socket_path), 0o660)
      |             self.logger.info(f"Set permissions for {self.socket_path} to 0660.")
      |         except OSError as e:
      |             self.logger.warning(f"Could not set permissions/owner for socket {self.socket_path}: {e}")
      | 
      |         # Publier le descripteur de capacité
      |         self._publish_capability_descriptor() # <<< APPEL ICI
      | 
      |         if not self.caps_file_path.exists(): # Redondant si _publish a déjà vérifié, mais ok
      |             self.logger.error(f"Reminder: Caps file {self.caps_file_path} is missing for '{self.server_name}'.")
      |         # else: # Plus besoin de ce log car _publish_capability_descriptor logue déjà
      |             # self.logger.info(f"Service capabilities defined in: {self.caps_file_path.name}")
      | 
      |         if self._on_startup_hook:
      |             self.logger.info(f"Running on_startup() for {self.server_name}...")
      |             await self._on_startup_hook(self)
      | 
      |         try:
      |             async with asyncio_server_obj: await asyncio_server_obj.serve_forever()
      |         except asyncio.CancelledError: self.logger.info(f"Server '{self.server_name}' main loop cancelled.")
      |         except Exception as e: self.logger.error(f"Server '{self.server_name}' exited with error: {e}", exc_info=True)
      |         finally:
      |             self.logger.info(f"Server '{self.server_name}' shutting down...")
      |             if self._on_shutdown_hook:
      |                 self.logger.info(f"Running on_shutdown() for {self.server_name}...")
      |                 try: await self._on_shutdown_hook(self)
      |                 except Exception as e_shutdown_hook: self.logger.error(f"Error in on_shutdown() for {self.server_name}: {e_shutdown_hook}", exc_info=True)
      |             
      |             self._unpublish_capability_descriptor() # <<< APPEL ICI POUR NETTOYER
      | 
      |             self.logger.info(f"Shutting down executor for {self.server_name}...")
      |             self.executor.shutdown(wait=True)
      |             self.logger.info(f"Executor for {self.server_name} shut down.")
      |             
      |             if self.socket_path.exists():
      |                 try: self.socket_path.unlink()
      |                 except OSError as e: self.logger.error(f"Error removing socket {self.socket_path} on shutdown: {e}")
      |             self.logger.info(f"Server '{self.server_name}' fully stopped.")
      | 
      |     async def run_in_executor(self, func: Callable[..., Any], *args: Any) -> Any:
      |         if self.executor._shutdown: # type: ignore
      |              self.logger.warning(f"Executor for {self.server_name} is shutdown. Cannot run task.")
      |              raise RuntimeError(f"Executor for {self.server_name} is already shut down.")
      |         loop = asyncio.get_running_loop()
      |         return await loop.run_in_executor(self.executor, func, *args)
      | 
      |     # --- Hook Management ---
      |     # Default hook implementations (private)
      |     async def _default_on_startup(self, server_instance: 'MCPServer'): # Renamed param for clarity
      |         self.logger.debug(f"{self.server_name} default on_startup called (instance: {id(server_instance)}).")
      |         pass
      | 
      |     async def _default_on_shutdown(self, server_instance: 'MCPServer'):
      |         self.logger.debug(f"{self.server_name} default on_shutdown called (instance: {id(server_instance)}).")
      |         pass
      | 
      |     # Public methods to set hooks
      |     def set_startup_hook(self, hook: Callable[['MCPServer'], Awaitable[None]]):
      |         """Assigns a coroutine to be called on server startup. Hook signature: async def my_hook(server: MCPServer)."""
      |         self._on_startup_hook = hook
      |         self.logger.info(f"Custom startup hook set for {self.server_name}.")
      | 
      |     def set_shutdown_hook(self, hook: Callable[['MCPServer'], Awaitable[None]]):
      |         """Assigns a coroutine to be called on server shutdown. Hook signature: async def my_hook(server: MCPServer)."""
      |         self._on_shutdown_hook = hook
      |         self.logger.info(f"Custom shutdown hook set for {self.server_name}.")
      | 
      |     # For direct attribute assignment (less formal, but used in your server files)
      |     # This ensures the type hint is correct if user does `server.on_startup = my_func`
      |     @property
      |     def on_startup(self) -> Optional[Callable[['MCPServer'], Awaitable[None]]]:
      |         return self._on_startup_hook
      | 
      |     @on_startup.setter
      |     def on_startup(self, hook: Callable[['MCPServer'], Awaitable[None]]):
      |         self.set_startup_hook(hook)
      | 
      |     @property
      |     def on_shutdown(self) -> Optional[Callable[['MCPServer'], Awaitable[None]]]:
      |         return self._on_shutdown_hook
      | 
      |     @on_shutdown.setter
      |     def on_shutdown(self, hook: Callable[['MCPServer'], Awaitable[None]]):
      |         self.set_shutdown_hook(hook)
      | 
      |     # Method to create custom error responses consistently
      |     def create_custom_error(self, request_id: Union[str, int, None], error_sub_code: int, message: str, data: Optional[Any] = None) -> Dict[str, Any]:
      |         """Creates a JSON-RPC error object using the server's custom error base."""
      |         # Ensure sub_code is negative if base is negative, or positive if base is positive
      |         # Here, base is -32000, so sub_codes like -1, -2 become -32001, -32002.
      |         # If you pass sub_code as 1, 2, it would be -31999, -31998.
      |         # Let's assume sub_code is positive (1, 2, 3...) and we subtract it from base.
      |         final_code = self.custom_error_code_base - abs(error_sub_code)
      |         return create_mcp_error(request_id, final_code, message, data)
      --- Fin Contenu ---
    Fichier: requirements.txt
      --- Début Contenu (ascii) ---
      | jsonschema>=4.17.0,<5.0.0
      --- Fin Contenu ---

    Répertoire: ./llmbasedos_src/shell
      Fichier: builtin_cmds.py
        --- Début Contenu (utf-8) ---
        | # llmbasedos_src/shell/builtin_cmds.py
        | import os
        | import json # Pour parser les arguments optionnels en JSON
        | import sys 
        | from pathlib import Path
        | from typing import List, Any, Dict, Optional
        | 
        | # Import ShellApp type pour l'annotation de type et les utilitaires Rich
        | from typing import TYPE_CHECKING
        | if TYPE_CHECKING:
        |     from .luca import ShellApp # Utilisé pour l'annotation de type de 'app'
        | 
        | # Shell utils (si cmd_llm l'utilise directement)
        | from .shell_utils import stream_llm_chat_to_console # stream_llm_chat_to_console est nécessaire pour cmd_llm
        | from rich.text import Text # Pour formater certains messages
        | 
        | # Liste des commandes builtin (pour la complétion et l'aide)
        | BUILTIN_COMMAND_LIST = [
        |     "exit", "quit", "help", "connect", "cd", "pwd", 
        |     "ls", "dir", "cat", "rm", "licence", "llm"
        | ]
        | 
        | # --- Implémentation des Commandes Built-in ---
        | # Nouvelle signature : async def cmd_nom_commande(args_list: List[str], app: 'ShellApp')
        | 
        | async def cmd_exit(args_list: List[str], app: 'ShellApp'):
        |     """Exits the luca-shell."""
        |     app.console.print("Exiting luca-shell...")
        |     raise EOFError # Signale à prompt_toolkit de quitter la boucle REPL
        | 
        | async def cmd_quit(args_list: List[str], app: 'ShellApp'):
        |     """Alias for the 'exit' command."""
        |     await cmd_exit(args_list, app)
        | 
        | async def cmd_help(args_list: List[str], app: 'ShellApp'):
        |     """Shows available commands or help for a specific command.
        |     Usage: help [builtin_command_name]
        |     """
        |     # Utiliser app.console qui est l'instance de Rich Console de ShellApp
        |     if not args_list:
        |         app.console.print("[bold]Available luca-shell built-in commands:[/bold]")
        |         # Assumer que ShellApp a une méthode pour créer une table Rich ou que Table est importé ici
        |         from rich.table import Table # Importer Table ici si besoin local
        |         tbl = Table(title="Built-in Commands", show_header=True, header_style="bold magenta")
        |         tbl.add_column("Command", style="dim", width=15)
        |         tbl.add_column("Description")
        |         
        |         for cmd_name_str in BUILTIN_COMMAND_LIST:
        |             handler_func = getattr(sys.modules[__name__], f"cmd_{cmd_name_str}", None)
        |             docstring = "No description available."
        |             if handler_func and handler_func.__doc__:
        |                 docstring = handler_func.__doc__.strip().splitlines()[0]
        |             tbl.add_row(cmd_name_str, docstring)
        |         app.console.print(tbl)
        |         
        |         available_mcp_cmds = sorted(list(app.available_mcp_commands))
        |         if available_mcp_cmds:
        |             app.console.print(f"\n[bold]Available MCP commands ({len(available_mcp_cmds)} discovered):[/bold]")
        |             for i in range(0, len(available_mcp_cmds), 5):
        |                  app.console.print("  " + ", ".join(available_mcp_cmds[i:i+5]))
        |         else:
        |             app.console.print("\n[yellow]No MCP commands currently discovered. Try 'connect' or check gateway.[/yellow]")
        |             
        |         app.console.print("\nType 'help <builtin_command_name>' for details on built-ins.")
        |         app.console.print("For MCP methods, use 'mcp.listCapabilities' or check protocol documentation.")
        |     else:
        |         cmd_to_help_str = args_list[0]
        |         handler_func = getattr(sys.modules[__name__], f"cmd_{cmd_to_help_str}", None)
        |         if handler_func and handler_func.__doc__:
        |             app.console.print(f"[bold]Help for built-in command '{cmd_to_help_str}':[/bold]\n{handler_func.__doc__.strip()}")
        |         else:
        |             app.console.print(f"No help found for built-in command '{cmd_to_help_str}'. If it's an MCP command, its description can be found via 'mcp.listCapabilities'.")
        | 
        | async def cmd_connect(args_list: List[str], app: 'ShellApp'):
        |     """Attempts to (re)connect to the MCP gateway."""
        |     app.console.print("Attempting to (re)connect to MCP gateway...")
        |     if await app.ensure_connection(force_reconnect=True):
        |         app.console.print("[green]Successfully connected/reconnected to MCP Gateway.[/green]")
        |         app.console.print("Type 'mcp.hello' or 'mcp.listCapabilities' to see available remote commands.")
        |     else:
        |         app.console.print("[[error]Failed to connect[/]]. Check gateway status and URL configured in shell.")
        | 
        | async def cmd_cd(args_list: List[str], app: 'ShellApp'):
        |     """Changes the current working directory. Usage: cd <path>"""
        |     if not args_list:
        |         target_path_str = os.path.expanduser("~")
        |     else:
        |         target_path_str = args_list[0]
        |     
        |     current_cwd = app.get_cwd()
        |     expanded_path_str = os.path.expanduser(target_path_str)
        |     
        |     new_path_obj: Path
        |     if os.path.isabs(expanded_path_str):
        |         new_path_obj = Path(expanded_path_str).resolve()
        |     else:
        |         new_path_obj = (current_cwd / expanded_path_str).resolve()
        | 
        |     response = await app.send_mcp_request(None, "mcp.fs.list", [str(new_path_obj)]) 
        |     
        |     if response and "result" in response:
        |         app.set_cwd(new_path_obj)
        |     elif response and "error" in response:
        |         # Utiliser app._format_and_print_mcp_response pour afficher l'erreur MCP de manière standard
        |         await app._format_and_print_mcp_response("mcp.fs.list", response, request_path_for_ls=str(new_path_obj))
        |     else:
        |         app.console.print(f"[[error]cd error[/]]: Error verifying path '{new_path_obj}'. No or invalid response from gateway.")
        | 
        | async def cmd_pwd(args_list: List[str], app: 'ShellApp'):
        |     """Prints the current working directory managed by the shell."""
        |     app.console.print(str(app.get_cwd()))
        | 
        | async def cmd_ls(args_list: List[str], app: 'ShellApp'):
        |     """Lists files and directories. Usage: ls [path]"""
        |     path_arg_str = args_list[0] if args_list else "."
        |     
        |     expanded_path_str = os.path.expanduser(path_arg_str) if path_arg_str.startswith('~') else path_arg_str
        |     abs_path_obj = (app.get_cwd() / expanded_path_str).resolve() if not os.path.isabs(expanded_path_str) else Path(expanded_path_str).resolve()
        |     
        |     response = await app.send_mcp_request(None, "mcp.fs.list", [str(abs_path_obj)])
        |     await app._format_and_print_mcp_response("mcp.fs.list", response, request_path_for_ls=str(abs_path_obj))
        | 
        | async def cmd_dir(args_list: List[str], app: 'ShellApp'):
        |     """Alias for the 'ls' command."""
        |     await cmd_ls(args_list, app)
        | 
        | async def cmd_cat(args_list: List[str], app: 'ShellApp'):
        |     """Displays file content. Usage: cat <path> [text|base64]"""
        |     if not args_list:
        |         app.console.print("[[error]Usage[/]]: cat <path> [text|base64]")
        |         return
        | 
        |     path_str_arg = args_list[0]
        |     expanded_path_str = os.path.expanduser(path_str_arg) if path_str_arg.startswith('~') else path_str_arg
        |     abs_path_obj = (app.get_cwd() / expanded_path_str).resolve() if not os.path.isabs(expanded_path_str) else Path(expanded_path_str).resolve()
        |     
        |     mcp_params: List[Any] = [str(abs_path_obj)]
        |     if len(args_list) > 1: # Pour l'argument optionnel d'encodage
        |         mcp_params.append(args_list[1])
        | 
        |     response = await app.send_mcp_request(None, "mcp.fs.read", mcp_params)
        |     await app._format_and_print_mcp_response("mcp.fs.read", response)
        | 
        | async def cmd_rm(args_list: List[str], app: 'ShellApp'):
        |     """Deletes a file or directory. Usage: rm <path> [-r|--recursive] [--force|-f]"""
        |     if not args_list:
        |         app.console.print("[[error]Usage[/]]: rm <path> [-r|--recursive] [--force|-f]")
        |         return
        |     
        |     path_to_delete_str = args_list[0]
        |     recursive_flag = any(flag in args_list for flag in ["-r", "--recursive"])
        |     force_flag = any(flag in args_list for flag in ["-f", "--force"])
        | 
        |     if not force_flag:
        |         confirm_msg = f"Delete '{path_to_delete_str}'{' recursively' if recursive_flag else ''}? This is permanent. "
        |         app.console.print(f"[yellow]{confirm_msg}Add --force or -f to confirm. Skipping for now.[/yellow]")
        |         return
        | 
        |     expanded_path_str = os.path.expanduser(path_to_delete_str) if path_to_delete_str.startswith('~') else path_to_delete_str
        |     abs_path_str_to_delete = str((app.get_cwd() / expanded_path_str).resolve() if not os.path.isabs(expanded_path_str) else Path(expanded_path_str).resolve())
        |     
        |     mcp_params_for_rm = [abs_path_str_to_delete, recursive_flag]
        |     response = await app.send_mcp_request(None, "mcp.fs.delete", mcp_params_for_rm)
        |     await app._format_and_print_mcp_response("mcp.fs.delete", response, request_path_for_ls=abs_path_str_to_delete)
        | 
        | async def cmd_licence(args_list: List[str], app: 'ShellApp'):
        |     """Displays current licence information from the gateway."""
        |     response = await app.send_mcp_request(None, "mcp.licence.check", [])
        |     await app._format_and_print_mcp_response("mcp.licence.check", response)
        | 
        | async def cmd_llm(args_list: List[str], app: 'ShellApp'):
        |     """
        |     Sends a chat prompt to the LLM via mcp.llm.chat for a streaming response.
        |     Usage: llm "Your prompt text" ['<json_options_dict_string>']
        |     Example: llm "What is the capital of France?"
        |     Example: llm "Translate to Spanish: Hello World" '{"model": "gpt-4o"}'
        |     Note: Options dictionary string must be valid JSON.
        |     """
        |     if not args_list:
        |         app.console.print(Text("Usage: llm \"<prompt_text>\" ['<json_options_dict_string>']", style="yellow"))
        |         app.console.print(Text("Example: llm \"Tell me a joke about developers.\"", style="yellow")); return
        | 
        |     prompt_str = args_list[0]
        |     options_json_str = args_list[1] if len(args_list) > 1 else "{}" 
        |     
        |     llm_options_dict: Dict[str, Any] = {}
        |     try:
        |         llm_options_dict = json.loads(options_json_str)
        |         if not isinstance(llm_options_dict, dict):
        |             raise ValueError("LLM options, if provided, must be a valid JSON dictionary string.")
        |     except (json.JSONDecodeError, ValueError) as e:
        |         # Utiliser app.console et escape pour afficher l'erreur
        |         from rich.markup import escape # Importer escape localement si pas déjà global
        |         app.console.print(f"[[error]Invalid LLM options JSON string[/]]: {escape(str(e))}"); return
        | 
        |     messages_for_llm_chat = [{"role": "user", "content": prompt_str}]
        |     
        |     # stream_llm_chat_to_console s'occupera de ensure_connection via l'instance app
        |     await stream_llm_chat_to_console(
        |         app=app, # Passer l'instance de ShellApp
        |         # console=app.console, # <<< LIGNE SUPPRIMÉE, car app est passé
        |         messages=messages_for_llm_chat,
        |         llm_options=llm_options_dict
        |     )
        --- Fin Contenu ---
      Fichier: luca.py
        --- Début Contenu (utf-8) ---
        | # llmbasedos_src/shell/luca.py
        | import asyncio
        | import json
        | import logging
        | import logging.config # For dictConfig
        | import os
        | import sys
        | from pathlib import Path
        | import uuid
        | import shlex # For parsing command line string
        | from typing import Any, Dict, Optional, List, Callable, Awaitable, Set, Tuple # For type hints
        | import signal # Import du module signal
        | 
        | import websockets # Main library for WebSocket client
        | from websockets.client import WebSocketClientProtocol # For precise type hinting
        | from websockets.exceptions import ConnectionClosed, ConnectionClosedOK, WebSocketException 
        | 
        | from prompt_toolkit import PromptSession
        | from prompt_toolkit.history import FileHistory
        | from prompt_toolkit.auto_suggest import AutoSuggestFromHistory
        | from prompt_toolkit.completion import Completer, Completion
        | from prompt_toolkit.styles import Style as PromptStyle
        | from rich.console import Console
        | from rich.text import Text
        | from rich.syntax import Syntax
        | from datetime import datetime # Pour le formatage dans _rich_format_mcp_fs_list
        | from rich.markup import escape # Pour échapper les messages d'erreur
        | 
        | # --- Import des modules locaux ---
        | from . import builtin_cmds 
        | # stream_llm_chat_to_console est utilisé par builtin_cmds.cmd_llm, pas directement ici.
        | 
        | # --- Configuration du Shell, Logging, Console Rich ---
        | SHELL_HISTORY_FILE = Path(os.path.expanduser("~/.llmbasedos_shell_history"))
        | GATEWAY_WS_URL_CONF = os.getenv("LLMBDO_GATEWAY_WS_URL", "ws://localhost:8000/ws")
        | 
        | LOG_LEVEL_STR_CONF = os.getenv("LLMBDO_SHELL_LOG_LEVEL", "INFO").upper()
        | LOG_FORMAT_CONF = os.getenv("LLMBDO_SHELL_LOG_FORMAT", "simple")
        | 
        | def setup_shell_logging():
        |     log_level_int = logging.getLevelName(LOG_LEVEL_STR_CONF)
        |     if not isinstance(log_level_int, int):
        |         log_level_int = logging.INFO
        |         logging.warning(f"Invalid shell log level '{LOG_LEVEL_STR_CONF}', defaulting to INFO.")
        | 
        |     formatter_to_use = LOG_FORMAT_CONF
        |     formatter_class = "logging.Formatter"
        |     formatter_details = {"format": "%(asctime)s - %(name)s - %(levelname)s - %(message)s"}
        | 
        |     if formatter_to_use == "json":
        |         try:
        |             from python_json_logger import jsonlogger # type: ignore 
        |             formatter_class = "python_json_logger.jsonlogger.JsonFormatter"
        |             formatter_details = {"format": "%(asctime)s %(levelname)s %(name)s %(module)s %(funcName)s %(lineno)d %(message)s"}
        |         except ImportError:
        |             logging.warning("python-json-logger not found. Defaulting to 'simple' log format for shell.")
        |             formatter_to_use = "simple"
        |     
        |     if formatter_to_use != "simple" and formatter_to_use != "json":
        |         logging.warning(f"Invalid shell log format '{LOG_FORMAT_CONF}', defaulting to 'simple'.")
        |         formatter_to_use = "simple"
        |         formatter_class = "logging.Formatter"
        |         formatter_details = {"format": "%(asctime)s - %(name)s - %(levelname)s - %(message)s"}
        | 
        |     LOGGING_CONFIG_DICT = {
        |         "version": 1, "disable_existing_loggers": False,
        |         "formatters": {formatter_to_use: {"()": formatter_class, **formatter_details}},
        |         "handlers": {"console_stderr": {"class": "logging.StreamHandler", "formatter": formatter_to_use, "stream": "ext://sys.stderr"}},
        |         "root": {"handlers": ["console_stderr"], "level": "WARNING"}, 
        |         "loggers": {
        |             "llmbasedos.shell": {"handlers": ["console_stderr"], "level": log_level_int, "propagate": False},
        |             "websockets.client": {"handlers": ["console_stderr"], "level": "WARNING", "propagate": False},
        |             "websockets.protocol": {"handlers": ["console_stderr"], "level": "WARNING", "propagate": False},
        |         }
        |     }
        |     try:
        |         logging.config.dictConfig(LOGGING_CONFIG_DICT)
        |     except Exception as e_log_conf: 
        |         logging.basicConfig(level=log_level_int, format="%(asctime)s - %(name)s - %(levelname)s - %(message)s (fallback)")
        |         logging.error(f"Failed to apply dictConfig for shell logging: {e_log_conf}. Using basicConfig.", exc_info=True)
        | 
        | setup_shell_logging()
        | logger = logging.getLogger("llmbasedos.shell.luca")
        | console = Console(stderr=True, force_terminal=True if sys.stderr.isatty() else False)
        | 
        | 
        | class ShellApp:
        |     def __init__(self, gateway_url: str, console_instance: Console):
        |         self.gateway_url: str = gateway_url
        |         self.console: Console = console_instance
        |         self.mcp_websocket: Optional[WebSocketClientProtocol] = None
        |         self.pending_responses: Dict[str, asyncio.Future] = {}
        |         self.active_streams: Dict[str, asyncio.Queue] = {} # Initialisé ici
        |         self.available_mcp_commands: List[str] = []
        |         self.response_listener_task: Optional[asyncio.Task] = None
        |         self.cwd_state: Path = Path(os.path.expanduser("~")).resolve()
        |         self.is_shutting_down: bool = False
        |         self.prompt_style = PromptStyle.from_dict({
        |             'prompt': 'fg:ansibrightblue bold', 
        |             'path': 'fg:ansigreen bold', 
        |             'disconnected': 'fg:ansired bold',
        |             'error': 'fg:ansired bold'
        |         })
        | 
        |     def get_cwd(self) -> Path: return self.cwd_state
        |     def set_cwd(self, new_path: Path):
        |         try: self.cwd_state = new_path.resolve()
        |         except Exception as e: self.console.print(f"[[error]Error setting CWD to '{new_path}': {e}[/]]")
        | 
        |     def _is_websocket_open(self) -> bool:
        |         return bool(self.mcp_websocket and self.mcp_websocket.open)
        | 
        |     async def _cancel_existing_listener(self):
        |         if self.response_listener_task and not self.response_listener_task.done():
        |             logger.debug("Cancelling previous response listener task.")
        |             self.response_listener_task.cancel()
        |             try: await self.response_listener_task
        |             except asyncio.CancelledError: logger.debug("Previous listener task successfully cancelled.")
        |             except Exception as e_cancel: logger.error(f"Error awaiting previous listener cancellation: {e_cancel}")
        |         self.response_listener_task = None
        | 
        |     async def _start_response_listener(self):
        |         await self._cancel_existing_listener()
        |         if self._is_websocket_open():
        |             self.response_listener_task = asyncio.create_task(self._response_listener_logic(), name="ShellResponseListener")
        |             logger.info("Response listener task started for new connection.")
        |         else:
        |             logger.error("Cannot start response listener: WebSocket is not connected or not open.")
        | 
        |     async def _response_listener_logic(self):
        |         active_ws = self.mcp_websocket
        |         if not active_ws: logger.error("Listener logic: No active WebSocket at start."); return
        |         logger.debug(f"Listener logic running for WebSocket: id={id(active_ws)}")
        |         
        |         try:
        |             async for message_str in active_ws:
        |                 if self.is_shutting_down or self.mcp_websocket != active_ws or not self.mcp_websocket.open:
        |                     logger.info(f"Listener (ws_id={id(active_ws)}): Conditions changed. Exiting loop."); break
        |                 try:
        |                     response = json.loads(message_str)
        |                     logger.debug(f"Gateway RCV (ShellApp): {str(response)[:200]}...")
        |                     response_id = response.get("id")
        | 
        |                     if response_id in self.active_streams:
        |                         queue = self.active_streams[response_id]
        |                         try: await queue.put(response)
        |                         except Exception as e_put_q: logger.error(f"Error putting message for stream {response_id} into queue: {e_put_q}")
        |                         continue 
        | 
        |                     future = self.pending_responses.pop(response_id, None)
        |                     if future:
        |                         if not future.done(): future.set_result(response)
        |                         else: logger.warning(f"Listener: Future for ID {response_id} already done. Ignored.")
        |                     else:
        |                         logger.warning(f"Listener: Rcvd response for unknown/non-stream ID: {response_id}. Data: {str(response)[:100]}")
        |                 except json.JSONDecodeError: logger.error(f"Listener: Invalid JSON from gateway: {message_str}")
        |                 except Exception as e_inner: logger.error(f"Listener: Error processing message: {e_inner}", exc_info=True)
        |         
        |         except (ConnectionClosed, ConnectionClosedOK) as e_ws_closed:
        |             logger.warning(f"Listener: WebSocket connection (id={id(active_ws)}) closed: {e_ws_closed}")
        |         except WebSocketException as e_ws_generic:
        |              logger.error(f"Listener: WebSocketException (id={id(active_ws)}): {e_ws_generic}", exc_info=True)
        |         except asyncio.CancelledError: logger.info(f"Listener: Task (id={id(active_ws)}) explicitly cancelled.")
        |         except Exception as e_outer: logger.error(f"Listener: Task (id={id(active_ws)}) ended with critical error: {e_outer}", exc_info=True)
        |         finally:
        |             logger.info(f"Listener: Task stopped for WebSocket id={id(active_ws)}.")
        |             if self.mcp_websocket == active_ws and (not self.mcp_websocket or not self.mcp_websocket.open):
        |                 self.mcp_websocket = None
        |             for req_id, fut in list(self.pending_responses.items()):
        |                 if not fut.done(): fut.set_exception(RuntimeError(f"Gateway conn lost. Req ID: {req_id}"))
        |                 self.pending_responses.pop(req_id, None)
        |             for req_id, q in list(self.active_streams.items()):
        |                 try: await q.put(RuntimeError("Gateway connection lost (listener ended)."))
        |                 except Exception: pass
        |             self.active_streams.clear()
        | 
        |     async def start_mcp_stream_request(
        |         self, method: str, params: List[Any]
        |     ) -> Tuple[Optional[str], Optional[asyncio.Queue]]:
        |         if self.is_shutting_down or not self._is_websocket_open():
        |             if not await self.ensure_connection():
        |                 self.console.print("[[error]Cannot start stream[/]]: Gateway connection failed.")
        |                 return None, None
        |             if not self._is_websocket_open():
        |                 self.console.print("[[error]Cannot start stream[/]]: Gateway connection still unavailable.")
        |                 return None, None
        |         
        |         request_id = str(uuid.uuid4())
        |         payload = {"jsonrpc": "2.0", "method": method, "params": params, "id": request_id}
        |         
        |         stream_queue: asyncio.Queue = asyncio.Queue(maxsize=100)
        |         self.active_streams[request_id] = stream_queue
        |         logger.info(f"Stream queue created for request ID {request_id}")
        | 
        |         try:
        |             if not self.mcp_websocket: raise ConnectionError("WebSocket is None before send.")
        |             await self.mcp_websocket.send(json.dumps(payload))
        |             logger.debug(f"Stream request {request_id} ({method}) sent.")
        |             return request_id, stream_queue
        |         except Exception as e:
        |             logger.error(f"Failed to send stream request {request_id} ({method}): {e}", exc_info=True)
        |             self.active_streams.pop(request_id, None)
        |             return None, None
        | 
        |     async def ensure_connection(self, force_reconnect: bool = False) -> bool:
        |         if self.is_shutting_down: return False
        |         if not force_reconnect and self._is_websocket_open(): return True
        |         
        |         action_str = "Reconnecting" if force_reconnect or self.mcp_websocket else "Connecting"
        |         logger.info(f"{action_str} to MCP Gateway: {self.gateway_url}")
        |         
        |         await self._cancel_existing_listener()
        |         if self.mcp_websocket and self.mcp_websocket.open:
        |             try: 
        |                 logger.debug(f"Closing existing open WebSocket (id={id(self.mcp_websocket)}) before {action_str.lower()}.")
        |                 await self.mcp_websocket.close(code=1000, reason="Client initiated reconnect")
        |             except WebSocketException as e_close_old: logger.debug(f"Error closing old websocket: {e_close_old}")
        |         self.mcp_websocket = None
        |         
        |         try:
        |             new_ws: WebSocketClientProtocol = await websockets.connect(
        |                 self.gateway_url, open_timeout=10, close_timeout=5,
        |                 ping_interval=20, ping_timeout=20
        |             )
        |             self.mcp_websocket = new_ws
        |             logger.info(f"Successfully established new WebSocket connection (id={id(self.mcp_websocket)}).")
        |             await self._start_response_listener()
        |             
        |             try:
        |                 hello_resp = await self.send_mcp_request(None, "mcp.hello", [])
        |                 if hello_resp and "result" in hello_resp and isinstance(hello_resp["result"], list):
        |                     self.available_mcp_commands = sorted(list(set(hello_resp["result"])))
        |                     logger.debug(f"Fetched {len(self.available_mcp_commands)} MCP commands.")
        |                 else:
        |                     logger.warning(f"Failed to get/parse command list from mcp.hello: {str(hello_resp)[:200]}")
        |                     self.available_mcp_commands = []
        |             except Exception as e_hello:
        |                 logger.error(f"Error calling mcp.hello on connect: {e_hello}", exc_info=True)
        |                 self.available_mcp_commands = []
        |             return True
        |         except ConnectionRefusedError: logger.error(f"Connection refused by Gateway at {self.gateway_url}.")
        |         except asyncio.TimeoutError: logger.error(f"Timeout connecting to Gateway at {self.gateway_url}.")
        |         except WebSocketException as e_ws_conn_main: logger.error(f"WebSocket connection failure to {self.gateway_url}: {e_ws_conn_main}")
        |         except Exception as e_conn_main_other: logger.error(f"Failed to connect to Gateway at {self.gateway_url}: {e_conn_main_other}", exc_info=True)
        |         
        |         if self.mcp_websocket and self.mcp_websocket.open:
        |             try: await self.mcp_websocket.close()
        |             except: pass
        |         self.mcp_websocket = None; await self._cancel_existing_listener()
        |         return False
        | 
        |     async def send_mcp_request(
        |         self, request_id_override: Optional[str], method: str, params: List[Any], timeout: float = 20.0
        |     ) -> Optional[Dict[str, Any]]:
        |         if self.is_shutting_down: 
        |             return {"jsonrpc": "2.0", "id": request_id_override, "error": {"code": -32000, "message": "Shell is shutting down."}}
        |         
        |         if not self._is_websocket_open():
        |             logger.warning(f"send_mcp_request: No active connection for '{method}'. Attempting to connect...")
        |             if not await self.ensure_connection():
        |                  return {"jsonrpc": "2.0", "id": request_id_override, "error": {"code": -32003, "message": "Gateway connection failed."}}
        |             if not self._is_websocket_open():
        |                  return {"jsonrpc": "2.0", "id": request_id_override, "error": {"code": -32003, "message": "Gateway connection still unavailable."}}
        | 
        |         req_id = request_id_override if request_id_override is not None else str(uuid.uuid4())
        |         payload = {"jsonrpc": "2.0", "method": method, "params": params, "id": req_id}
        |         
        |         current_loop = asyncio.get_running_loop(); future: asyncio.Future = current_loop.create_future()
        |         self.pending_responses[req_id] = future
        | 
        |         try:
        |             logger.debug(f"ShellApp SEND (ID {req_id}): {method} {str(params)[:100]}...")
        |             if not self.mcp_websocket: raise ConnectionError("WebSocket is None before send.")
        |             await self.mcp_websocket.send(json.dumps(payload))
        |             response = await asyncio.wait_for(future, timeout=timeout)
        |             return response
        |         except asyncio.TimeoutError:
        |             logger.error(f"Timeout (ID {req_id}) for {method}.")
        |             popped_future = self.pending_responses.pop(req_id, None)
        |             if popped_future and not popped_future.done(): popped_future.cancel()
        |             return {"jsonrpc": "2.0", "id": req_id, "error": {"code": -32000, "message": "Request timed out."}}
        |         except (ConnectionClosed, ConnectionClosedOK, WebSocketException) as e_ws_send_err: 
        |             logger.error(f"Connection error during send/wait for {method} (ID {req_id}): {e_ws_send_err}")
        |             self.pending_responses.pop(req_id, None)
        |             ws_instance_from_exc = getattr(e_ws_send_err, 'ws_client', getattr(e_ws_send_err, 'protocol', self.mcp_websocket))
        |             if self.mcp_websocket and self.mcp_websocket == ws_instance_from_exc :
        |                  try: await self.mcp_websocket.close()
        |                  except: pass
        |                  self.mcp_websocket = None 
        |             return {"jsonrpc": "2.0", "id": req_id, "error": {"code": -32001, "message": f"Gateway connection error: {e_ws_send_err}"}}
        |         except Exception as e_send_req:
        |             logger.error(f"Error sending MCP request {method} (ID {req_id}): {e_send_req}", exc_info=True)
        |             self.pending_responses.pop(req_id, None)
        |             return {"jsonrpc": "2.0", "id": req_id, "error": {"code": -32002, "message": f"Shell client error sending request: {e_send_req}"}}
        | 
        |     async def _rich_format_mcp_fs_list(self, result: List[Dict[str,Any]], request_path: str):
        |         # S'assurer que Table est importé ici ou est un attribut de self.console si elle vient de Rich
        |         from rich.table import Table # Import local pour être sûr
        |         table = Table(show_header=True, header_style="bold cyan", title=f"Contents of {escape(request_path) if request_path else 'directory'}")
        |         table.add_column("Type", width=10); table.add_column("Size", justify="right", width=10)
        |         table.add_column("Modified", width=20); table.add_column("Name")
        |         for item in sorted(result, key=lambda x: (x.get('type') != 'directory', str(x.get('name','')).lower())):
        |             item_type = item.get("type", "other")
        |             color = "blue" if item_type == "directory" else "green" if item_type == "file" else "magenta" if item_type == "symlink" else "bright_black"
        |             size_val = item.get("size", -1); size_str = ""
        |             if item_type == "directory": size_str = "[DIR]"
        |             elif isinstance(size_val, int) and size_val >= 0:
        |                 num = float(size_val); units = ["B", "KB", "MB", "GB", "TB"]; i = 0
        |                 while num >= 1024 and i < len(units) - 1: num /= 1024.0; i += 1
        |                 size_str = f"{num:.1f}{units[i]}" if i > 0 else f"{int(num)}{units[i]}"
        |             else: size_str = "N/A"
        |             mod_iso = item.get("modified_at", "")
        |             mod_display = datetime.fromisoformat(mod_iso.replace("Z", "+00:00")).strftime("%Y-%m-%d %H:%M:%S") if mod_iso else "N/A"
        |             table.add_row(Text(item_type, style=color), size_str, mod_display, Text(escape(item.get("name", "?")), style=color))
        |         self.console.print(table)
        | 
        |     async def _rich_format_mcp_fs_read(self, result: Dict[str,Any]):
        |         content = result.get("content", ""); encoding = result.get("encoding"); mime_type = result.get("mime_type", "application/octet-stream")
        |         if encoding == "text":
        |             lexer = "text"; simple_mime = mime_type.split('/')[-1].split('+')[0].lower()
        |             known_lexers = ["json", "xml", "python", "markdown", "html", "css", "javascript", "yaml", "c", "cpp", "java", "go", "rust", "php", "ruby", "perl", "sql", "ini", "toml", "diff", "dockerfile"]
        |             shell_lexers = ["bash", "sh", "zsh", "fish", "powershell", "batch"]
        |             if simple_mime in known_lexers: lexer = simple_mime
        |             elif simple_mime in ["x-yaml", "vnd.yaml"]: lexer = "yaml"
        |             elif simple_mime in ["x-python", "x-python3"]: lexer = "python"
        |             elif simple_mime in ["x-shellscript", "x-sh", "x-bash"]: lexer = "bash"
        |             elif simple_mime in shell_lexers : lexer = simple_mime
        |             self.console.print(Syntax(content, lexer, theme="native", line_numbers=True, word_wrap=True, background_color="default"))
        |         elif encoding == "base64": self.console.print(f"[yellow]Base64 (MIME: {mime_type}):[/yellow]\n{escape(content[:500])}{'...' if len(content)>500 else ''}")
        |         else: self.console.print(escape(str(result)))
        | 
        |     async def _format_and_print_mcp_response(self, mcp_method: str, response: Optional[Dict[str,Any]], request_path_for_ls: Optional[str] = None):
        |         if not response: self.console.print("[[error]No response or connection failed.[/]]"); return
        |         if "error" in response:
        |             err = response["error"]
        |             self.console.print(f"[[error]MCP Error (Code {err.get('code')})[/]]: {escape(str(err.get('message')))}")
        |             if "data" in err: self.console.print(Syntax(json.dumps(err['data'],indent=2),"json",theme="native",background_color="default"))
        |         elif "result" in response:
        |             result = response["result"]
        |             if mcp_method == "mcp.fs.list" and isinstance(result, list):
        |                 await self._rich_format_mcp_fs_list(result, request_path_for_ls or "current directory")
        |             elif mcp_method == "mcp.fs.read" and isinstance(result, dict):
        |                 await self._rich_format_mcp_fs_read(result)
        |             elif isinstance(result, (dict, list)):
        |                 self.console.print(Syntax(json.dumps(result,indent=2),"json",theme="native",line_numbers=True,background_color="default"))
        |             else: self.console.print(escape(str(result)))
        |         else: self.console.print(Syntax(json.dumps(response,indent=2),"json",theme="native",background_color="default"))
        | 
        |     async def handle_command_line(self, command_line_str_raw: str):
        |         logger.info(f"SHELL RCV RAW: '{command_line_str_raw}'")
        |         path_disp = str(self.get_cwd()); home_str = str(Path.home())
        |         if path_disp.startswith(home_str) and path_disp != home_str : path_disp = "~" + path_disp[len(home_str):]
        |         prompt_connected_str = f"{path_disp} luca> "; prompt_disconnected_str = f"[Disconnected] {path_disp} luca> "
        |         command_line_str = command_line_str_raw; stripped_prompt_prefix = "" # Pour le log
        |         
        |         if command_line_str_raw.startswith(prompt_disconnected_str):
        |             stripped_prompt_prefix = prompt_disconnected_str
        |             command_line_str = command_line_str_raw[len(prompt_disconnected_str):].lstrip()
        |         elif command_line_str_raw.startswith(prompt_connected_str):
        |             stripped_prompt_prefix = prompt_connected_str
        |             command_line_str = command_line_str_raw[len(prompt_connected_str):].lstrip()
        |         
        |         if stripped_prompt_prefix: logger.info(f"SHELL STRIPPED CMD: '{command_line_str}' (using prefix '{stripped_prompt_prefix}')")
        |         else: # Si aucun prompt standard n'est trouvé, on lstrip juste
        |              command_line_str = command_line_str_raw.lstrip()
        |              if command_line_str_raw != command_line_str: logger.info(f"SHELL STRIPPED (no specific prompt): '{command_line_str}'")
        | 
        |         if not command_line_str.strip(): logger.debug("Command line empty after strip."); return
        |         try: parts = shlex.split(command_line_str)
        |         except ValueError as e_shlex: self.console.print(f"[[error]Parsing error[/]]: {escape(str(e_shlex))}"); return
        |         if not parts: logger.debug("Empty command after shlex.split."); return
        | 
        |         cmd_name = parts[0]; cmd_args_list = parts[1:]
        |         logger.info(f"SHELL FINAL CMD_NAME: '{cmd_name}', ARGS: {cmd_args_list}")
        | 
        |         if hasattr(builtin_cmds, f"cmd_{cmd_name}"):
        |             handler = getattr(builtin_cmds, f"cmd_{cmd_name}")
        |             try: await handler(cmd_args_list, self)
        |             except Exception as e_builtin: logger.error(f"Error in builtin '{cmd_name}': {e_builtin}", exc_info=True); self.console.print(f"[[error]Error in '{cmd_name}'[/]]: {escape(str(e_builtin))}")
        |             return
        | 
        |         mcp_full_method = cmd_name; parsed_params: List[Any] = []; request_path_for_ls: Optional[str] = None
        |         
        |         # Nouvelle logique de parsing des paramètres pour les appels MCP directs
        |         if len(cmd_args_list) == 1:
        |             try: # Essayer de parser le seul argument comme une liste JSON complète
        |                 potential_params_list = json.loads(cmd_args_list[0])
        |                 if isinstance(potential_params_list, list):
        |                     parsed_params = potential_params_list # Utiliser cette liste directement
        |                     if mcp_full_method == "mcp.fs.list" and parsed_params and isinstance(parsed_params[0], str):
        |                         request_path_for_ls = parsed_params[0]
        |                 else: # Ce n'était pas une liste JSON, traiter comme un seul argument
        |                     parsed_params.append(potential_params_list) # Si c'est un nombre, bool, etc.
        |             except json.JSONDecodeError: # Pas un JSON valide, traiter comme une chaîne
        |                 arg_s = cmd_args_list[0]
        |                 expanded_arg_s = os.path.expanduser(arg_s) if arg_s.startswith('~') else arg_s
        |                 if mcp_full_method.startswith("mcp.fs.") and not os.path.isabs(expanded_arg_s):
        |                     abs_path = (self.get_cwd() / expanded_arg_s).resolve()
        |                     parsed_params.append(str(abs_path))
        |                     if mcp_full_method == "mcp.fs.list": request_path_for_ls = str(abs_path)
        |                 else:
        |                     parsed_params.append(expanded_arg_s)
        |                     if mcp_full_method == "mcp.fs.list" and os.path.isabs(expanded_arg_s): request_path_for_ls = expanded_arg_s
        |         else: # Plusieurs arguments, les traiter individuellement
        |             for arg_s in cmd_args_list:
        |                 try: parsed_params.append(json.loads(arg_s))
        |                 except json.JSONDecodeError:
        |                     expanded_arg_s = os.path.expanduser(arg_s) if arg_s.startswith('~') else arg_s
        |                     if mcp_full_method.startswith("mcp.fs.") and not os.path.isabs(expanded_arg_s) and parsed_params == []: # Seulement pour le premier arg chemin
        |                         abs_path = (self.get_cwd() / expanded_arg_s).resolve()
        |                         parsed_params.append(str(abs_path))
        |                         if mcp_full_method == "mcp.fs.list": request_path_for_ls = str(abs_path)
        |                     else:
        |                         parsed_params.append(expanded_arg_s)
        |                         if mcp_full_method == "mcp.fs.list" and os.path.isabs(expanded_arg_s) and parsed_params == [expanded_arg_s]: request_path_for_ls = expanded_arg_s
        |         
        |         logger.info(f"SHELL PARSED PARAMS for {mcp_full_method}: {parsed_params}")
        |         
        |         if mcp_full_method == "mcp.llm.chat": 
        |             self.console.print(Text("Use the 'llm' command for interactive chat streaming. Ex: llm \"Your prompt\"", style="yellow")); return
        | 
        |         response = await self.send_mcp_request(None, mcp_full_method, parsed_params)
        |         await self._format_and_print_mcp_response(mcp_full_method, response, request_path_for_ls=request_path_for_ls)
        | 
        |     async def run_repl(self):
        |         if not await self.ensure_connection(force_reconnect=True):
        |             self.console.print("[[error]Failed to connect[/]] to gateway on startup. Try 'connect' or check gateway.")
        | 
        |         class AppCompleter(Completer):
        |             def __init__(self, shell_app_instance: 'ShellApp'): self.shell_app = shell_app_instance
        |             def get_completions(self, document, complete_event):
        |                 text_before = document.text_before_cursor.lstrip(); words = text_before.split()
        |                 if not words or (len(words) == 1 and not text_before.endswith(' ')):
        |                     current_w = words[0] if words else ""
        |                     all_cmds = sorted(list(set(builtin_cmds.BUILTIN_COMMAND_LIST + self.shell_app.available_mcp_commands)))
        |                     for cmd_s in all_cmds:
        |                         if cmd_s.startswith(current_w): yield Completion(cmd_s, start_position=-len(current_w))
        | 
        |         pt_session = PromptSession(history=FileHistory(str(SHELL_HISTORY_FILE)),
        |                                    auto_suggest=AutoSuggestFromHistory(),
        |                                    completer=AppCompleter(self), style=self.prompt_style, enable_suspend=True)
        |         
        |         while not self.is_shutting_down:
        |             try:
        |                 path_disp = str(self.get_cwd()); home_str = str(Path.home())
        |                 if path_disp.startswith(home_str) and path_disp != home_str : path_disp = "~" + path_disp[len(home_str):]
        |                 
        |                 prompt_list_parts = [('class:path', f"{path_disp} "), ('class:prompt', 'luca> ')]
        |                 if not self._is_websocket_open(): prompt_list_parts.insert(0, ('class:disconnected', "[Disconnected] "))
        |                 
        |                 cmd_line_str = await pt_session.prompt_async(prompt_list_parts)
        |                 await self.handle_command_line(cmd_line_str)
        |             except KeyboardInterrupt: self.console.print() ; continue
        |             except EOFError: self.console.print("Exiting luca-shell (EOF)..."); break
        |             except Exception as e_repl_loop:
        |                 logger.critical(f"Critical error in REPL loop: {e_repl_loop}", exc_info=True)
        |                 self.console.print(f"[[error]REPL Error[/]]: {escape(str(e_repl_loop))}.")
        |         
        |         await self.shutdown()
        | 
        |     async def shutdown(self):
        |         if self.is_shutting_down: return
        |         self.is_shutting_down = True
        |         logger.info("ShellApp shutting down...")
        |         await self._cancel_existing_listener()
        |         
        |         if self.mcp_websocket and self.mcp_websocket.open:
        |             logger.info("Closing WebSocket connection to gateway...");
        |             try: await self.mcp_websocket.close(code=1000, reason="Client shutdown")
        |             except Exception as e_ws_close: logger.debug(f"Exception closing websocket on shutdown: {e_ws_close}")
        |         self.mcp_websocket = None
        |         logger.info("ShellApp shutdown complete.")
        | 
        | # --- Point d'Entrée Principal ---
        | if __name__ == "__main__":
        |     app = ShellApp(GATEWAY_WS_URL_CONF, console)
        |     main_event_loop = asyncio.get_event_loop()
        |     
        |     _should_exit_main_event = asyncio.Event()
        |     def _main_signal_handler(sig, frame):
        |         logger.info(f"Signal {signal.Signals(sig).name} received by main, setting shutdown event...")
        |         if not main_event_loop.is_closed():
        |             main_event_loop.call_soon_threadsafe(_should_exit_main_event.set)
        | 
        |     if os.name == 'posix':
        |         signal.signal(signal.SIGINT, _main_signal_handler)
        |         signal.signal(signal.SIGTERM, _main_signal_handler)
        |     else: 
        |         logger.info("Signal handlers for SIGINT/SIGTERM not set (non-POSIX OS). Relying on KeyboardInterrupt/EOFError.")
        | 
        |     async def main_with_shutdown_wrapper():
        |         repl_task = main_event_loop.create_task(app.run_repl())
        |         shutdown_signal_task = main_event_loop.create_task(_should_exit_main_event.wait())
        |         done, pending = await asyncio.wait([repl_task, shutdown_signal_task], return_when=asyncio.FIRST_COMPLETED)
        |         if shutdown_signal_task in done:
        |             logger.info("Shutdown event set, cancelling REPL task.")
        |             if not repl_task.done(): repl_task.cancel(); await asyncio.gather(repl_task, return_exceptions=True)
        |         if not app.is_shutting_down: await app.shutdown()
        | 
        |     try:
        |         main_event_loop.run_until_complete(main_with_shutdown_wrapper())
        |     except Exception as e_shell_main_exc:
        |         logger.critical(f"Luca Shell (main) crashed OUTSIDE REPL: {e_shell_main_exc}", exc_info=True)
        |         console.print(f"[[error]Shell crashed fatally[/]]: {escape(str(e_shell_main_exc))}")
        |     finally:
        |         logger.info("Luca Shell (main) final cleanup starting...")
        |         if hasattr(app, 'is_shutting_down') and not app.is_shutting_down:
        |             logger.info("Running app.shutdown() in final finally block.")
        |             # ... (logique de fermeture de boucle affinée) ...
        |         logger.info("Luca Shell (main) process finished.")
        --- Fin Contenu ---
      Fichier: requirements.txt
        --- Début Contenu (ascii) ---
        | prompt_toolkit>=3.0.0
        | websockets>=10.0,<11.0  # Force les versions 10.x
        | rich>=10.0.0
        | python-json-logger>=2.0.0
        --- Fin Contenu ---
      Fichier: shell_utils.py
        --- Début Contenu (utf-8) ---
        | # llmbasedos_src/shell/shell_utils.py
        | import asyncio
        | import json
        | import uuid
        | import logging
        | import sys # Pour sys.stdout.flush()
        | from typing import List, Dict, Any, Optional
        | 
        | # Importer directement les types nécessaires de websockets (pas besoin pour cette fonction si app gère le ws)
        | # from websockets.client import WebSocketClientProtocol # Non utilisé directement ici
        | # from websockets.exceptions import ConnectionClosed, ConnectionClosedOK, WebSocketException # Non utilisé directement ici
        | 
        | from rich.console import Console
        | from rich.text import Text
        | from rich.syntax import Syntax
        | from rich.markup import escape # Pour échapper les messages d'erreur
        | 
        | # Import TYPE_CHECKING pour l'annotation de type de ShellApp
        | from typing import TYPE_CHECKING
        | if TYPE_CHECKING:
        |     from .luca import ShellApp # Pour l'annotation de type 'app'
        | 
        | logger = logging.getLogger("llmbasedos.shell.utils")
        | 
        | async def stream_llm_chat_to_console(
        |     app: 'ShellApp', # Instance de ShellApp qui gère la connexion et les queues
        |     messages: List[Dict[str, str]],
        |     llm_options: Optional[Dict[str, Any]] = None
        | ) -> Optional[str]: # Returns full response text or None on error/no connection
        |     """
        |     Initiates an mcp.llm.chat stream request via ShellApp,
        |     reads chunks from the associated asyncio.Queue, and prints them to the console.
        |     """
        |     
        |     actual_llm_options = {"stream": True, **(llm_options or {})}
        |     # Forcer stream=True car cette fonction est pour le streaming vers la console
        |     actual_llm_options["stream"] = True 
        | 
        |     # Demander à ShellApp d'initier le stream et de nous donner l'ID de requête et la queue
        |     request_id, stream_queue = await app.start_mcp_stream_request(
        |         "mcp.llm.chat", [messages, actual_llm_options]
        |     )
        | 
        |     if not request_id or not stream_queue:
        |         # start_mcp_stream_request a déjà dû afficher une erreur si la connexion a échoué
        |         logger.error("LLM Stream: Failed to initiate stream request via ShellApp.")
        |         return None
        | 
        |     app.console.print(Text("Assistant: ", style="bold blue"), end="")
        |     full_response_text = ""
        |     
        |     try:
        |         while True: # Boucle pour consommer les messages de la queue
        |             response_json: Optional[Dict[str, Any]] = None # Pour la portée
        |             try:
        |                 # Obtenir le prochain chunk depuis la queue avec un timeout
        |                 response_json = await asyncio.wait_for(stream_queue.get(), timeout=120.0) # Timeout de 2min par chunk
        |                 stream_queue.task_done() # Indiquer que l'item a été traité
        | 
        |             except asyncio.TimeoutError:
        |                 app.console.print("\n[[error]LLM Stream[/]]: Timeout waiting for response chunk.")
        |                 logger.error(f"LLM Stream: Timeout (ID {request_id}).")
        |                 break # Sortir de la boucle de stream
        |             
        |             # Vérifier si le listener a mis une exception dans la queue (ex: connexion perdue)
        |             if isinstance(response_json, Exception): # Le listener peut mettre une Exception pour signaler la fin
        |                 logger.error(f"LLM Stream: Received exception from queue (ID {request_id}): {response_json}")
        |                 app.console.print(f"\n[[error]LLM Stream Error[/]]: {escape(str(response_json))}")
        |                 break
        | 
        |             # S'assurer que response_json est bien un dictionnaire (pour mypy et la robustesse)
        |             if not isinstance(response_json, dict):
        |                 logger.error(f"LLM Stream: Received non-dict item from queue (ID {request_id}): {type(response_json)}")
        |                 app.console.print(f"\n[[error]LLM Stream Error[/]]: Received unexpected data type from gateway.")
        |                 break
        | 
        |             logger.debug(f"STREAM_UTIL RCV from Queue (Expected ID {request_id}, Got ID {response_json.get('id')}): {str(response_json)[:200]}")
        |             
        |             # Normalement, le listener dans ShellApp ne devrait mettre dans la queue que les messages pour cet ID.
        |             # Mais une vérification ici peut être une sécurité additionnelle.
        |             if response_json.get("id") != request_id:
        |                 logger.warning(f"STREAM_UTIL: Mismatched ID in stream queue! Expected {request_id}, got {response_json.get('id')}. Ignoring chunk.")
        |                 continue
        | 
        |             if "error" in response_json:
        |                 err = response_json["error"]
        |                 app.console.print(f"\n[[error]LLM Error (Code {err.get('code')})[/]]: {escape(str(err.get('message')))}")
        |                 if err.get('data'):
        |                      app.console.print(Syntax(json.dumps(err['data'], indent=2), "json", theme="native", background_color="default"))
        |                 break # Erreur termine le stream
        | 
        |             result = response_json.get("result", {})
        |             if result.get("type") == "llm_chunk":
        |                 llm_api_chunk = result.get("content", {}) # C'est le payload brut de l'API LLM
        |                 delta = ""
        |                 if isinstance(llm_api_chunk, dict): # Structure type OpenAI
        |                     delta = llm_api_chunk.get("choices", [{}])[0].get("delta", {}).get("content", "")
        |                 
        |                 if delta:
        |                     app.console.print(delta, end="")
        |                     sys.stdout.flush() # Forcer l'affichage immédiat
        |                     full_response_text += delta
        |             elif result.get("type") == "llm_stream_end":
        |                 app.console.print() # Nouvelle ligne finale
        |                 logger.info(f"LLM Stream (ID {request_id}) ended successfully. Total length: {len(full_response_text)}")
        |                 break # Fin normale du stream
        |             else:
        |                 logger.warning(f"STREAM_UTIL: Unknown result type from queue: '{result.get('type')}' for ID {request_id}")
        |         
        |     except Exception as e_outer_stream: # Erreur inattendue dans la logique de la boucle while
        |         logger.error(f"STREAM_UTIL: General error processing stream (ID {request_id}): {e_outer_stream}", exc_info=True)
        |         app.console.print(f"\n[[error]LLM stream processing error[/]]: {escape(str(e_outer_stream))}")
        |     finally:
        |         # Le `finally` est pour le `try` qui entoure la boucle `while`.
        |         # S'assurer que la queue est retirée des streams actifs de ShellApp si ce n'est pas déjà fait.
        |         if request_id and request_id in app.active_streams: # Vérifier si request_id a été défini
        |             logger.debug(f"Cleaning up stream queue for request ID {request_id} in stream_llm_chat_to_console's finally block.")
        |             # Vider la queue pour éviter que des messages restants ne soient lus par une future instance
        |             # ou que le listener ne bloque en essayant de mettre dans une queue pleine.
        |             queue_to_clean = app.active_streams.get(request_id)
        |             if queue_to_clean:
        |                 while not queue_to_clean.empty():
        |                     try: queue_to_clean.get_nowait(); queue_to_clean.task_done()
        |                     except asyncio.QueueEmpty: break
        |                     except Exception: break # Pour les autres erreurs de queue
        |             app.active_streams.pop(request_id, None) # Enlever la référence de la queue des streams actifs
        |             
        |     return full_response_text
        --- Fin Contenu ---

      Répertoire: ./llmbasedos_src/shell/tests
        Fichier: test_cd.py
          --- Début Contenu (ascii) ---
          | # llmbasedos/shell/tests/test_cd.py
          | import pytest
          | # Basic placeholder for tests.
          | # Actual tests would require more infrastructure:
          | # - Mocking websockets.connect and MCP responses
          | # - Or, setting up a test instance of the gateway and fs_server.
          | 
          | # @pytest.mark.asyncio
          | # async def test_cd_to_existing_dir(mock_mcp_gateway, tmp_path):
          | #     # mock_mcp_gateway would simulate gateway responses
          | #     # tmp_path is a pytest fixture for temporary directory
          | #     # shell_app = ShellApp("ws://dummy")
          | #     # initial_cwd = shell_app.get_cwd()
          | #     # test_dir = tmp_path / "testdir"
          | #     # test_dir.mkdir()
          | 
          | #     # Simulate mcp.fs.list success for test_dir
          | #     mock_mcp_gateway.add_response_handler(
          | #         "mcp.fs.list",
          | #         lambda params: {"id": "1", "result": []} if params[0] == str(test_dir) else {"id": "1", "error": {"code": -1, "message": "not found"}}
          | #     )
          |     
          | #     # await shell_app.handle_command_line(f"cd {str(test_dir)}")
          | #     # assert shell_app.get_cwd() == test_dir.resolve()
          |     
          | #     # await shell_app.handle_command_line(f"cd ..")
          | #     # assert shell_app.get_cwd() == tmp_path.resolve()
          |     
          | #     # await shell_app.handle_command_line(f"cd {str(initial_cwd)}") # Back to original
          | #     # assert shell_app.get_cwd() == initial_cwd
          | 
          |     pass # Replace with actual tests
          --- Fin Contenu ---

    Répertoire: ./llmbasedos_src/gateway
      Fichier: __init__.py
        --- Début Contenu (ascii) ---
        | # llmbasedos/gateway/__init__.py
        | import logging
        | import logging.config
        | import os
        | 
        | from .config import LOGGING_CONFIG, LOG_LEVEL_STR # Supprimer LOG_FORMAT
        | 
        | # Centralized logging configuration for the gateway module
        | # This should be called once when the gateway starts.
        | # main.py will call setup_logging().
        | 
        | def setup_gateway_logging():
        |     log_level_int = logging.getLevelName(LOG_LEVEL_STR)
        |     
        |     formatter_class = "python_json_logger.jsonlogger.JsonFormatter" if LOG_FORMAT == "json" else "logging.Formatter"
        |     formatter_config = {
        |         "format": "%(asctime)s %(levelname)s %(name)s %(module)s %(funcName)s %(lineno)d %(message)s"
        |     } if LOG_FORMAT == "json" else {
        |         "format": "%(asctime)s - %(name)s - %(levelname)s - %(message)s"
        |     }
        | 
        |     LOGGING_CONFIG = {
        |         "version": 1,
        |         "disable_existing_loggers": False,
        |         "formatters": {
        |             LOG_FORMAT: {"()": formatter_class, **formatter_config}
        |         },
        |         "handlers": {
        |             "console": {
        |                 "class": "logging.StreamHandler",
        |                 "formatter": LOG_FORMAT,
        |                 "stream": "ext://sys.stdout" # Or sys.stderr
        |             }
        |         },
        |         "root": { # Catch-all for other libraries if not configured
        |             "handlers": ["console"],
        |             "level": "WARNING",
        |         },
        |         "loggers": {
        |             "llmbasedos.gateway": {"handlers": ["console"], "level": log_level_int, "propagate": False},
        |             "uvicorn": {"handlers": ["console"], "level": "INFO", "propagate": False},
        |             "uvicorn.error": {"handlers": ["console"], "level": "INFO", "propagate": False},
        |             "uvicorn.access": {"handlers": ["console"], "level": "INFO", "propagate": False}, # Access logs
        |             "fastapi": {"handlers": ["console"], "level": "INFO", "propagate": False},
        |             "websockets": {"handlers": ["console"], "level": "INFO", "propagate": False}, # For client part
        |         }
        |     }
        |     logging.config.dictConfig(LOGGING_CONFIG)
        |     logger = logging.getLogger("llmbasedos.gateway")
        |     logger.info(f"llmbasedos.gateway package initialized. Log level: {LOG_LEVEL_STR}")
        | 
        | # setup_gateway_logging() # Call from main.py on startup instead of module import time
        --- Fin Contenu ---
      Fichier: auth.py
        --- Début Contenu (ascii) ---
        | # llmbasedos/gateway/auth.py
        | import logging
        | from datetime import datetime, timedelta, timezone
        | from pathlib import Path
        | from typing import Dict, Any, Optional, Tuple, List
        | import hashlib
        | import yaml # For loading licence tiers config
        | 
        | from fastapi import WebSocket # For client context (IP for rate limiting)
        | from pydantic import BaseModel, Field, field_validator
        | 
        | from .config import (
        |     LICENCE_FILE_PATH, LICENCE_TIERS_CONFIG_PATH, DEFAULT_LICENCE_TIERS,
        |     JSONRPC_AUTH_ERROR, JSONRPC_RATE_LIMIT_ERROR, JSONRPC_PERMISSION_DENIED_ERROR,
        |     JSONRPC_LLM_QUOTA_EXCEEDED_ERROR, JSONRPC_LLM_MODEL_NOT_ALLOWED_ERROR
        | )
        | 
        | logger = logging.getLogger("llmbasedos.gateway.auth")
        | 
        | # In-memory store for rate limiting & LLM token usage.
        | # Key: client_identifier (e.g., licence_key_hash or remote_addr for FREE tier)
        | # Value structure:
        | # {
        | #   "requests": [timestamp1, timestamp2, ...],
        | #   "llm_tokens": {"YYYY-MM-DD": daily_token_count_for_this_client}
        | # }
        | CLIENT_USAGE_RECORDS: Dict[str, Dict[str, Any]] = {}
        | LOADED_LICENCE_TIERS: Dict[str, Dict[str, Any]] = {}
        | 
        | 
        | class LicenceDetails(BaseModel):
        |     tier: str = "FREE"
        |     key_id: Optional[str] = None
        |     user_identifier: Optional[str] = None
        |     expires_at: Optional[datetime] = None
        |     is_valid: bool = False
        |     raw_content: Optional[str] = None # For auditing
        | 
        |     # Tier-specific settings, populated by _apply_tier_settings
        |     rate_limit_requests: int = 0
        |     rate_limit_window_seconds: int = 3600
        |     allowed_capabilities: List[str] = Field(default_factory=list)
        |     llm_access: bool = False
        |     allowed_llm_models: List[str] = Field(default_factory=list) # Can contain "*"
        |     max_llm_tokens_per_request: int = 0
        |     max_llm_tokens_per_day: int = 0
        | 
        |     def __init__(self, **data: Any):
        |         super().__init__(**data)
        |         self._apply_tier_settings()
        | 
        |     def _apply_tier_settings(self):
        |         global LOADED_LICENCE_TIERS
        |         if not LOADED_LICENCE_TIERS: # Ensure tiers are loaded
        |             _load_licence_tiers_config()
        |         
        |         # Fallback to FREE tier config if current tier is unknown or invalid after parsing
        |         tier_config = LOADED_LICENCE_TIERS.get(self.tier, LOADED_LICENCE_TIERS.get("FREE", DEFAULT_LICENCE_TIERS["FREE"]))
        |         
        |         self.rate_limit_requests = tier_config.get("rate_limit_requests", 0)
        |         self.rate_limit_window_seconds = tier_config.get("rate_limit_window_seconds", 3600)
        |         self.allowed_capabilities = tier_config.get("allowed_capabilities", [])
        |         self.llm_access = tier_config.get("llm_access", False)
        |         self.allowed_llm_models = tier_config.get("allowed_llm_models", [])
        |         self.max_llm_tokens_per_request = tier_config.get("max_llm_tokens_per_request", 0)
        |         self.max_llm_tokens_per_day = tier_config.get("max_llm_tokens_per_day", 0)
        | 
        |     @field_validator('expires_at', mode='before')
        |     @classmethod
        |     def ensure_timezone_aware(cls, v):
        |         if isinstance(v, datetime) and v.tzinfo is None:
        |             return v.replace(tzinfo=timezone.utc) # Assume UTC if naive
        |         return v
        | 
        | 
        | _CACHED_LICENCE: Optional[LicenceDetails] = None
        | _LICENCE_FILE_MTIME: Optional[float] = None
        | _LICENCE_TIERS_FILE_MTIME: Optional[float] = None
        | 
        | def _load_licence_tiers_config():
        |     """Loads licence tier definitions from YAML, merging with defaults."""
        |     global LOADED_LICENCE_TIERS, _LICENCE_TIERS_FILE_MTIME
        |     
        |     current_mtime = None
        |     config_exists = False
        |     if LICENCE_TIERS_CONFIG_PATH.exists():
        |         try:
        |             current_mtime = LICENCE_TIERS_CONFIG_PATH.stat().st_mtime
        |             config_exists = True
        |         except FileNotFoundError: # Should not happen if exists() is true, but defensive
        |             pass
        | 
        |     if LOADED_LICENCE_TIERS and current_mtime == _LICENCE_TIERS_FILE_MTIME:
        |         return # Already loaded and up-to-date
        | 
        |     logger.info("Loading/Re-loading licence tier definitions...")
        |     LOADED_LICENCE_TIERS = DEFAULT_LICENCE_TIERS.copy() # Start with defaults
        | 
        |     if config_exists:
        |         try:
        |             with LICENCE_TIERS_CONFIG_PATH.open('r') as f:
        |                 custom_tiers = yaml.safe_load(f)
        |             if isinstance(custom_tiers, dict):
        |                 # Deep merge custom tiers into defaults (simple dict update here, can be more granular)
        |                 for tier_name, tier_conf in custom_tiers.items():
        |                     if tier_name in LOADED_LICENCE_TIERS and isinstance(LOADED_LICENCE_TIERS[tier_name], dict) and isinstance(tier_conf, dict):
        |                         LOADED_LICENCE_TIERS[tier_name].update(tier_conf)
        |                     else:
        |                         LOADED_LICENCE_TIERS[tier_name] = tier_conf
        |                 logger.info(f"Successfully loaded and merged custom tiers from {LICENCE_TIERS_CONFIG_PATH}")
        |             else:
        |                 logger.warning(f"Custom tiers config file {LICENCE_TIERS_CONFIG_PATH} is not a valid YAML dictionary. Using defaults.")
        |         except yaml.YAMLError as e:
        |             logger.error(f"Error parsing licence tiers YAML {LICENCE_TIERS_CONFIG_PATH}: {e}. Using defaults/previous.")
        |         except Exception as e:
        |             logger.error(f"Error loading licence tiers file {LICENCE_TIERS_CONFIG_PATH}: {e}. Using defaults/previous.")
        |     else:
        |         logger.info(f"Licence tiers config file {LICENCE_TIERS_CONFIG_PATH} not found. Using default tiers.")
        |     
        |     _LICENCE_TIERS_FILE_MTIME = current_mtime
        | 
        | 
        | def _parse_licence_key_content(content: str) -> LicenceDetails:
        |     """Parses the raw licence key string. Robust parsing and verification needed for prod."""
        |     try:
        |         # Example format: TIER:USER_ID:EXPIRY_YYYY-MM-DD (or from a signed JWT, etc.)
        |         # For now, assume simple YAML or JSON content in the key file for flexibility
        |         key_data = yaml.safe_load(content) # Allows more structured key files
        |         if not isinstance(key_data, dict):
        |             raise ValueError("Licence key content is not a valid YAML/JSON dictionary.")
        | 
        |         tier = str(key_data.get("tier", "FREE")).upper()
        |         user_id = str(key_data.get("user_id", "anonymous"))
        |         expiry_str = key_data.get("expires_at") # Expects ISO format string or None
        |         key_id_hash = hashlib.sha256(content.encode()).hexdigest()[:16]
        | 
        |         expires_at_dt: Optional[datetime] = None
        |         if expiry_str:
        |             try:
        |                 expires_at_dt = datetime.fromisoformat(expiry_str.replace("Z", "+00:00"))
        |                 if expires_at_dt.tzinfo is None: # Ensure tz-aware
        |                     expires_at_dt = expires_at_dt.replace(tzinfo=timezone.utc)
        |                 if datetime.now(timezone.utc) > expires_at_dt:
        |                     logger.warning(f"Licence key for {user_id} (ID {key_id_hash}) expired on {expiry_str}.")
        |                     return LicenceDetails(tier="FREE", key_id=key_id_hash, user_identifier=user_id, expires_at=expires_at_dt, is_valid=False, raw_content=content)
        |             except ValueError:
        |                 logger.error(f"Invalid expiry date format '{expiry_str}' in licence. Ignoring expiry.")
        |         
        |         _load_licence_tiers_config() # Ensure tiers are fresh before checking tier existence
        |         if tier not in LOADED_LICENCE_TIERS:
        |             logger.warning(f"Unknown licence tier '{tier}'. Defaulting to FREE.")
        |             return LicenceDetails(tier="FREE", key_id=key_id_hash, user_identifier=user_id, expires_at=expires_at_dt, is_valid=(tier=="FREE"), raw_content=content)
        | 
        |         logger.info(f"Licence parsed: Tier '{tier}', User '{user_id}', KeyID '{key_id_hash}', Expires '{expiry_str or 'N/A'}'")
        |         return LicenceDetails(tier=tier, key_id=key_id_hash, user_identifier=user_id, expires_at=expires_at_dt, is_valid=True, raw_content=content)
        |     
        |     except Exception as e:
        |         logger.error(f"Error parsing licence key content: {e}. Defaulting to FREE tier.", exc_info=True)
        |         return LicenceDetails(tier="FREE", is_valid=True, raw_content=content if isinstance(content,str) else str(content))
        | 
        | 
        | def get_licence_details() -> LicenceDetails:
        |     """Loads and caches licence details, reloading if file or tiers config modified."""
        |     global _CACHED_LICENCE, _LICENCE_FILE_MTIME, _LICENCE_TIERS_FILE_MTIME
        | 
        |     # Force reload if tiers config changed, as it affects LicenceDetails population
        |     _load_licence_tiers_config() 
        | 
        |     current_key_mtime = None
        |     try:
        |         if LICENCE_FILE_PATH.exists(): current_key_mtime = LICENCE_FILE_PATH.stat().st_mtime
        |     except FileNotFoundError: pass # Handled below
        | 
        |     if _CACHED_LICENCE and current_key_mtime == _LICENCE_FILE_MTIME: # Key file unchanged
        |         # Tier settings might have changed, re-apply them if _CACHED_LICENCE exists
        |         _CACHED_LICENCE._apply_tier_settings() # Ensures it picks up latest tier definitions
        |         return _CACHED_LICENCE
        | 
        |     if not LICENCE_FILE_PATH.exists():
        |         logger.warning(f"Licence file not found at {LICENCE_FILE_PATH}. Using FREE tier.")
        |         _CACHED_LICENCE = LicenceDetails(tier="FREE", is_valid=True)
        |         _LICENCE_FILE_MTIME = None
        |         return _CACHED_LICENCE
        | 
        |     try:
        |         logger.info(f"Loading licence key from {LICENCE_FILE_PATH}")
        |         content = LICENCE_FILE_PATH.read_text()
        |         _CACHED_LICENCE = _parse_licence_key_content(content)
        |         _LICENCE_FILE_MTIME = current_key_mtime
        |         return _CACHED_LICENCE
        |     except Exception as e:
        |         logger.error(f"Failed to load/parse licence key {LICENCE_FILE_PATH}: {e}. Using FREE tier.", exc_info=True)
        |         _CACHED_LICENCE = LicenceDetails(tier="FREE", is_valid=True)
        |         _LICENCE_FILE_MTIME = current_key_mtime
        |         return _CACHED_LICENCE
        | 
        | 
        | def check_rate_limit(licence: LicenceDetails, client_websocket: WebSocket) -> Tuple[bool, Optional[str], Optional[int]]:
        |     """Checks API request rate limits. Returns (allowed, error_message, error_code)."""
        |     # Elite might still have very high limits, or this check can be skipped.
        |     if licence.tier == "ELITE" and licence.rate_limit_requests == 0: # 0 means unlimited for ELITE
        |         return True, None, None
        | 
        |     client_id = licence.key_id if licence.is_valid and licence.key_id else f"ip:{client_websocket.client.host}"
        |     now_utc = datetime.now(timezone.utc)
        |     
        |     if client_id not in CLIENT_USAGE_RECORDS: CLIENT_USAGE_RECORDS[client_id] = {"requests": [], "llm_tokens": {}}
        |     
        |     # Prune old request timestamps
        |     window_start = now_utc - timedelta(seconds=licence.rate_limit_window_seconds)
        |     CLIENT_USAGE_RECORDS[client_id]["requests"] = [ts for ts in CLIENT_USAGE_RECORDS[client_id]["requests"] if ts > window_start]
        | 
        |     if len(CLIENT_USAGE_RECORDS[client_id]["requests"]) < licence.rate_limit_requests:
        |         CLIENT_USAGE_RECORDS[client_id]["requests"].append(now_utc)
        |         return True, None, None
        |     else:
        |         # Oldest request in window + window duration gives next allowed time
        |         next_allowed_ts = CLIENT_USAGE_RECORDS[client_id]["requests"][0] + timedelta(seconds=licence.rate_limit_window_seconds)
        |         wait_seconds = max(0, int((next_allowed_ts - now_utc).total_seconds()))
        |         msg = (f"Rate limit exceeded for tier {licence.tier}. "
        |                f"Limit: {licence.rate_limit_requests} reqs / {licence.rate_limit_window_seconds // 60} mins. "
        |                f"Try again in {wait_seconds}s.")
        |         logger.warning(f"Client {client_id}: {msg}")
        |         return False, msg, JSONRPC_RATE_LIMIT_ERROR
        | 
        | def check_llm_token_quotas(licence: LicenceDetails, client_websocket: WebSocket, requested_tokens: int) -> Tuple[bool, Optional[str], Optional[int]]:
        |     """Checks LLM token quotas. Returns (allowed, error_message, error_code)."""
        |     if not licence.llm_access: # Should be caught by permission check already
        |         return False, "LLM access denied for this tier.", JSONRPC_PERMISSION_DENIED_ERROR
        | 
        |     if licence.max_llm_tokens_per_request == 0 and licence.max_llm_tokens_per_day == 0 : # 0 means unlimited for this tier for LLM
        |         return True, None, None
        | 
        |     # Per-request limit
        |     if licence.max_llm_tokens_per_request > 0 and requested_tokens > licence.max_llm_tokens_per_request:
        |         msg = f"Requested tokens ({requested_tokens}) exceed per-request limit ({licence.max_llm_tokens_per_request}) for tier {licence.tier}."
        |         return False, msg, JSONRPC_LLM_QUOTA_EXCEEDED_ERROR
        | 
        |     # Per-day limit
        |     if licence.max_llm_tokens_per_day > 0:
        |         client_id = licence.key_id if licence.is_valid and licence.key_id else f"ip:{client_websocket.client.host}" # Consistent client ID
        |         today_str = datetime.now(timezone.utc).strftime("%Y-%m-%d")
        | 
        |         if client_id not in CLIENT_USAGE_RECORDS: CLIENT_USAGE_RECORDS[client_id] = {"requests": [], "llm_tokens": {}}
        |         if "llm_tokens" not in CLIENT_USAGE_RECORDS[client_id]: CLIENT_USAGE_RECORDS[client_id]["llm_tokens"] = {}
        | 
        |         current_daily_usage = CLIENT_USAGE_RECORDS[client_id]["llm_tokens"].get(today_str, 0)
        |         if current_daily_usage + requested_tokens > licence.max_llm_tokens_per_day:
        |             msg = (f"Requested tokens ({requested_tokens}) would exceed daily limit ({licence.max_llm_tokens_per_day}). "
        |                    f"Used today: {current_daily_usage}. Tier: {licence.tier}.")
        |             return False, msg, JSONRPC_LLM_QUOTA_EXCEEDED_ERROR
        |     
        |     return True, None, None # Allowed
        | 
        | def record_llm_token_usage(licence: LicenceDetails, client_websocket: WebSocket, tokens_used: int):
        |     """Records LLM token usage after a successful request."""
        |     if not licence.llm_access or tokens_used == 0: return
        |     if licence.max_llm_tokens_per_day == 0 : return # No daily limit to track against for this tier
        | 
        |     client_id = licence.key_id if licence.is_valid and licence.key_id else f"ip:{client_websocket.client.host}"
        |     today_str = datetime.now(timezone.utc).strftime("%Y-%m-%d")
        | 
        |     if client_id not in CLIENT_USAGE_RECORDS: CLIENT_USAGE_RECORDS[client_id] = {"requests": [], "llm_tokens": {}}
        |     if "llm_tokens" not in CLIENT_USAGE_RECORDS[client_id]: CLIENT_USAGE_RECORDS[client_id]["llm_tokens"] = {}
        |     
        |     # Clean up old daily token records (e.g., older than a few days) to prevent memory leak
        |     # This could be done in a separate periodic task. For now, simple prune on write.
        |     current_date = datetime.now(timezone.utc).date()
        |     for date_str in list(CLIENT_USAGE_RECORDS[client_id]["llm_tokens"].keys()):
        |         record_date = datetime.strptime(date_str, "%Y-%m-%d").date()
        |         if (current_date - record_date).days > 7: # Keep 7 days of history
        |             del CLIENT_USAGE_RECORDS[client_id]["llm_tokens"][date_str]
        | 
        |     CLIENT_USAGE_RECORDS[client_id]["llm_tokens"][today_str] = CLIENT_USAGE_RECORDS[client_id]["llm_tokens"].get(today_str, 0) + tokens_used
        |     logger.debug(f"Client {client_id} used {tokens_used} LLM tokens. Daily total for {today_str}: {CLIENT_USAGE_RECORDS[client_id]['llm_tokens'][today_str]}")
        | 
        | 
        | def check_permission(licence: LicenceDetails, capability_method: str, llm_model_requested: Optional[str] = None) -> Tuple[bool, Optional[str], Optional[int]]:
        |     """Checks capability permission and LLM model permission if applicable."""
        |     # Capability permission
        |     auth_logger.info(f"CHECK_PERM: Received capability_method: '{capability_method}' for tier {licence.tier}") # <<< AJOUTEZ CE LOG
        |     allowed_caps = licence.allowed_capabilities
        |     cap_allowed = False
        |     if "*" in allowed_caps: cap_allowed = True
        |     else:
        |         for pattern in allowed_caps:
        |             if pattern.endswith(".*") and capability_method.startswith(pattern[:-1]): cap_allowed = True; break
        |             elif capability_method == pattern: cap_allowed = True; break
        |     
        |     if not cap_allowed:
        |         msg = f"Permission denied for method '{capability_method}' with tier '{licence.tier}'."
        |         logger.warning(msg + f" (Client ID: {licence.key_id or 'anonymous_ip'})")
        |         return False, msg, JSONRPC_PERMISSION_DENIED_ERROR
        | 
        |     # LLM specific checks if method is mcp.llm.chat
        |     if capability_method == "mcp.llm.chat":
        |         if not licence.llm_access: # General LLM access for tier
        |             msg = f"LLM access is disabled for tier '{licence.tier}'."
        |             return False, msg, JSONRPC_PERMISSION_DENIED_ERROR
        |         
        |         if llm_model_requested: # If a specific model was requested by client
        |             if "*" not in licence.allowed_llm_models and llm_model_requested not in licence.allowed_llm_models:
        |                 # Could also check against AVAILABLE_LLM_MODELS for existence.
        |                 # For now, just tier permission.
        |                 msg = f"LLM model '{llm_model_requested}' not allowed for tier '{licence.tier}'. Allowed: {licence.allowed_llm_models}"
        |                 return False, msg, JSONRPC_LLM_MODEL_NOT_ALLOWED_ERROR
        |     
        |     return True, None, None # All checks passed
        | 
        | auth_logger = logging.getLogger("llmbasedos.gateway.auth")
        | def authenticate_and_authorize_request(
        |     websocket: WebSocket, method_name: str, llm_model_requested: Optional[str] = None, llm_tokens_to_request: int = 0
        | ) -> Tuple[Optional[LicenceDetails], Optional[Dict[str, Any]]]:
        |     """
        |     Full auth pipeline for a request.
        |     Returns (LicenceDetails_if_ok, None) or (None_or_LicenceDetails_for_context, error_dict).
        |     """
        |     auth_logger.info(f"AUTH: Received method to check: '{method_name}' for client {websocket.client if websocket else 'unix_client'}") 
        |     licence = get_licence_details() # Gets current (possibly cached) licence
        | 
        |     # 1. Basic validity (already handled by get_licence_details defaulting to FREE)
        |     # If licence.is_valid is False but tier is not FREE, it means an issue like expiry.
        |     # The tier settings on 'licence' would reflect FREE if it was downgraded.
        | 
        |     # 2. Rate Limiting for API calls
        |     allowed, msg, err_code = check_rate_limit(licence, websocket)
        |     if not allowed:
        |         return licence, {"code": err_code, "message": msg} # Return licence for context if needed
        | 
        |     # 3. Permission check for capability and specific LLM model
        |     allowed, msg, err_code = check_permission(licence, method_name, llm_model_requested)
        |     if not allowed:
        |         return licence, {"code": err_code, "message": msg}
        | 
        |     # 4. LLM Token Quotas (only if method is mcp.llm.chat and tokens are requested)
        |     if method_name == "mcp.llm.chat" and llm_tokens_to_request > 0 : # llm_tokens_to_request could be an estimate
        |         # Note: `llm_tokens_to_request` for `mcp.llm.chat` is tricky as actual output tokens are unknown.
        |         # This check is more for services that declare input token cost, or for max_per_request on prompt.
        |         # For chat, we might only check max_per_day *before* the call, and per_request on prompt length.
        |         # The actual usage is recorded *after* the call.
        |         # Let's assume llm_tokens_to_request here is a pre-estimate of prompt tokens for per-request check.
        |         # For daily quota, we check *before* call if *any* tokens are being requested.
        |         
        |         allowed, msg, err_code = check_llm_token_quotas(licence, websocket, llm_tokens_to_request)
        |         if not allowed:
        |             return licence, {"code": err_code, "message": msg}
        | 
        |     return licence, None # All checks passed
        | 
        | def get_licence_info_for_mcp_call(client_websocket: WebSocket) -> Dict[str, Any]:
        |     """Prepares data for mcp.licence.check endpoint."""
        |     licence = get_licence_details() # Current effective licence
        |     
        |     # For displaying "remaining" quota, we need client_id
        |     client_id = licence.key_id if licence.is_valid and licence.key_id else f"ip:{client_websocket.client.host}"
        |     
        |     # API Rate Limit remaining
        |     requests_remaining = "N/A"
        |     if licence.rate_limit_requests > 0 : # If there is a limit
        |         now_utc = datetime.now(timezone.utc)
        |         window_start = now_utc - timedelta(seconds=licence.rate_limit_window_seconds)
        |         client_reqs = CLIENT_USAGE_RECORDS.get(client_id, {}).get("requests", [])
        |         valid_reqs_in_window = [ts for ts in client_reqs if ts > window_start]
        |         requests_remaining = max(0, licence.rate_limit_requests - len(valid_reqs_in_window))
        | 
        |     # LLM Tokens per day remaining
        |     llm_tokens_today_remaining = "N/A"
        |     if licence.llm_access and licence.max_llm_tokens_per_day > 0:
        |         today_str = datetime.now(timezone.utc).strftime("%Y-%m-%d")
        |         used_today = CLIENT_USAGE_RECORDS.get(client_id, {}).get("llm_tokens", {}).get(today_str, 0)
        |         llm_tokens_today_remaining = max(0, licence.max_llm_tokens_per_day - used_today)
        | 
        |     return {
        |         "tier": licence.tier,
        |         "key_id": licence.key_id,
        |         "user_identifier": licence.user_identifier,
        |         "is_valid": licence.is_valid,
        |         "expires_at": licence.expires_at.isoformat() if licence.expires_at else None,
        |         "effective_permissions": licence.allowed_capabilities,
        |         "quotas": {
        |             "api_requests_limit": f"{licence.rate_limit_requests}/{licence.rate_limit_window_seconds // 60}min",
        |             "api_requests_remaining_in_window": requests_remaining,
        |             "llm_access": licence.llm_access,
        |             "allowed_llm_models": licence.allowed_llm_models,
        |             "max_llm_tokens_per_request": licence.max_llm_tokens_per_request if licence.max_llm_tokens_per_request > 0 else "unlimited",
        |             "max_llm_tokens_per_day": licence.max_llm_tokens_per_day if licence.max_llm_tokens_per_day > 0 else "unlimited",
        |             "llm_tokens_today_remaining": llm_tokens_today_remaining,
        |         },
        |         "note": "Remaining quotas are specific to your client identifier (licence key or IP)."
        |     }
        --- Fin Contenu ---
      Fichier: config.py
        --- Début Contenu (utf-8) ---
        | # llmbasedos_pkg/gateway/config.py
        | import os
        | from pathlib import Path
        | from typing import Dict, Any, List, Optional
        | import logging
        | import yaml # Garder l'import, la logique de chargement peut rester pour plus tard
        | 
        | # --- Core Gateway Settings ---
        | GATEWAY_HOST: str = os.getenv("LLMBDO_GATEWAY_HOST", "0.0.0.0")
        | GATEWAY_WEB_PORT: int = int(os.getenv("LLMBDO_GATEWAY_WEB_PORT", "8000"))
        | GATEWAY_UNIX_SOCKET_PATH: Path = Path(os.getenv("LLMBDO_GATEWAY_UNIX_SOCKET_PATH", "/run/mcp/gateway.sock"))
        | GATEWAY_EXECUTOR_MAX_WORKERS: int = int(os.getenv("LLMBDO_GATEWAY_EXECUTOR_WORKERS", "4"))
        | 
        | # --- MCP Settings ---
        | MCP_CAPS_DIR: Path = Path(os.getenv("LLMBDO_MCP_CAPS_DIR", "/run/mcp"))
        | MCP_CAPS_DIR.mkdir(parents=True, exist_ok=True)
        | 
        | # --- Licence & Auth Settings ---
        | LICENCE_FILE_PATH: Path = Path(os.getenv("LLMBDO_LICENCE_FILE_PATH", "/etc/llmbasedos/lic.key"))
        | LICENCE_TIERS_CONFIG_PATH_STR: str = os.getenv("LLMBDO_LICENCE_TIERS_CONFIG_PATH", "/etc/llmbasedos/licence_tiers.yaml")
        | LICENCE_TIERS_CONFIG_PATH: Path = Path(LICENCE_TIERS_CONFIG_PATH_STR)
        | 
        | # --- MODIFICATION PRINCIPALE ICI ---
        | # Mettre le tier FREE par défaut comme étant totalement permissif pour les tests
        | DEFAULT_LICENCE_TIERS: Dict[str, Dict[str, Any]] = {
        |     "FREE": {
        |         "rate_limit_requests": 10000,             # Très permissif
        |         "rate_limit_window_seconds": 3600,
        |         "allowed_capabilities": ["*"],            # Toutes les capacités
        |         "llm_access": True,                       # Accès LLM autorisé
        |         "allowed_llm_models": ["*"],              # Tous les modèles LLM
        |         "max_llm_tokens_per_request": 0,          # 0 = illimité
        |         "max_llm_tokens_per_day": 0               # 0 = illimité
        |     },
        |     "PRO": { # Garder PRO et ELITE pour la structure, même si FREE est utilisé
        |         "rate_limit_requests": 20000, "rate_limit_window_seconds": 3600,
        |         "allowed_capabilities": ["*"],
        |         "llm_access": True, "allowed_llm_models": ["*"],
        |         "max_llm_tokens_per_request": 0, "max_llm_tokens_per_day": 0
        |     },
        |     "ELITE": {
        |         "rate_limit_requests": 50000, "rate_limit_window_seconds": 3600,
        |         "allowed_capabilities": ["*"], "llm_access": True, "allowed_llm_models": ["*"],
        |         "max_llm_tokens_per_request": 0, "max_llm_tokens_per_day": 0
        |     }
        | }
        | 
        | LICENCE_TIERS: Dict[str, Dict[str, Any]] = DEFAULT_LICENCE_TIERS # Initialiser avec les défauts modifiés
        | 
        | # Logique de chargement du fichier YAML (on la garde, mais elle sera surchargée par les défauts si le fichier n'est pas bon)
        | # Pour les tests actuels, on s'assure que même si le chargement YAML échoue, FREE est permissif.
        | # Si vous voulez forcer l'utilisation des défauts ci-dessus pour le test, vous pouvez commenter tout le bloc try-except ci-dessous.
        | if LICENCE_TIERS_CONFIG_PATH.exists() and LICENCE_TIERS_CONFIG_PATH.is_file():
        |     try:
        |         with LICENCE_TIERS_CONFIG_PATH.open('r') as f:
        |             loaded_config = yaml.safe_load(f) # Renommé pour éviter confusion
        |             if isinstance(loaded_config, dict) and loaded_config.get("tiers") and isinstance(loaded_config["tiers"], dict):
        |                 # Fusionner intelligemment : les valeurs du YAML écrasent les défauts
        |                 # Si une clé existe dans YAML et dans DEFAULT, YAML gagne.
        |                 # Si une clé existe seulement dans DEFAULT, elle est conservée.
        |                 merged_tiers = {}
        |                 for tier_name, default_conf in DEFAULT_LICENCE_TIERS.items():
        |                     merged_tiers[tier_name] = default_conf.copy() # Commencer avec une copie du défaut
        |                     if tier_name in loaded_config["tiers"]:
        |                         merged_tiers[tier_name].update(loaded_config["tiers"][tier_name]) # Mettre à jour avec les valeurs du YAML
        | 
        |                 # Ajouter les tiers du YAML qui ne sont pas dans les défauts (moins probable)
        |                 for tier_name, custom_conf in loaded_config["tiers"].items():
        |                     if tier_name not in merged_tiers:
        |                         merged_tiers[tier_name] = custom_conf
        |                 
        |                 LICENCE_TIERS = merged_tiers
        |                 logging.info(f"Loaded and merged licence tiers from {LICENCE_TIERS_CONFIG_PATH} with defaults.")
        |             else:
        |                 logging.warning(f"Invalid or empty 'tiers' structure in {LICENCE_TIERS_CONFIG_PATH}. Using permissive FREE default.")
        |                 # Dans ce cas, LICENCE_TIERS reste le DEFAULT_LICENCE_TIERS permissif défini ci-dessus
        |     except Exception as e_tiers:
        |         logging.error(f"Error loading licence tiers from {LICENCE_TIERS_CONFIG_PATH}: {e_tiers}. Using permissive FREE default.", exc_info=True)
        |         # LICENCE_TIERS reste le DEFAULT_LICENCE_TIERS permissif
        | else:
        |     logging.info(f"Licence tiers config file not found at {LICENCE_TIERS_CONFIG_PATH}. Using permissive FREE default tiers.")
        | 
        | 
        | # --- Upstream LLM Settings ---
        | OPENAI_API_KEY: Optional[str] = os.getenv("OPENAI_API_KEY")
        | if not OPENAI_API_KEY:
        |     logging.warning("OPENAI_API_KEY environment variable not set. OpenAI models may not function.")
        | DEFAULT_LLM_PROVIDER: str = os.getenv("LLMBDO_DEFAULT_LLM_PROVIDER", "openai")
        | AVAILABLE_LLM_MODELS: Dict[str, Dict[str, Any]] = {
        |     "gpt-4o": {
        |         "provider": "openai", "model_name": "gpt-4o",
        |         "api_base_url": os.getenv("OPENAI_API_BASE_URL", "https://api.openai.com/v1"),
        |         "api_key": None, "is_default": True if DEFAULT_LLM_PROVIDER == "openai" else False,
        |     },
        |     "local-llama": {
        |         "provider": "llama_cpp", "model_name": os.getenv("LLAMA_CPP_DEFAULT_MODEL", "default-model-alias"),
        |         "api_base_url": os.getenv("LLAMA_CPP_API_BASE_URL", "http://localhost:8080/v1"),
        |         "api_key": None, "is_default": True if DEFAULT_LLM_PROVIDER == "llama_cpp" else False,
        |     },
        | }
        | 
        | # --- Logging ---
        | LOG_LEVEL_STR: str = os.getenv("LLMBDO_LOG_LEVEL", "INFO").upper()
        | LOG_LEVEL_FALLBACK: int = logging.INFO
        | LOG_LEVEL: int = logging.getLevelName(LOG_LEVEL_STR)
        | if not isinstance(LOG_LEVEL, int):
        |     logging.warning(f"Invalid LLMBDO_LOG_LEVEL '{LOG_LEVEL_STR}'. Defaulting to INFO.")
        |     LOG_LEVEL = LOG_LEVEL_FALLBACK
        |     LOG_LEVEL_STR = logging.getLevelName(LOG_LEVEL_FALLBACK)
        | 
        | # Utiliser la configuration de logging simplifiée pour les tests
        | LOGGING_CONFIG: Dict[str, Any] = {
        |     "version": 1, "disable_existing_loggers": False,
        |     "formatters": {"simple": {"format": "%(asctime)s - %(name)s - %(levelname)s - %(message)s"}},
        |     "handlers": {"console": {"class": "logging.StreamHandler", "formatter": "simple", "stream": "ext://sys.stdout"}},
        |     "root": {"handlers": ["console"], "level": "DEBUG"}, # DEBUG pour voir tout
        |     "loggers": { 
        |         "uvicorn": {"level": "INFO", "handlers": ["console"], "propagate": False},
        |         "fastapi": {"level": "INFO", "handlers": ["console"], "propagate": False},
        |         "websockets": {"level": "WARNING", "handlers": ["console"], "propagate": False},
        |         "llmbasedos": {"level": "DEBUG", "handlers": ["console"], "propagate": False}, # Notre app en DEBUG
        |         "httpx": {"level": "WARNING", "handlers": ["console"], "propagate": False},
        |         "watchdog": {"level": "WARNING", "handlers": ["console"], "propagate": False},
        |     }
        | }
        | 
        | # JSON RPC Default Error Codes (inchangés)
        | JSONRPC_PARSE_ERROR: int = -32700
        | JSONRPC_INVALID_REQUEST: int = -32600
        | JSONRPC_METHOD_NOT_FOUND: int = -32601
        | JSONRPC_INVALID_PARAMS: int = -32602
        | JSONRPC_INTERNAL_ERROR: int = -32603
        | JSONRPC_AUTH_ERROR: int = -32000
        | JSONRPC_RATE_LIMIT_ERROR: int = -32001
        | JSONRPC_PERMISSION_DENIED_ERROR: int = -32002
        | JSONRPC_LLM_QUOTA_EXCEEDED_ERROR: int = -32003 
        | JSONRPC_LLM_MODEL_NOT_ALLOWED_ERROR: int = -32004
        --- Fin Contenu ---
      Fichier: dispatch.py
        --- Début Contenu (ascii) ---
        | # llmbasedos/gateway/dispatch.py
        | import asyncio
        | import json
        | import logging
        | from typing import Any, Dict, Optional, Union, List, AsyncGenerator
        | from concurrent.futures import ThreadPoolExecutor # For blocking UNIX socket calls
        | 
        | # Use centralized JSON-RPC helpers
        | from llmbasedos.mcp_server_framework import (
        |     create_mcp_response, create_mcp_error,
        |     JSONRPC_PARSE_ERROR, JSONRPC_INVALID_REQUEST, JSONRPC_METHOD_NOT_FOUND,
        |     JSONRPC_INVALID_PARAMS, JSONRPC_INTERNAL_ERROR
        | )
        | from . import registry
        | from . import upstream
        | from .auth import LicenceDetails, get_licence_info_for_mcp_call, record_llm_token_usage # Added record_llm_token_usage
        | from .config import GATEWAY_EXECUTOR_MAX_WORKERS # For thread pool size
        | from .upstream import LLMError # For catching specific LLM errors from upstream module
        | 
        | logger = logging.getLogger("llmbasedos.gateway.dispatch")
        | 
        | # Thread pool for blocking UNIX socket operations
        | # This should be initialized once, e.g. in main.py and passed around, or as a global here.
        | # For simplicity, global here.
        | _dispatch_executor = ThreadPoolExecutor(max_workers=GATEWAY_EXECUTOR_MAX_WORKERS, thread_name_prefix="gateway_dispatch_worker")
        | 
        | # --- UNIX Socket Client (Blocking version for executor) ---
        | def _send_request_to_backend_server_blocking(
        |     socket_path: str, request_payload: Dict[str, Any]
        | ) -> Dict[str, Any]:
        |     """Sends JSON-RPC to backend via UNIX socket (BLOCKING). For use with run_in_executor."""
        |     # This function is synchronous and will run in a separate thread.
        |     # It uses standard Python sockets.
        |     import socket # Standard library socket
        | 
        |     request_id = request_payload.get("id")
        |     logger.debug(f"SYNC: Connecting to UNIX socket: {socket_path} for req ID {request_id}")
        |     
        |     sock = None # Initialize sock to None
        |     try:
        |         sock = socket.socket(socket.AF_UNIX, socket.SOCK_STREAM)
        |         sock.settimeout(5.0) # Connection timeout
        |         sock.connect(socket_path)
        |         
        |         request_bytes = json.dumps(request_payload).encode('utf-8') + b'\0'
        |         logger.debug(f"SYNC: Sending to {socket_path} (ID {request_id}): {request_payload['method']}")
        |         sock.sendall(request_bytes)
        | 
        |         response_buffer = bytearray()
        |         sock.settimeout(15.0) # Response timeout (can be longer for some methods)
        |         while True:
        |             chunk = sock.recv(4096)
        |             if not chunk: break # Server closed connection
        |             response_buffer.extend(chunk)
        |             if b'\0' in chunk: # Found delimiter
        |                 response_buffer = response_buffer.split(b'\0', 1)[0]
        |                 break
        |         
        |         if not response_buffer:
        |             logger.error(f"SYNC: No response from {socket_path} for ID {request_id}")
        |             return create_mcp_error(request_id, JSONRPC_INTERNAL_ERROR, "No response from backend.")
        | 
        |         response_str = response_buffer.decode('utf-8')
        |         logger.debug(f"SYNC: Received from {socket_path} (ID {request_id}): {response_str[:200]}...")
        |         return json.loads(response_str)
        | 
        |     except socket.timeout:
        |         logger.error(f"SYNC: Timeout with {socket_path} for ID {request_id}")
        |         return create_mcp_error(request_id, JSONRPC_INTERNAL_ERROR, f"Timeout with backend server {socket_path}.")
        |     except ConnectionRefusedError:
        |         logger.error(f"SYNC: Connection refused by {socket_path} for ID {request_id}")
        |         return create_mcp_error(request_id, JSONRPC_INTERNAL_ERROR, f"Backend {socket_path} unavailable (conn refused).")
        |     except FileNotFoundError:
        |         logger.error(f"SYNC: UNIX socket not found: {socket_path} for ID {request_id}")
        |         return create_mcp_error(request_id, JSONRPC_INTERNAL_ERROR, f"Backend {socket_path} unavailable (socket not found).")
        |     except json.JSONDecodeError:
        |         logger.error(f"SYNC: Failed to decode JSON from {socket_path} for ID {request_id}. Resp: {response_buffer.decode(errors='ignore')}")
        |         return create_mcp_error(request_id, JSONRPC_INTERNAL_ERROR, "Invalid JSON response from backend.")
        |     except Exception as e:
        |         logger.error(f"SYNC: Error with {socket_path} for ID {request_id}: {e}", exc_info=True)
        |         return create_mcp_error(request_id, JSONRPC_INTERNAL_ERROR, f"Comm error with backend: {type(e).__name__}.")
        |     finally:
        |         if sock: sock.close()
        | 
        | 
        | async def handle_mcp_request(
        |     request: Dict[str, Any],
        |     licence_details: LicenceDetails, # Auth already done by main WebSocket/UNIX handler
        |     client_websocket_for_context: Any # Pass WebSocket or mock for context (IP, etc.)
        | ) -> Union[Dict[str, Any], AsyncGenerator[Dict[str, Any], None]]:
        |     """
        |     Handles an authorized JSON-RPC request.
        |     Routes to gateway's own methods or dispatches to backend MCP servers.
        |     """
        |     request_id = request.get("id") # Should always be present for valid JSON-RPC reqs
        |     method_name = request.get("method")
        |     params = request.get("params", [])
        | 
        |     # Basic validation of method_name and params structure (already done by auth layer potentially)
        |     if not isinstance(method_name, str):
        |         return create_mcp_error(request_id, JSONRPC_INVALID_REQUEST, "Method name must be a string.")
        |     if not isinstance(params, (list, dict)): # JSON-RPC params can be array or object
        |         return create_mcp_error(request_id, JSONRPC_INVALID_PARAMS, "Params must be an array or object.")
        | 
        |     # --- 1. Gateway's own MCP methods ---
        |     if method_name == "mcp.hello":
        |         all_methods = registry.get_all_registered_method_names()
        |         permitted_methods = [] # Filter based on licence_details.allowed_capabilities
        |         if "*" in licence_details.allowed_capabilities:
        |             permitted_methods = all_methods
        |         else:
        |             for m_name in all_methods:
        |                 is_allowed = any(
        |                     (m_name.startswith(p[:-1]) if p.endswith(".*") else m_name == p)
        |                     for p in licence_details.allowed_capabilities
        |                 )
        |                 if is_allowed: permitted_methods.append(m_name)
        |         
        |         # Ensure core gateway methods are always present if allowed by base tier definition
        |         # This check is implicitly handled by `allowed_capabilities` now.
        |         return create_mcp_response(request_id, result=sorted(list(set(permitted_methods))))
        | 
        |     elif method_name == "mcp.listCapabilities":
        |         all_services_meta = registry.get_detailed_capabilities_list()
        |         # TODO: Finer-grained filtering based on licence (capabilities within each service)
        |         return create_mcp_response(request_id, result=all_services_meta)
        | 
        |     elif method_name == "mcp.licence.check":
        |         # Pass client_websocket_for_context for IP-based quota display if needed
        |         lic_info = get_licence_info_for_mcp_call(client_websocket_for_context)
        |         return create_mcp_response(request_id, result=lic_info)
        | 
        |     elif method_name == "mcp.llm.chat":
        |         # Params validation (basic, schema would be better)
        |         if not isinstance(params, list) or len(params) == 0 or not isinstance(params[0], list):
        |             return create_mcp_error(request_id, JSONRPC_INVALID_PARAMS, "Invalid params for mcp.llm.chat. Expected: [[messages_list], ?llm_options_dict]")
        |         
        |         messages_list: List[Dict[str,str]] = params[0]
        |         llm_options = params[1] if len(params) > 1 and isinstance(params[1], dict) else {}
        |         
        |         stream_response_flag = llm_options.pop("stream", False) # Client requests streaming
        |         requested_model_alias = llm_options.pop("model", None)
        |         
        |         # Actual token counting for output is hard with streaming.
        |         # For now, record_llm_token_usage might be called with estimated input tokens
        |         # or after full response if non-streaming, or summed up after streaming.
        |         # Let's assume for now token quota checks were done by auth layer based on estimates.
        |         # The actual recording happens after generation.
        | 
        |         async def llm_stream_generator():
        |             total_tokens_used_in_stream = 0 # For recording usage after stream
        |             try:
        |                 async for llm_event in upstream.call_llm_chat_completion(
        |                     messages=messages_list, licence=licence_details, # licence passed for context if upstream needs
        |                     requested_model_alias=requested_model_alias, stream=True, **llm_options
        |                 ):
        |                     if llm_event["event"] == "error":
        |                         # Forward error from upstream as JSON-RPC error
        |                         err_data = llm_event["data"]
        |                         yield create_mcp_error(request_id, JSONRPC_INTERNAL_ERROR, # Or more specific LLM error code
        |                                                f"LLM Error ({err_data.get('provider')}/{err_data.get('model_name')}): {err_data.get('message')}",
        |                                                data=err_data.get('details'))
        |                         return # Stop stream on error
        | 
        |                     elif llm_event["event"] == "chunk":
        |                         # TODO: Estimate tokens from this chunk for more accurate live counting if needed
        |                         # For OpenAI, chunk format is like:
        |                         # {"id":"chatcmpl-xxx","object":"chat.completion.chunk","created":1678690,"model":"gpt-3.5-turbo","choices":[{"index":0,"delta":{"content":" World"},"finish_reason":null}]}
        |                         # We need to count output tokens to record usage.
        |                         # This is complex with streaming. A simple way: count based on `delta.content`.
        |                         # For now, this demo won't implement detailed token counting from stream chunks.
        |                         # Assume a fixed token cost or count after full stream for recording.
        |                         yield create_mcp_response(request_id, result={"type": "llm_chunk", "content": llm_event["data"]})
        |                     
        |                     elif llm_event["event"] == "done":
        |                         # TODO: If non-streaming, result_payload["usage"]["total_tokens"] gives count for OpenAI
        |                         # For streaming, we'd need to sum them up.
        |                         # For now, let's assume a placeholder for tokens_used, or get from final "done" if available.
        |                         # This is where `record_llm_token_usage` would be called.
        |                         # For simplicity, not implementing exact token counting from stream here.
        |                         # Assume a placeholder value or record only for non-streaming where total is known.
        |                         # record_llm_token_usage(licence_details, client_websocket_for_context, total_tokens_used_in_stream)
        |                         yield create_mcp_response(request_id, result={"type": "llm_stream_end"})
        |                         return # End this generator
        |             
        |             except Exception as e_stream_handler: # Catch errors from processing the generator
        |                 logger.error(f"Error processing LLM stream for ID {request_id}: {e_stream_handler}", exc_info=True)
        |                 yield create_mcp_error(request_id, JSONRPC_INTERNAL_ERROR, f"LLM stream processing error: {type(e_stream_handler).__name__}")
        |             # finally:
        |                 # Call record_llm_token_usage here if tokens were accumulated
        |                 # logger.debug(f"LLM stream for {request_id} finished. Total output tokens (estimated/actual): {total_tokens_used_in_stream}")
        |                 # if total_tokens_used_in_stream > 0:
        |                 #    record_llm_token_usage(licence_details, client_websocket_for_context, total_tokens_used_in_stream)
        | 
        | 
        |         if stream_response_flag:
        |             return llm_stream_generator() # Return the async generator
        |         else:
        |             # Collect non-streaming result (should be one 'chunk' event with full data, then 'done')
        |             final_result_payload = None
        |             async for event_response in llm_stream_generator():
        |                 # The generator yields full JSON-RPC responses. We need the 'result' part from the 'llm_chunk'.
        |                 if event_response.get("result", {}).get("type") == "llm_chunk":
        |                     # This will be the full LLM provider's response object
        |                     final_result_payload = event_response["result"]["content"] 
        |                 elif event_response.get("result", {}).get("type") == "llm_stream_end":
        |                     break # Done collecting
        |                 elif "error" in event_response: # Error occurred
        |                     return event_response # Forward the error JSON-RPC response
        |             
        |             if final_result_payload:
        |                 # Record token usage for non-streaming if data is available
        |                 # OpenAI example: final_result_payload.get("usage", {}).get("total_tokens", 0)
        |                 tokens_actually_used = 0 # Placeholder for actual token counting
        |                 if isinstance(final_result_payload, dict) and "usage" in final_result_payload:
        |                     tokens_actually_used = final_result_payload["usage"].get("total_tokens", 0)
        |                 if tokens_actually_used > 0:
        |                     record_llm_token_usage(licence_details, client_websocket_for_context, tokens_actually_used)
        |                 
        |                 return create_mcp_response(request_id, result=final_result_payload)
        |             else:
        |                 return create_mcp_error(request_id, JSONRPC_INTERNAL_ERROR, "LLM call (non-stream) failed to produce a result.")
        | 
        |     # --- 2. Dispatch to a registered backend MCP server ---
        |     routing_info = registry.get_capability_routing_info(method_name)
        |     if routing_info:
        |         socket_path = routing_info["socket_path"]
        |         logger.info(f"Dispatching '{method_name}' (ID {request_id}) to backend at {socket_path}")
        |         
        |         request_to_forward = request.copy() # Contains id, method, params
        |         request_to_forward["jsonrpc"] = "2.0" # Ensure protocol version is set
        |         logger.debug(f"GATEWAY DISPATCH: request_to_forward before sending to backend: {json.dumps(request_to_forward)}")
        |         loop = asyncio.get_running_loop()
        |         try:
        |             # Run blocking socket call in thread pool executor
        |             response_from_server = await loop.run_in_executor(
        |                 _dispatch_executor,
        |                 _send_request_to_backend_server_blocking, 
        |                 socket_path, 
        |                 request_to_forward
        |             )
        |             # Ensure the ID from the backend matches our original request_id if it was changed/missing
        |             if "id" not in response_from_server or response_from_server["id"] != request_id:
        |                 logger.warning(f"Backend server {socket_path} returned mismatched/missing ID. Original: {request_id}, Backend: {response_from_server.get('id')}. Fixing.")
        |                 response_from_server["id"] = request_id
        |             return response_from_server
        |         except Exception as e_exec: # Error from run_in_executor itself or if task cancelled
        |             logger.error(f"Error dispatching to backend {socket_path} via executor: {e_exec}", exc_info=True)
        |             return create_mcp_error(request_id, JSONRPC_INTERNAL_ERROR, f"Failed to dispatch to backend: {type(e_exec).__name__}")
        |     
        |     # --- 3. Method not found ---
        |     logger.warning(f"Method not found: {method_name} (ID {request_id})")
        |     return create_mcp_error(request_id, JSONRPC_METHOD_NOT_FOUND, f"Method '{method_name}' not found.")
        | 
        | def shutdown_dispatch_executor():
        |     """Shuts down the thread pool executor used for dispatching."""
        |     logger.info("Shutting down dispatch thread pool executor...")
        |     _dispatch_executor.shutdown(wait=True)
        |     logger.info("Dispatch executor shut down.")
        --- Fin Contenu ---
      Fichier: licence_tiers.yaml
        --- Début Contenu (ascii) ---
        | # ./llmbasedos_src/gateway/licence_tiers.yaml
        | tiers:
        |   rate_limit_requests: 10000
        |   rate_limit_window_seconds: 3600
        |   allowed_capabilities:
        |     - "*" # All capabilities
        |   llm_access: true
        |   allowed_llm_models:
        |     - "*" # All configured models
        | 
        | PRO:
        |   rate_limit_requests: 1000
        |   rate_limit_window_seconds: 3600
        |   allowed_capabilities:
        |     - "mcp.hello"
        |     - "mcp.listCapabilities"
        |     - "mcp.licence.check"
        |     - "mcp.fs.*"
        |     - "mcp.mail.list"
        |     - "mcp.mail.read"
        |     - "mcp.sync.*"
        |     - "mcp.agent.listWorkflows"
        |     - "mcp.agent.runWorkflow"
        |     - "mcp.agent.getWorkflowStatus"
        |     - "mcp.llm.chat"
        |   llm_access: true
        |   allowed_llm_models: # Example: Allow specific models for PRO, or "*" for all configured
        |     - "gpt-3.5-turbo"
        |     - "local-model" 
        |     # Or just: "*"
        | 
        | ELITE:
        |   rate_limit_requests: 10000
        |   rate_limit_window_seconds: 3600
        |   allowed_capabilities:
        |     - "*" # All capabilities
        |   llm_access: true
        |   allowed_llm_models:
        |     - "*" # All configured models
        --- Fin Contenu ---
      Fichier: main.py
        --- Début Contenu (utf-8) ---
        | # llmbasedos_pkg/gateway/main.py
        | import asyncio
        | import json
        | import logging
        | import logging.config
        | from pathlib import Path
        | import os
        | import signal 
        | from typing import Any, Dict, List, Optional, AsyncGenerator, Set
        | 
        | import uvicorn # type: ignore
        | from fastapi import FastAPI, WebSocket, WebSocketDisconnect
        | from starlette.websockets import WebSocketState # Nécessaire pour WebSocketState.CONNECTED
        | from contextlib import asynccontextmanager
        | 
        | # Imports depuis le package llmbasedos
        | from llmbasedos.mcp_server_framework import create_mcp_error, JSONRPC_PARSE_ERROR, JSONRPC_INVALID_REQUEST, JSONRPC_INTERNAL_ERROR
        | from .config import (
        |     GATEWAY_UNIX_SOCKET_PATH, GATEWAY_HOST, GATEWAY_WEB_PORT,
        |     LOGGING_CONFIG, # S'assurer que c'est bien défini et importable
        |     JSONRPC_AUTH_ERROR, # Pour l'erreur d'auth
        |     # D'autres codes d'erreur de config.py pourraient être nécessaires ici
        | )
        | from . import registry
        | from . import dispatch
        | from .auth import authenticate_and_authorize_request, LicenceDetails 
        | 
        | # --- Configuration du Logging ---
        | def setup_gateway_logging():
        |     try:
        |         logging.config.dictConfig(LOGGING_CONFIG)
        |         logging.getLogger("llmbasedos.gateway.main").info("Gateway logging configured via dictConfig.")
        |     except ValueError as e_log_val: # Peut arriver si python-json-logger n'est pas là et que le formatter 'json' est demandé
        |         logging.basicConfig(level=logging.INFO, format="%(asctime)s - %(name)s - %(levelname)s - %(message)s (fallback)")
        |         logging.getLogger("llmbasedos.gateway.main").error(f"Failed to apply dictConfig for logging: {e_log_val}. Using basicConfig.", exc_info=True)
        |     except Exception as e_log_other:
        |         logging.basicConfig(level=logging.INFO, format="%(asctime)s - %(name)s - %(levelname)s - %(message)s (fallback)")
        |         logging.getLogger("llmbasedos.gateway.main").error(f"Unexpected error applying dictConfig: {e_log_other}. Using basicConfig.", exc_info=True)
        | 
        | # Logger global, sera correctement initialisé dans le lifespan_manager
        | logger: logging.Logger = logging.getLogger("llmbasedos.gateway.main")
        | 
        | # --- Variables Globales pour les Serveurs/Tâches en Arrière-plan ---
        | _unix_socket_server_instance: Optional[asyncio.AbstractServer] = None # Renommé pour clarté
        | _active_unix_client_tasks: Set[asyncio.Task] = set()
        | _capability_watcher_task_instance: Optional[asyncio.Task] = None # Renommé pour clarté
        | _shutdown_event_flag = asyncio.Event() # Renommé pour clarté
        | 
        | # --- Mock Client Context pour Sockets UNIX (pour l'authentification) ---
        | class MockUnixClientContext:
        |     def __init__(self, peername: Any):
        |         self.host = "unix_socket_client" # Identifiant générique
        |         self.port = 0 
        |         self.peername_str = str(peername) # Pour le logging
        |     def __repr__(self): return f"<MockUnixClientContext peer='{self.peername_str}'>"
        | 
        | 
        | # --- Fonctions de Gestion des Tâches en Arrière-plan ---
        | async def _start_unix_socket_server_logic():
        |     global _unix_socket_server_instance
        |     socket_path_obj = Path(GATEWAY_UNIX_SOCKET_PATH) # GATEWAY_UNIX_SOCKET_PATH vient de .config et est un Path
        | 
        |     try:
        |         socket_path_obj.parent.mkdir(parents=True, exist_ok=True)
        |         if socket_path_obj.exists(): socket_path_obj.unlink()
        |     except OSError as e:
        |         logger.error(f"Error preparing UNIX socket path {socket_path_obj}: {e}. UNIX server may fail.")
        |         return
        | 
        |     try:
        |         _unix_socket_server_instance = await asyncio.start_unix_server(
        |             _run_unix_socket_client_handler_managed, path=str(socket_path_obj)
        |         )
        |         addr = _unix_socket_server_instance.sockets[0].getsockname() if _unix_socket_server_instance.sockets else str(socket_path_obj)
        |         logger.info(f"MCP Gateway listening on UNIX socket: {addr}")
        |         try:
        |             uid = os.geteuid(); gid = os.getgid()
        |             os.chown(str(socket_path_obj), uid, gid)
        |             os.chmod(str(socket_path_obj), 0o660)
        |             logger.info(f"Set permissions for {socket_path_obj} to 0660 (uid:{uid}, gid:{gid}).")
        |         except OSError as e_perm:
        |             logger.warning(f"Could not set optimal permissions for UNIX socket {socket_path_obj}: {e_perm}.")
        |     except Exception as e_start_unix:
        |         logger.error(f"Failed to start UNIX socket server on {socket_path_obj}: {e_start_unix}", exc_info=True) # CORRIGÉ ICI
        |         _unix_socket_server_instance = None
        | 
        | async def _stop_unix_socket_server_logic():
        |     global _unix_socket_server_instance
        |     if _unix_socket_server_instance:
        |         logger.info("Stopping UNIX socket server...")
        |         _unix_socket_server_instance.close()
        |         try: await _unix_socket_server_instance.wait_closed()
        |         except Exception as e_wait: logger.error(f"Error during wait_closed for UNIX server: {e_wait}")
        |         _unix_socket_server_instance = None
        |         logger.info("UNIX server socket now closed.")
        | 
        |     if _active_unix_client_tasks:
        |         logger.info(f"Cancelling {len(_active_unix_client_tasks)} active UNIX client tasks...")
        |         for task in list(_active_unix_client_tasks): task.cancel()
        |         results = await asyncio.gather(*_active_unix_client_tasks, return_exceptions=True)
        |         for i, res in enumerate(results):
        |             if isinstance(res, Exception) and not isinstance(res, asyncio.CancelledError):
        |                 logger.error(f"Error in cancelled UNIX client task {i}: {res}")
        |         _active_unix_client_tasks.clear()
        |         logger.info("Active UNIX client tasks finished processing.")
        |     
        |     if GATEWAY_UNIX_SOCKET_PATH.exists():
        |         try: GATEWAY_UNIX_SOCKET_PATH.unlink()
        |         except OSError as e_unlink: logger.error(f"Error removing UNIX socket file on stop: {e_unlink}")
        |     logger.info("UNIX socket server fully stopped.")
        | 
        | async def _handle_single_unix_client_task(reader: asyncio.StreamReader, writer: asyncio.StreamWriter):
        |     peername = writer.get_extra_info('peername', 'unknown_unix_peer')
        |     client_desc = f"unix_client_{str(peername).replace('/', '_').replace(':', '_')}" # Make it more filename friendly
        |     logger.info(f"UNIX socket client connected: {client_desc}")
        |     mock_client_ctx = MockUnixClientContext(peername)
        |     message_buffer = bytearray()
        | 
        |     try:
        |         while not _shutdown_event_flag.is_set():
        |             try:
        |                 chunk = await asyncio.wait_for(reader.read(4096), timeout=1.0)
        |                 if not chunk: logger.info(f"UNIX client {client_desc} disconnected (EOF)."); break
        |                 message_buffer.extend(chunk)
        | 
        |                 while b'\0' in message_buffer:
        |                     if _shutdown_event_flag.is_set(): break
        |                     message_bytes, rest_of_buffer = message_buffer.split(b'\0', 1)
        |                     message_buffer = rest_of_buffer
        |                     message_str = message_bytes.decode('utf-8')
        |                     logger.debug(f"UNIX RCV from {client_desc}: {message_str[:250]}...")
        |                     
        |                     try: request_data = json.loads(message_str)
        |                     except json.JSONDecodeError:
        |                         err_resp = create_mcp_error(None, JSONRPC_PARSE_ERROR, "Invalid JSON payload.")
        |                         writer.write(json.dumps(err_resp).encode('utf-8') + b'\0'); await writer.drain()
        |                         continue
        | 
        |                     req_id = request_data.get("id"); method_name = request_data.get("method")
        |                     if not isinstance(method_name, str):
        |                         err_resp = create_mcp_error(req_id, JSONRPC_INVALID_REQUEST, "Method name missing or not a string.")
        |                         writer.write(json.dumps(err_resp).encode('utf-8') + b'\0'); await writer.drain()
        |                         continue
        |                     logger.debug(f"GATEWAY MAIN: Raw request_data.params from client: {request_data.get('params')}") # <<< AJOUTER CE LOG
        |                     # Note: authenticate_and_authorize_request is a placeholder from previous versions.
        |                     # Ensure its signature matches and it handles MockUnixClientContext correctly.
        |                     licence_ctx, auth_error_obj = authenticate_and_authorize_request(mock_client_ctx, method_name) # type: ignore
        |                     
        |                     if auth_error_obj:
        |                         logger.warning(f"Auth failed for UNIX {client_desc}, method '{method_name}': {auth_error_obj['message']}")
        |                         err_resp = create_mcp_error(req_id, auth_error_obj["code"], auth_error_obj["message"], auth_error_obj.get("data"))
        |                         writer.write(json.dumps(err_resp).encode('utf-8') + b'\0'); await writer.drain()
        |                         if auth_error_obj["code"] == JSONRPC_AUTH_ERROR: break 
        |                         continue
        |                     if not licence_ctx:
        |                         logger.error(f"Internal auth error for UNIX {client_desc}. Denying.")
        |                         err_resp = create_mcp_error(req_id, JSONRPC_INTERNAL_ERROR, "Internal authentication error.")
        |                         writer.write(json.dumps(err_resp).encode('utf-8') + b'\0'); await writer.drain(); continue
        | 
        |                     response_or_generator = await dispatch.handle_mcp_request(request_data, licence_ctx, mock_client_ctx) # type: ignore
        | 
        |                     if isinstance(response_or_generator, AsyncGenerator):
        |                         async for stream_chunk_resp in response_or_generator:
        |                             if _shutdown_event_flag.is_set(): break
        |                             writer.write(json.dumps(stream_chunk_resp).encode('utf-8') + b'\0')
        |                             await writer.drain()
        |                         if _shutdown_event_flag.is_set(): break
        |                         logger.debug(f"Finished streaming to UNIX {client_desc} (ID {req_id})")
        |                     else:
        |                         writer.write(json.dumps(response_or_generator).encode('utf-8') + b'\0')
        |                         await writer.drain()
        |             
        |             except asyncio.TimeoutError: continue # Timeout on read, allows checking _shutdown_event_flag
        |             except (asyncio.IncompleteReadError, ConnectionResetError, BrokenPipeError):
        |                 logger.info(f"UNIX client {client_desc} connection issue."); break
        |             except UnicodeDecodeError: logger.error(f"UNIX client {client_desc} sent invalid UTF-8."); break
        |         if _shutdown_event_flag.is_set(): logger.info(f"UNIX client {client_desc} handler exiting due to shutdown signal.")
        |     except asyncio.CancelledError: logger.info(f"UNIX client task for {client_desc} cancelled.")
        |     except Exception as e_client: logger.error(f"Error in UNIX client handler for {client_desc}: {e_client}", exc_info=True)
        |     finally:
        |         logger.info(f"Closing UNIX connection for {client_desc}")
        |         if not writer.is_closing(): 
        |             try: writer.close(); await writer.wait_closed()
        |             except Exception as e_close: logger.debug(f"Error closing writer for {client_desc}: {e_close}")
        | 
        | async def _run_unix_socket_client_handler_managed(reader: asyncio.StreamReader, writer: asyncio.StreamWriter):
        |     task = asyncio.current_task()
        |     _active_unix_client_tasks.add(task) # type: ignore
        |     try:
        |         await _handle_single_unix_client_task(reader, writer)
        |     finally:
        |         _active_unix_client_tasks.discard(task) # type: ignore
        | 
        | # --- Gestionnaire de Lifespan FastAPI ---
        | @asynccontextmanager
        | async def lifespan_manager(app_fastapi: FastAPI): # Renommé 'app' en 'app_fastapi' pour éviter conflit
        |     global logger, _capability_watcher_task_instance, _unix_socket_server_instance
        | 
        |     setup_gateway_logging()
        |     logger = logging.getLogger("llmbasedos.gateway.main") # S'assurer que logger est bien celui configuré
        |     logger.info("Gateway Lifespan: Startup sequence initiated...")
        | 
        |     loop = asyncio.get_running_loop()
        |     active_signals = []
        |     def _shutdown_signal_handler(sig: signal.Signals): # Wrapper non-async
        |         logger.info(f"Signal {sig.name} received by lifespan, setting shutdown event...")
        |         if not _shutdown_event_flag.is_set():
        |             loop.call_soon_threadsafe(_shutdown_event_flag.set) # Thread-safe way to set event from signal handler
        |     
        |     for sig_val in (signal.SIGINT, signal.SIGTERM):
        |         try:
        |             loop.add_signal_handler(sig_val, lambda s=sig_val: _shutdown_signal_handler(s))
        |             active_signals.append(sig_val)
        |         except (ValueError, RuntimeError) as e_signal: # ex: not in main thread
        |              logger.warning(f"Could not set signal handler for {sig_val}: {e_signal}. Relying on Uvicorn for shutdown.")
        | 
        | 
        |     if hasattr(registry, 'start_capability_watcher_task') and callable(registry.start_capability_watcher_task):
        |         _capability_watcher_task_instance = asyncio.create_task(registry.start_capability_watcher_task(), name="CapabilityWatcher")
        |         logger.info("Capability watcher task created.")
        |     else:
        |         logger.warning("registry.start_capability_watcher_task not found or not callable.")
        |     
        |     await _start_unix_socket_server_logic()
        |     logger.info("Gateway Lifespan: Startup complete. Application is ready.")
        |     
        |     try:
        |         yield # L'application tourne
        |     finally:
        |         logger.info("Gateway Lifespan: Shutdown sequence initiated (from finally block)...")
        |         if not _shutdown_event_flag.is_set(): # Si shutdown n'a pas été initié par signal
        |             _shutdown_event_flag.set() # Déclencher l'arrêt des tâches en background
        | 
        |         if _capability_watcher_task_instance and not _capability_watcher_task_instance.done():
        |             logger.info("Cancelling capability watcher task...")
        |             _capability_watcher_task_instance.cancel()
        |             try: await _capability_watcher_task_instance
        |             except asyncio.CancelledError: logger.info("Capability watcher task successfully cancelled.")
        |             except Exception as e_watch_stop: logger.error(f"Error stopping watcher task: {e_watch_stop}", exc_info=True)
        |         
        |         await _stop_unix_socket_server_logic()
        |         
        |         if hasattr(dispatch, 'shutdown_dispatch_executor') and callable(dispatch.shutdown_dispatch_executor):
        |             try: dispatch.shutdown_dispatch_executor()
        |             except Exception as e_disp_shutdown : logger.error(f"Error shutting down dispatch executor: {e_disp_shutdown}")
        |             logger.info("Dispatch executor shutdown requested.")
        |         
        |         # Retirer les gestionnaires de signaux
        |         for sig_val in active_signals:
        |             try: loop.remove_signal_handler(sig_val)
        |             except Exception as e_rem_sig: logger.debug(f"Error removing signal handler for {sig_val}: {e_rem_sig}")
        |         
        |         logger.info("Gateway Lifespan: Shutdown complete.")
        | 
        | # Initialisation de l'application FastAPI
        | app = FastAPI(
        |     title="llmbasedos MCP Gateway",
        |     description="Central router for Model Context Protocol requests.",
        |     version="0.1.2", # Version incrémentée
        |     lifespan=lifespan_manager
        | )
        | 
        | # --- WebSocket Endpoint ---
        | @app.websocket("/ws")
        | async def websocket_mcp_endpoint(websocket: WebSocket):
        |     await websocket.accept()
        |     client_addr = f"{websocket.client.host}:{websocket.client.port}" # type: ignore # client peut être None en théorie
        |     logger.info(f"WebSocket client connected: {client_addr}")
        |     
        |     try:
        |         while not _shutdown_event_flag.is_set(): # Vérifier l'arrêt
        |             try:
        |                 # Utiliser un timeout pour permettre de vérifier _shutdown_event_flag
        |                 raw_data = await asyncio.wait_for(websocket.receive_text(), timeout=1.0)
        |             except asyncio.TimeoutError:
        |                 # Vérifier si le client est toujours connecté (ping/pong implicite de websockets)
        |                 if websocket.client_state != WebSocketState.CONNECTED:
        |                     logger.info(f"WS client {client_addr} appears disconnected after read timeout.")
        |                     break
        |                 continue # Timeout, on revérifie _shutdown_event_flag et on réessaie de lire
        | 
        |             logger.debug(f"WS RCV from {client_addr}: {raw_data[:250]}...")
        |             
        |             try: request_data = json.loads(raw_data)
        |             except json.JSONDecodeError:
        |                 err_resp = create_mcp_error(None, JSONRPC_PARSE_ERROR, "Invalid JSON payload.")
        |                 await websocket.send_text(json.dumps(err_resp)); continue
        | 
        |             request_id = request_data.get("id"); method_name = request_data.get("method")
        |             if not isinstance(method_name, str):
        |                 err_resp = create_mcp_error(request_id, JSONRPC_INVALID_REQUEST, "Method name missing or not a string.")
        |                 await websocket.send_text(json.dumps(err_resp)); continue
        |             logger.debug(f"Extracted method_name from request: '{method_name}'")
        |             licence_ctx, auth_error_obj = authenticate_and_authorize_request(websocket, method_name)
        |             
        |             if auth_error_obj:
        |                 logger.warning(f"Auth failed for WS {client_addr}, method '{method_name}': {auth_error_obj['message']}")
        |                 err_resp = create_mcp_error(request_id, auth_error_obj["code"], auth_error_obj["message"], auth_error_obj.get("data"))
        |                 await websocket.send_text(json.dumps(err_resp))
        |                 if auth_error_obj["code"] == JSONRPC_AUTH_ERROR: # type: ignore # JSONRPC_AUTH_ERROR est bien dans config
        |                     await websocket.close(code=1008); break
        |                 continue
        |             if not licence_ctx:
        |                 logger.error(f"Internal auth error for WS {client_addr}. Denying.");
        |                 err_resp = create_mcp_error(request_id, JSONRPC_INTERNAL_ERROR, "Internal authentication error.")
        |                 await websocket.send_text(json.dumps(err_resp)); continue
        | 
        |             response_or_generator = await dispatch.handle_mcp_request(request_data, licence_ctx, websocket)
        | 
        |             if isinstance(response_or_generator, AsyncGenerator):
        |                 logger.info(f"Streaming response to WS {client_addr} for '{method_name}' (ID {request_id})")
        |                 try:
        |                     async for stream_chunk_resp in response_or_generator:
        |                         if _shutdown_event_flag.is_set(): break # Arrêter le stream si shutdown
        |                         await websocket.send_text(json.dumps(stream_chunk_resp))
        |                     if _shutdown_event_flag.is_set(): break
        |                     logger.debug(f"Finished streaming to WS {client_addr} (ID {request_id})")
        |                 except WebSocketDisconnect: logger.info(f"WS {client_addr} disconnected during stream (ID {request_id})."); break
        |                 except Exception as stream_exc:
        |                     logger.error(f"Error streaming to WS {client_addr} (ID {request_id}): {stream_exc}", exc_info=True)
        |                     if websocket.client_state == WebSocketState.CONNECTED:
        |                          try: await websocket.send_text(json.dumps(create_mcp_error(request_id, JSONRPC_INTERNAL_ERROR, "Streaming error.")))
        |                          except: pass # Ignorer si on ne peut plus envoyer
        |                     break 
        |             else: # Single JSON-RPC response dict
        |                 logger.debug(f"WS SEND to {client_addr} (ID {request_id}): {str(response_or_generator)[:250]}...")
        |                 await websocket.send_text(json.dumps(response_or_generator))
        |         if _shutdown_event_flag.is_set(): logger.info(f"WebSocket handler for {client_addr} exiting due to shutdown signal.")
        | 
        |     except WebSocketDisconnect: logger.info(f"WebSocket client {client_addr} disconnected.")
        |     except asyncio.CancelledError: logger.info(f"WebSocket task for {client_addr} cancelled.") # Peut arriver sur shutdown rapide
        |     except Exception as e:
        |         logger.error(f"Error in WebSocket handler for {client_addr}: {e}", exc_info=True)
        |         if hasattr(websocket, 'client_state') and websocket.client_state == WebSocketState.CONNECTED:
        |             try: await websocket.close(code=1011)
        |             except: pass
        |     finally:
        |         logger.info(f"WebSocket connection cleanup for {client_addr}")
        |         # La fermeture explicite est déjà gérée dans les blocs except ou par le client.
        |         # S'assurer que le client sait que la connexion est finie.
        | 
        | # --- Point d'Entrée Principal (pour Uvicorn) ---
        | def run_gateway_service():
        |     uvicorn.run(
        |         "llmbasedos.gateway.main:app",
        |         host=GATEWAY_HOST, port=GATEWAY_WEB_PORT,
        |         log_config=None, # Laisser le lifespan manager configurer le logging
        |         # workers=1 # Important pour la gestion de l'état global (_shutdown_event_flag, etc.)
        |                   # Si workers > 1, il faut une communication inter-processus pour les signaux.
        |     )
        | 
        | if __name__ == "__main__":
        |     run_gateway_service()
        --- Fin Contenu ---
      Fichier: registry.py
        --- Début Contenu (ascii) ---
        | # llmbasedos/gateway/registry.py
        | import json
        | import logging
        | from pathlib import Path
        | from typing import Dict, Any, List, Optional, Tuple
        | import asyncio
        | 
        | from watchdog.observers import Observer
        | from watchdog.events import FileSystemEventHandler, FileModifiedEvent, FileCreatedEvent, FileDeletedEvent, DirCreatedEvent
        | 
        | from .config import MCP_CAPS_DIR
        | 
        | logger = logging.getLogger("llmbasedos.gateway.registry")
        | 
        | CAPABILITY_REGISTRY: Dict[str, Dict[str, Any]] = {} # method_name -> routing_info
        | RAW_CAPS_REGISTRY: Dict[str, Dict[str, Any]] = {}   # service_name -> raw_caps_data
        | 
        | def _clear_service_from_registry(service_name: str):
        |     methods_to_remove = [mname for mname, details in CAPABILITY_REGISTRY.items() if details.get("service_name") == service_name]
        |     for mname in methods_to_remove:
        |         del CAPABILITY_REGISTRY[mname]
        |         logger.debug(f"Unregistered method: {mname}")
        |     if service_name in RAW_CAPS_REGISTRY:
        |         del RAW_CAPS_REGISTRY[service_name]
        | 
        | def _load_capability_file(file_path: Path) -> bool:
        |     if not file_path.name.endswith(".cap.json"): return False
        |     service_name = file_path.name.removesuffix(".cap.json")
        |     socket_path_str = str(MCP_CAPS_DIR / f"{service_name}.sock") # Convention
        | 
        |     _clear_service_from_registry(service_name) # Clear old entries first
        | 
        |     try:
        |         with file_path.open('r') as f: cap_data = json.load(f)
        |         if not isinstance(cap_data, dict):
        |             logger.error(f"Invalid format in {file_path}: not a dictionary."); return False
        |         
        |         RAW_CAPS_REGISTRY[service_name] = cap_data
        |         logger.info(f"Loaded raw capabilities for '{service_name}' from {file_path.name}")
        | 
        |         capabilities_list = cap_data.get("capabilities")
        |         if not isinstance(capabilities_list, list):
        |             logger.error(f"Invalid {file_path.name}: 'capabilities' missing or not a list."); return False
        | 
        |         for cap_item in capabilities_list:
        |             if not isinstance(cap_item, dict): continue
        |             method_name = cap_item.get("method")
        |             if not method_name or not isinstance(method_name, str): continue
        | 
        |             expected_prefix = f"mcp.{service_name}."
        |             if not method_name.startswith(expected_prefix):
        |                 logger.warning(f"Method '{method_name}' in {file_path.name} for service '{service_name}' "
        |                                f"does not follow '{expected_prefix}action' convention. Registering anyway.")
        |             
        |             if method_name in CAPABILITY_REGISTRY:
        |                 logger.warning(f"Method '{method_name}' from {file_path.name} conflicts with existing from "
        |                                f"'{CAPABILITY_REGISTRY[method_name]['service_name']}'. Overwriting.")
        | 
        |             CAPABILITY_REGISTRY[method_name] = {
        |                 "socket_path": socket_path_str, "service_name": service_name,
        |                 "method_definition": cap_item, # Includes params_schema, description etc.
        |             }
        |             logger.debug(f"Registered method: {method_name} -> {socket_path_str}")
        |         return True
        |     except Exception as e:
        |         logger.error(f"Error processing capability file {file_path}: {e}", exc_info=True)
        |     return False
        | 
        | def discover_capabilities(initial_load: bool = False):
        |     """Discovers all .cap.json files. If initial_load, clears registries first."""
        |     if initial_load:
        |         CAPABILITY_REGISTRY.clear()
        |         RAW_CAPS_REGISTRY.clear()
        |         logger.info(f"Discovering capabilities in {MCP_CAPS_DIR}...")
        |     
        |     if not MCP_CAPS_DIR.exists() or not MCP_CAPS_DIR.is_dir():
        |         if initial_load: logger.warning(f"Caps dir {MCP_CAPS_DIR} not found. No capabilities loaded.")
        |         return
        | 
        |     loaded_count = 0
        |     for file_path in MCP_CAPS_DIR.glob("*.cap.json"):
        |         # On initial load, always try to load. On subsequent calls (from watcher),
        |         # _load_capability_file handles clearing existing entries for that service.
        |         if _load_capability_file(file_path):
        |             loaded_count +=1
        |     
        |     if initial_load or loaded_count > 0:
        |         logger.info(f"Capability discovery/update complete. {len(CAPABILITY_REGISTRY)} methods from "
        |                     f"{len(RAW_CAPS_REGISTRY)} services registered.")
        | 
        | def get_capability_routing_info(method_name: str) -> Optional[Dict[str, Any]]:
        |     return CAPABILITY_REGISTRY.get(method_name)
        | 
        | def get_all_registered_method_names() -> List[str]:
        |     return sorted(list(CAPABILITY_REGISTRY.keys()))
        | 
        | def get_detailed_capabilities_list() -> List[Dict[str, Any]]:
        |     return [
        |         {"service_name": s_name, "description": raw.get("description", "N/A"),
        |          "version": raw.get("version", "N/A"), "capabilities": raw.get("capabilities", [])}
        |         for s_name, raw in RAW_CAPS_REGISTRY.items()
        |     ]
        | 
        | class CapsFileEventHandler(FileSystemEventHandler):
        |     def _trigger_rediscovery_for_path(self, path_str: str, action: str):
        |         event_path = Path(path_str)
        |         if event_path.name.endswith(".cap.json"):
        |             logger.info(f"Capability file {action}: {event_path.name}. Updating registry.")
        |             if action == "deleted":
        |                 service_name = event_path.name.removesuffix(".cap.json")
        |                 _clear_service_from_registry(service_name)
        |                 logger.info(f"Unloaded capabilities for service '{service_name}'.")
        |             else: # created or modified
        |                 _load_capability_file(event_path) # Reloads specific file
        |         # No need to rediscover all on single file change, specific load/unload is better.
        | 
        |     def on_created(self, event): # FileSystemEvent or DirCreatedEvent
        |         if isinstance(event, DirCreatedEvent) and Path(event.src_path) == MCP_CAPS_DIR:
        |             logger.info(f"Capability directory {MCP_CAPS_DIR} created. Re-discovering all.")
        |             discover_capabilities(initial_load=False) # Rescan all
        |         elif not event.is_directory:
        |             self._trigger_rediscovery_for_path(event.src_path, "created")
        | 
        |     def on_modified(self, event):
        |         if not event.is_directory: self._trigger_rediscovery_for_path(event.src_path, "modified")
        | 
        |     def on_deleted(self, event):
        |         if not event.is_directory: self._trigger_rediscovery_for_path(event.src_path, "deleted")
        | 
        | _WATCHDOG_OBSERVER: Optional[Observer] = None
        | 
        | async def start_capability_watcher_task():
        |     """Async task to run the watchdog observer."""
        |     global _WATCHDOG_OBSERVER
        |     
        |     # Ensure MCP_CAPS_DIR exists before starting watcher, watchdog might fail otherwise
        |     if not MCP_CAPS_DIR.exists():
        |         try:
        |             logger.info(f"MCP_CAPS_DIR {MCP_CAPS_DIR} does not exist. Attempting to create.")
        |             MCP_CAPS_DIR.mkdir(parents=True, exist_ok=True)
        |         except OSError as e:
        |             logger.error(f"Failed to create MCP_CAPS_DIR {MCP_CAPS_DIR}: {e}. Watcher may not start correctly.")
        |             # Depending on Watchdog version/OS, it might still watch a non-existent path's parent.
        |             
        |     discover_capabilities(initial_load=True) # Initial full discovery
        | 
        |     event_handler = CapsFileEventHandler()
        |     _WATCHDOG_OBSERVER = Observer()
        |     try:
        |         _WATCHDOG_OBSERVER.schedule(event_handler, str(MCP_CAPS_DIR), recursive=False)
        |         _WATCHDOG_OBSERVER.start()
        |         logger.info(f"Started capability watcher on {MCP_CAPS_DIR}")
        |     except Exception as e:
        |         logger.error(f"Failed to start watchdog on {MCP_CAPS_DIR}: {e}. Dynamic updates may fail.", exc_info=True)
        |         _WATCHDOG_OBSERVER = None # Ensure it's None if start failed
        |         return # Cannot continue if observer fails to start
        | 
        |     try:
        |         while _WATCHDOG_OBSERVER and _WATCHDOG_OBSERVER.is_alive(): # Check _WATCHDOG_OBSERVER for None
        |             await asyncio.sleep(1) # Keep task alive, observer runs in own thread
        |     except asyncio.CancelledError:
        |         logger.info("Capability watcher task cancelled.")
        |     finally:
        |         if _WATCHDOG_OBSERVER and _WATCHDOG_OBSERVER.is_alive():
        |             _WATCHDOG_OBSERVER.stop()
        |         if _WATCHDOG_OBSERVER: # Join only if it was started
        |             _WATCHDOG_OBSERVER.join(timeout=5) # Wait for observer thread to finish
        |         _WATCHDOG_OBSERVER = None
        |         logger.info("Capability watcher stopped.")
        | 
        | async def stop_capability_watcher(): # Called on shutdown
        |     global _WATCHDOG_OBSERVER
        |     if _WATCHDOG_OBSERVER and _WATCHDOG_OBSERVER.is_alive():
        |         logger.info("Stopping capability watcher...")
        |         _WATCHDOG_OBSERVER.stop()
        |     # Join is handled by the task's finally block.
        --- Fin Contenu ---
      Fichier: requirements.txt
        --- Début Contenu (ascii) ---
        | # llmbasedos/gateway/requirements.txt
        | fastapi>=0.100.0,<0.111.0
        | uvicorn[standard]>=0.20.0 # [standard] includes websockets, cython-based http-tools, etc.
        | websockets>=10.0 # Explicitly for unix socket client in dispatch, uvicorn brings its own for server.
        | pydantic>=2.0.0
        | python-json-logger>=2.0.0 # For structured logging
        | pyyaml>=6.0 # For config, licence tiers, etc.
        | watchdog>=3.0.0 # For monitoring .cap.json files
        | httpx>=0.25.0 # For async HTTP requests to OpenAI/llama.cpp
        --- Fin Contenu ---
      Fichier: upstream.py
        --- Début Contenu (utf-8) ---
        | import logging
        | import httpx
        | import json
        | from typing import Any, Dict, List, Optional, AsyncGenerator, Tuple
        | 
        | # Importez UNIQUEMENT les constantes de config.py qui sont réellement nécessaires au niveau du module ici.
        | # Les clés spécifiques (OPENAI_API_KEY) et les URLs de base seront récupérées via AVAILABLE_LLM_MODELS.
        | from .config import (
        |     AVAILABLE_LLM_MODELS,   # La structure qui définit tous les modèles et leurs détails
        |     DEFAULT_LLM_PROVIDER,   # Pour résoudre un alias par défaut
        |     # OPENAI_API_KEY est maintenant supposé être DANS AVAILABLE_LLM_MODELS pour les modèles openai, ou une variable globale
        |     # que _get_model_config vérifie. Pour simplifier, je vais supposer que _get_model_config
        |     # récupère aussi la clé API si nécessaire à partir de AVAILABLE_LLM_MODELS ou d'une config globale.
        |     # Assurons-nous que OPENAI_API_KEY est disponible si un modèle openai est sélectionné.
        |     OPENAI_API_KEY # Nécessaire si la fonction _get_model_config ne la récupère pas d'ailleurs
        | )
        | from .auth import LicenceDetails
        | 
        | logger = logging.getLogger("llmbasedos.gateway.upstream")
        | 
        | class LLMError(Exception):
        |     def __init__(self, message, provider=None, model_name=None, status_code=None, details=None):
        |         super().__init__(message)
        |         self.provider = provider
        |         self.model_name = model_name
        |         self.status_code = status_code
        |         self.details = details
        |     def __str__(self):
        |         return f"LLMError (Provider: {self.provider}, Model: {self.model_name}, Status: {self.status_code}): {self.args[0]}"
        | 
        | def _get_model_config(requested_model_alias: Optional[str]) -> Tuple[str, str, str, Optional[str]]:
        |     """
        |     Resolves a model alias to (provider_type, provider_model_name, api_base_url, api_key_if_needed).
        |     API key is specifically for OpenAI in this example.
        |     Raises ValueError if alias not found or config incomplete.
        |     """
        |     # Construct a default alias if none is provided
        |     effective_alias = requested_model_alias
        |     if not effective_alias:
        |         # Find a model that is marked as default for the DEFAULT_LLM_PROVIDER
        |         for alias, config_data in AVAILABLE_LLM_MODELS.items():
        |             if config_data.get("provider") == DEFAULT_LLM_PROVIDER and config_data.get("is_default", False):
        |                 effective_alias = alias
        |                 break
        |         if not effective_alias: # Fallback if no specific default found for the provider
        |             raise ValueError(f"No default LLM model alias found for provider '{DEFAULT_LLM_PROVIDER}'.")
        |     
        |     model_config = AVAILABLE_LLM_MODELS.get(effective_alias)
        |     if not model_config:
        |         raise ValueError(f"LLM model alias '{effective_alias}' not found in AVAILABLE_LLM_MODELS.")
        | 
        |     provider_type = model_config.get("provider")
        |     provider_model_name = model_config.get("model_name")
        |     api_base_url = model_config.get("api_base_url")
        |     api_key = model_config.get("api_key") # Can be None
        | 
        |     if not all([provider_type, provider_model_name, api_base_url]):
        |         raise ValueError(f"Incomplete configuration for model alias '{effective_alias}': provider, model_name, and api_base_url are required.")
        | 
        |     if provider_type == "openai":
        |         # For OpenAI, an API key is essential. It can come from model_config or global OPENAI_API_KEY.
        |         # Prioritize key from model_config if present.
        |         key_to_use = api_key or OPENAI_API_KEY # OPENAI_API_KEY from .config import
        |         if not key_to_use:
        |             raise ValueError(f"OpenAI API key not configured for model alias '{effective_alias}' or globally.")
        |         api_key = key_to_use # Ensure api_key is set for return
        |     elif provider_type == "llama_cpp":
        |         # llama.cpp OpenAI-compatible endpoint usually doesn't require an API key.
        |         pass # api_key will be None if not set in config
        |     else:
        |         raise ValueError(f"Unsupported LLM provider type: {provider_type} for model alias '{effective_alias}'.")
        |     
        |     return provider_type, provider_model_name, api_base_url.rstrip('/'), api_key
        | 
        | 
        | async def call_llm_chat_completion(
        |     messages: List[Dict[str, str]],
        |     licence: LicenceDetails,
        |     requested_model_alias: Optional[str] = None,
        |     stream: bool = False,
        |     **kwargs: Any # temperature, max_tokens etc.
        | ) -> AsyncGenerator[Dict[str, Any], None]:
        |     """
        |     Proxies chat completion to configured LLM.
        |     Caller (dispatch.py) handles auth.py checks for model permission and quotas before calling this.
        |     This function focuses on the HTTP call and streaming.
        |     """
        |     try:
        |         provider_type, actual_model_name, api_base_url, provider_api_key = _get_model_config(requested_model_alias)
        |     except ValueError as e_conf:
        |         logger.error(f"LLM model config error for alias '{requested_model_alias}': {e_conf}")
        |         yield {"event": "error", "data": {"message": str(e_conf), "provider": "config", "model_name": requested_model_alias or "default"}}
        |         return
        | 
        |     endpoint = f"{api_base_url}/chat/completions" # Common for OpenAI-compatible APIs
        |     headers = {"Content-Type": "application/json"}
        |     if provider_type == "openai" and provider_api_key:
        |         headers["Authorization"] = f"Bearer {provider_api_key}"
        |     
        |     payload = {"model": actual_model_name, "messages": messages, "stream": stream, **kwargs}
        |     logger.debug(f"OpenAI Payload: {json.dumps(payload, indent=2)}") # Logguer le payloa
        |     timeout_seconds = 300.0
        |     try:
        |         async with httpx.AsyncClient(timeout=timeout_seconds) as client:
        |             logger.debug(f"LLM Req to {endpoint} (Model: {actual_model_name}, Stream: {stream}): {json.dumps(payload, indent=2)[:500]}...")
        |             async with client.stream("POST", endpoint, headers=headers, json=payload) as response:
        |                 if response.status_code >= 400:
        |                     error_body = await response.aread()
        |                     error_text = error_body.decode(errors='ignore')
        |                     logger.error(f"LLM API error {response.status_code} from {provider_type}/{actual_model_name}: {error_text}")
        |                     yield {"event": "error", "data": {
        |                         "message": f"LLM API Error {response.status_code}",
        |                         "provider": provider_type, "model_name": actual_model_name,
        |                         "status_code": response.status_code, "details": error_text
        |                     }}
        |                     return
        | 
        |                 if stream:
        |                     async for line in response.aiter_lines():
        |                         if line.startswith("data: "):
        |                             line_data = line.removeprefix("data: ").strip()
        |                             if line_data == "[DONE]":
        |                                 yield {"event": "done", "data": {"provider": provider_type, "model_name": actual_model_name}}
        |                                 break
        |                             try:
        |                                 chunk = json.loads(line_data)
        |                                 logger.debug(f"GW RCV CHUNK FROM LLM API: {chunk}") # Logguer le chunk brut de l'API
        |                                 yield {"event": "chunk", "data": chunk}
        |                             except json.JSONDecodeError:
        |                                 logger.warning(f"Failed to decode LLM stream chunk from {provider_type}: {line_data}")
        |                         elif line.strip():
        |                              logger.warning(f"Unexpected line in LLM stream from {provider_type}: {line}")
        |                 else:
        |                     full_response_bytes = await response.aread()
        |                     try:
        |                         result = json.loads(full_response_bytes)
        |                         yield {"event": "chunk", "data": result}
        |                         yield {"event": "done", "data": {"provider": provider_type, "model_name": actual_model_name}}
        |                     except json.JSONDecodeError:
        |                         error_text = full_response_bytes.decode(errors='ignore')
        |                         logger.error(f"Failed to decode full LLM response from {provider_type}: {error_text}")
        |                         yield {"event": "error", "data": {
        |                             "message": "Invalid JSON response from LLM (non-stream)",
        |                             "provider": provider_type, "model_name": actual_model_name,
        |                             "details": error_text
        |                         }}
        |     except httpx.RequestError as e_req:
        |         logger.error(f"LLM request error to {provider_type}/{actual_model_name}: {e_req}", exc_info=True)
        |         yield {"event": "error", "data": {
        |             "message": f"LLM Request Error: {type(e_req).__name__}",
        |             "provider": provider_type, "model_name": actual_model_name, "details": str(e_req)
        |         }}
        |     except Exception as e_gen:
        |         logger.error(f"Unexpected error during LLM call to {provider_type}/{actual_model_name}: {e_gen}", exc_info=True)
        |         yield {"event": "error", "data": {
        |             "message": f"Unexpected error in LLM proxy: {type(e_gen).__name__}",
        |             "provider": provider_type, "model_name": actual_model_name, "details": str(e_gen)
        |         }}
        --- Fin Contenu ---

    Répertoire: ./llmbasedos_src/servers

      Répertoire: ./llmbasedos_src/servers/agent
        Fichier: caps.json
          --- Début Contenu (ascii) ---
          | {
          |     "service_name": "agent",
          |     "description": "Manages and executes agentic workflows.",
          |     "version": "0.1.1",
          |     "capabilities": [
          |         {
          |             "method": "mcp.agent.listWorkflows",
          |             "description": "Lists available agent workflows.",
          |             "params_schema": { "type": "array", "maxItems": 0 },
          |             "result_schema": {
          |                 "type": "array", "items": {
          |                     "type": "object", "properties": {
          |                         "workflow_id": {"type": "string"}, "name": {"type": "string"},
          |                         "description": {"type": ["string", "null"]},
          |                         "input_schema": {"type": ["object", "null"], "description": "JSON schema for inputs."}
          |                     }, "required": ["workflow_id", "name"]
          |                 }
          |             }
          |         },
          |         {
          |             "method": "mcp.agent.runWorkflow",
          |             "description": "Executes a workflow with given inputs.",
          |             "params_schema": {
          |                 "type": "array", "minItems": 1, "maxItems": 2, "items": [
          |                     {"type": "string", "description": "workflow_id"},
          |                     {"type": "object", "optional": true, "description": "Input parameters."}
          |                 ]
          |             },
          |             "result_schema": {
          |                 "type": "object", "properties": {
          |                     "execution_id": {"type": "string"}, "workflow_id": {"type": "string"},
          |                     "status": {"type": "string", "enum": ["pending", "started", "running", "completed", "failed"]},
          |                     "output": {"type": ["object", "null"]}, "error_message": {"type": ["string", "null"]}
          |                 }, "required": ["execution_id", "workflow_id", "status"]
          |             }
          |         },
          |         {
          |             "method": "mcp.agent.getWorkflowStatus",
          |             "description": "Gets status of a workflow execution.",
          |             "params_schema": { "type": "array", "minItems": 1, "maxItems": 1, "items": [{"type": "string", "description": "execution_id"}]},
          |             "result_schema": {
          |                 "type": "object", "properties": {
          |                     "execution_id": {"type": "string"}, "workflow_id": {"type": "string"},
          |                     "status": {"type": "string", "enum": ["pending", "running", "completed", "failed", "cancelled", "cancelling"]},
          |                     "start_time": {"type": ["string", "null"], "format": "date-time"},
          |                     "end_time": {"type": ["string", "null"], "format": "date-time"},
          |                     "output": {"type": ["object", "null"]}, "error_message": {"type": ["string", "null"]},
          |                     "log_preview": {"type": "array", "items": {"type": "string"}, "optional": true}
          |                 }, "required": ["execution_id", "workflow_id", "status"]
          |             }
          |         },
          |         {
          |             "method": "mcp.agent.stopWorkflow",
          |             "description": "Attempts to stop a running workflow execution.",
          |              "params_schema": { "type": "array", "minItems": 1, "maxItems": 1, "items": [{"type": "string", "description": "execution_id"}]},
          |             "result_schema": {
          |                 "type": "object", "properties": {
          |                     "execution_id": {"type": "string"},
          |                     "status": {"type": "string", "enum": ["stop_requested", "not_running", "already_completed", "failed_to_stop", "not_stoppable"]},
          |                     "message": {"type": ["string", "null"]}
          |                 }, "required": ["execution_id", "status"]
          |             }
          |         }
          |     ]
          | }
          --- Fin Contenu ---
        Fichier: requirements.txt
          --- Début Contenu (ascii) ---
          | # llmbasedos/servers/agent/requirements.txt
          | pyyaml>=6.0
          | docker>=6.0.0 # For Docker-based agents
          | requests>=2.25.0 # For HTTP-based agents like n8n-lite
          | # aiohttp if async HTTP calls are preferred for n8n-lite from async handlers
          --- Fin Contenu ---
        Fichier: server.py
          --- Début Contenu (ascii) ---
          | # llmbasedos/servers/agent/server.py
          | import asyncio
          | # Logger is obtained from MCPServer instance, no need for direct logging import here if using server.logger
          | import os
          | from pathlib import Path
          | import uuid
          | import yaml
          | import docker # For Docker-based agents
          | import requests # For HTTP-based agents (n8n-lite)
          | import threading # For managing long-running workflow execution threads
          | import time # For sleep in simple workflows or polling
          | from typing import Any, Dict, List, Optional, Union, Callable
          | from datetime import datetime, timezone
          | import subprocess # For script-based agents if needed
          | 
          | # --- Import Framework ---
          | # Assuming mcp_server_framework.py is in llmbasedos/ or a discoverable path
          | # from llmbasedos.mcp_server_framework import MCPServer
          | # For now, let's assume it's in a place Python can find it.
          | # If it's meant to be part of llmbasedos, the import would be:
          | from llmbasedos.mcp_server_framework import MCPServer 
          | from llmbasedos.common_utils import validate_mcp_path_param # Assurez-vous que fs_server en a besoin
          | 
          | # --- Server Specific Configuration ---
          | SERVER_NAME = "agent"
          | # CAPS_FILE_PATH_STR will be derived by MCPServer if caps.json is in the same dir as this file
          | # If not, it needs to be explicitly passed. For consistency with your previous files:
          | CAPS_FILE_PATH_STR = str(Path(__file__).parent / "caps.json")
          | 
          | # Custom error code base for agent-specific errors (e.g., workflow not found)
          | AGENT_CUSTOM_ERROR_BASE = -32040 # Will be combined with MCPServer's internal error codes
          | 
          | # Configuration paths (can be accessed via server.config later if MCPServer handles config loading)
          | WORKFLOWS_DIR_CONF = Path(os.getenv("LLMBDO_AGENT_WORKFLOWS_DIR", "/etc/llmbasedos/workflows"))
          | WORKFLOWS_DIR_CONF.mkdir(parents=True, exist_ok=True) # Ensure dir exists
          | 
          | AGENT_EXEC_LOG_DIR_CONF = Path(os.getenv("LLMBDO_AGENT_EXEC_LOG_DIR", f"/var/log/llmbasedos/{SERVER_NAME}_executions"))
          | AGENT_EXEC_LOG_DIR_CONF.mkdir(parents=True, exist_ok=True)
          | 
          | N8N_LITE_URL_CONF = os.getenv("LLMBDO_N8N_LITE_URL", "http://localhost:5678")
          | 
          | # Initialize server instance using the framework
          | # The MCPServer class will handle socket creation, logging setup, etc.
          | agent_server = MCPServer(
          |     server_name=SERVER_NAME,
          |     caps_file_path_str=CAPS_FILE_PATH_STR, # MCPServer might auto-detect if named 'caps.json' in module dir
          |     custom_error_code_base=AGENT_CUSTOM_ERROR_BASE
          | )
          | 
          | # Attach server-specific state to the server instance for access within handlers
          | # These will be initialized in the on_startup_hook
          | # agent_server.workflow_definitions: Dict[str, Dict[str, Any]] = {}
          | # agent_server.executions_state: Dict[str, Dict[str, Any]] = {}
          | # agent_server.docker_client: Optional[docker.DockerClient] = None
          | 
          | 
          | # --- Workflow Definition Loading (Sync, to be run in executor or startup thread) ---
          | def _load_workflow_definitions_blocking(server_instance: MCPServer):
          |     """
          |     Loads workflow definitions from YAML files into server_instance.workflow_definitions.
          |     This is a blocking function, intended to be run via server.run_in_executor.
          |     """
          |     # Access attributes via server_instance
          |     server_instance.workflow_definitions.clear() # type: ignore
          |     if not WORKFLOWS_DIR_CONF.exists() or not WORKFLOWS_DIR_CONF.is_dir():
          |         server_instance.logger.warning(f"Workflows directory {WORKFLOWS_DIR_CONF} not found. No workflows loaded.")
          |         return
          | 
          |     for filepath in WORKFLOWS_DIR_CONF.glob("*.yaml"): # Or .yml
          |         try:
          |             with filepath.open('r') as f:
          |                 workflow_yaml = yaml.safe_load(f)
          |             
          |             if not isinstance(workflow_yaml, dict): # Basic validation
          |                 server_instance.logger.warning(f"Workflow file {filepath.name} does not contain a valid YAML dictionary. Skipping.")
          |                 continue
          | 
          |             wf_id = str(workflow_yaml.get("id", filepath.stem)) # Use filename stem if ID not in YAML, ensure string
          |             wf_name = str(workflow_yaml.get("name", wf_id))    # Ensure string
          |             
          |             # Store minimal info, full YAML can be parsed on execution if complex
          |             server_instance.workflow_definitions[wf_id] = { # type: ignore
          |                 "workflow_id": wf_id,
          |                 "name": wf_name,
          |                 "description": workflow_yaml.get("description"),
          |                 "path": str(filepath),
          |                 "parsed_yaml": workflow_yaml, # Store full parsed content
          |                 "input_schema": workflow_yaml.get("input_schema") # For mcp.agent.listWorkflows
          |             }
          |             server_instance.logger.info(f"Loaded workflow definition: '{wf_name}' (ID: {wf_id}) from {filepath.name}")
          |         except yaml.YAMLError as ye:
          |             server_instance.logger.error(f"Error parsing YAML for workflow {filepath.name}: {ye}")
          |         except Exception as e:
          |             server_instance.logger.error(f"Error loading workflow {filepath.name}: {e}", exc_info=True)
          |     server_instance.logger.info(f"Loaded {len(server_instance.workflow_definitions)} workflow definitions.") # type: ignore
          | 
          | 
          | # --- Workflow Execution Target (Runs in a separate Python thread) ---
          | # This function contains blocking calls (Docker, HTTP requests, time.sleep).
          | # It's designed to run in a thread spawned by `handle_agent_run_workflow`.
          | def _execute_workflow_in_thread(
          |     server_instance: MCPServer, # Pass the MCPServer instance for logging and config access
          |     execution_id: str,
          |     workflow_id: str,
          |     inputs: Optional[Dict[str, Any]]
          | ):
          |     # Use server_instance.logger for logging
          |     # Access server_instance.executions_state, server_instance.docker_client, etc.
          | 
          |     exec_log_file = AGENT_EXEC_LOG_DIR_CONF / f"{execution_id}.log"
          |     
          |     def _log_exec(message: str, level: str = "info"):
          |         timestamp = datetime.now(timezone.utc).isoformat()
          |         log_line = f"{timestamp} [{level.upper()}] {message}\n"
          |         try:
          |             with open(exec_log_file, 'a', encoding='utf-8') as lf:
          |                 lf.write(log_line)
          |         except Exception as e_log_write:
          |             # Log to server logger if file write fails
          |             server_instance.logger.error(f"Exec {execution_id}: Failed to write to log file {exec_log_file}: {e_log_write}")
          |         
          |         # Also log to server's main logger
          |         # Python's default logging handlers are thread-safe.
          |         getattr(server_instance.logger, level, server_instance.logger.info)(f"Exec {execution_id} (WF {workflow_id}): {message}")
          | 
          |     _log_exec(f"Execution thread started. Inputs: {inputs}")
          |     
          |     # Access execution state via server_instance
          |     # Ensure execution_id exists (should be created before thread starts)
          |     if execution_id not in server_instance.executions_state: # type: ignore
          |         _log_exec(f"Critical error: Execution ID {execution_id} not found in state at thread start.", "error")
          |         return
          |         
          |     exec_data = server_instance.executions_state[execution_id] # type: ignore
          |     exec_data["status"] = "running"
          |     exec_data["start_time"] = datetime.now(timezone.utc)
          |     
          |     docker_container_obj: Optional[docker.models.containers.Container] = None # type: ignore
          |     # cancellation_event = exec_data.get("cancellation_event") # Assuming this is a threading.Event
          | 
          |     try:
          |         if workflow_id not in server_instance.workflow_definitions: # type: ignore
          |             raise ValueError(f"Workflow definition ID '{workflow_id}' not found at execution time.")
          |         
          |         wf_config = server_instance.workflow_definitions[workflow_id]["parsed_yaml"] # type: ignore
          |         wf_type = wf_config.get("type", "simple_sequential")
          | 
          |         if wf_type == "docker":
          |             if not server_instance.docker_client: # type: ignore
          |                 raise RuntimeError("Docker client not available for Docker workflow.")
          |             
          |             docker_image = wf_config.get("docker_image")
          |             if not docker_image:
          |                 raise ValueError("Docker image not specified in workflow config.")
          |             
          |             command_list = wf_config.get("command") # List or string
          |             environment_dict = {str(k).upper(): str(v) for k,v in (inputs or {}).items()}
          |             environment_dict.update(wf_config.get("environment", {}))
          | 
          |             _log_exec(f"Starting Docker container: Image='{docker_image}', Cmd='{command_list}', EnvKeys='{list(environment_dict.keys())}'")
          |             
          |             container = server_instance.docker_client.containers.run( # type: ignore
          |                 image=docker_image, command=command_list, environment=environment_dict,
          |                 detach=True, remove=False # Keep for logs, remove in finally block
          |             )
          |             docker_container_obj = container # Store for potential cleanup
          |             exec_data["docker_container_id"] = container.id
          |             _log_exec(f"Docker container {container.id} started.")
          | 
          |             for log_entry in container.logs(stream=True, follow=True, timestamps=True, stdout=True, stderr=True):
          |                 # if cancellation_event and cancellation_event.is_set():
          |                 #     _log_exec("Docker workflow cancellation requested during log streaming.", "warning")
          |                 #     container.stop(timeout=5)
          |                 #     break
          |                 _log_exec(f"DOCKER: {log_entry.decode('utf-8').strip()}")
          |             
          |             container.reload()
          |             container_state = container.attrs['State']
          |             exit_code = container_state.get('ExitCode', -1)
          |             
          |             if exit_code == 0:
          |                 exec_data["status"] = "completed"
          |                 exec_data["output"] = {"message": "Docker task completed successfully.", "exit_code": 0, "container_id": container.id}
          |                 _log_exec(f"Docker task ({container.id}) completed successfully (ExitCode: {exit_code}).")
          |             else:
          |                 error_details = container_state.get('Error', f"Docker task non-zero ExitCode: {exit_code}")
          |                 raise RuntimeError(f"Docker task ({container.id}) failed. {error_details}")
          | 
          |         elif wf_type == "n8n_webhook":
          |             webhook_url_tmpl = wf_config.get("webhook_url")
          |             if not webhook_url_tmpl:
          |                 webhook_url_tmpl = f"{N8N_LITE_URL_CONF.rstrip('/')}/webhook/{workflow_id}"
          |             final_webhook_url = webhook_url_tmpl # Add templating here if needed
          | 
          |             _log_exec(f"Calling HTTP webhook (n8n-lite type): {final_webhook_url}")
          |             http_timeout = wf_config.get("timeout_seconds", 300)
          |             response = requests.post(final_webhook_url, json=inputs, timeout=http_timeout)
          |             response.raise_for_status()
          |             
          |             exec_data["status"] = "completed"
          |             try:
          |                 exec_data["output"] = response.json()
          |             except requests.exceptions.JSONDecodeError:
          |                 exec_data["output"] = {"raw_response": response.text}
          |             _log_exec(f"HTTP webhook call successful. Status: {response.status_code}. Output (preview): {str(exec_data.get('output',''))[:100]}")
          | 
          |         elif wf_type == "simple_sequential":
          |             _log_exec("Executing simple sequential Python steps...")
          |             current_output = inputs or {}
          |             for i, step_config in enumerate(wf_config.get("steps", [])):
          |                 # if cancellation_event and cancellation_event.is_set():
          |                 #     _log_exec("Sequential workflow cancellation requested.", "warning"); break
          |                 step_name = step_config.get("name", f"Step_{i+1}")
          |                 step_action = step_config.get("action", "log_message")
          |                 _log_exec(f"Running step '{step_name}': Action='{step_action}'")
          |                 
          |                 if step_action == "log_message":
          |                     msg = step_config.get("message", "Default log from workflow.")
          |                     _log_exec(f"STEP LOG ({step_name}): {msg}")
          |                 elif step_action == "sleep":
          |                     duration = float(step_config.get("duration_seconds", 1.0))
          |                     time.sleep(duration)
          |                 else:
          |                     _log_exec(f"Unknown action '{step_action}' in step '{step_name}'. Skipping.", "warning")
          |             
          |             exec_data["status"] = "completed"
          |             exec_data["output"] = current_output
          |             _log_exec("Simple sequential workflow completed.")
          |         else:
          |             raise NotImplementedError(f"Workflow type '{wf_type}' execution logic not implemented.")
          | 
          |     except Exception as e_wf:
          |         _log_exec(f"Workflow execution critically failed: {e_wf}", level="error")
          |         # Ensure exec_data is still valid and accessible
          |         if execution_id in server_instance.executions_state: # type: ignore
          |             exec_data["status"] = "failed"
          |             exec_data["error_message"] = str(e_wf)
          |     finally:
          |         if execution_id in server_instance.executions_state: # type: ignore
          |             exec_data["end_time"] = datetime.now(timezone.utc)
          |             exec_data["thread_obj"] = None # Mark Python thread as logically done
          |         
          |         if docker_container_obj:
          |             try:
          |                 docker_container_obj.reload()
          |                 # Check config for auto-removal, default to True
          |                 auto_remove = wf_config.get("auto_remove_container", True) if 'wf_config' in locals() else True
          |                 if auto_remove and docker_container_obj.status in ['exited', 'dead', 'created']:
          |                     _log_exec(f"Removing Docker container {docker_container_obj.id}")
          |                     docker_container_obj.remove(v=True) # v=True also removes anonymous volumes
          |                 else:
          |                     _log_exec(f"Docker container {docker_container_obj.id} (Status: {docker_container_obj.status}) not auto-removed.", "debug")
          |             except Exception as e_docker_clean:
          |                 _log_exec(f"Error during Docker container cleanup for {docker_container_obj.id}: {e_docker_clean}", level="error")
          |         _log_exec(f"Execution thread finished.")
          | 
          | 
          | # --- Agent Capability Handlers (decorated for MCPServer framework) ---
          | @agent_server.register_method("mcp.agent.listWorkflows")
          | async def handle_agent_list_workflows(server: MCPServer, request_id: Optional[Union[str, int]], params: List[Any]):
          |     # Ensure workflow definitions are loaded if not already
          |     if not server.workflow_definitions: # type: ignore
          |         # _load_workflow_definitions_blocking is sync, run in executor
          |         await server.run_in_executor(_load_workflow_definitions_blocking, server)
          |     
          |     return [
          |         {
          |             "workflow_id": wf["workflow_id"],
          |             "name": wf["name"],
          |             "description": wf.get("description"),
          |             "input_schema": wf.get("input_schema") # From caps.json example
          |         }
          |         for wf_id, wf in server.workflow_definitions.items() # type: ignore
          |     ]
          | 
          | @agent_server.register_method("mcp.agent.runWorkflow")
          | async def handle_agent_run_workflow(server: MCPServer, request_id: Optional[Union[str, int]], params: List[Any]):
          |     # Schema validation would be handled by MCPServer based on caps.json if framework supports it.
          |     # Assuming params = [workflow_id_str, optional_inputs_dict]
          |     if not params or not isinstance(params[0], str):
          |         # This error should ideally be caught by framework's param validation based on caps.json
          |         raise server.create_custom_error(request_id, -1, "Invalid params: workflow_id (string) is required.")
          |         
          |     workflow_id = params[0]
          |     inputs = params[1] if len(params) > 1 and isinstance(params[1], dict) else {}
          | 
          |     if workflow_id not in server.workflow_definitions: # type: ignore
          |         raise server.create_custom_error(request_id, -2, f"Workflow ID '{workflow_id}' not found.")
          | 
          |     execution_id = f"exec_{uuid.uuid4().hex[:12]}"
          |     # cancellation_event = threading.Event() # For cooperative cancellation within the thread
          | 
          |     # Initialize execution state before starting the thread
          |     server.executions_state[execution_id] = { # type: ignore
          |         "execution_id": execution_id,
          |         "workflow_id": workflow_id,
          |         "status": "pending", # Will be updated by the thread
          |         "inputs": inputs,
          |         "output": None,
          |         "error_message": None,
          |         "log_file": str(AGENT_EXEC_LOG_DIR_CONF / f"{execution_id}.log"),
          |         # "cancellation_event": cancellation_event
          |     }
          | 
          |     # Workflow execution (_execute_workflow_in_thread) is blocking, so run it in a new Python thread.
          |     # This is different from server.run_in_executor which uses a shared thread pool.
          |     # For long-running, independent tasks, a new thread is often more appropriate.
          |     thread = threading.Thread(
          |         target=_execute_workflow_in_thread,
          |         args=(server, execution_id, workflow_id, inputs),
          |         daemon=True # Allows main program to exit even if threads are active (though join on shutdown is better)
          |     )
          |     server.executions_state[execution_id]["thread_obj"] = thread # type: ignore
          |     thread.start()
          |     
          |     return {"execution_id": execution_id, "workflow_id": workflow_id, "status": "started"}
          | 
          | 
          | @agent_server.register_method("mcp.agent.getWorkflowStatus")
          | async def handle_agent_get_workflow_status(server: MCPServer, request_id: Optional[Union[str, int]], params: List[Any]):
          |     if not params or not isinstance(params[0], str):
          |         raise server.create_custom_error(request_id, -1, "Invalid params: execution_id (string) is required.")
          |     execution_id = params[0]
          | 
          |     if execution_id not in server.executions_state: # type: ignore
          |         raise server.create_custom_error(request_id, -3, f"Execution ID '{execution_id}' not found.")
          |     
          |     exec_data = server.executions_state[execution_id] # type: ignore
          |     
          |     log_preview_list = []
          |     log_file_path_str = exec_data.get("log_file")
          |     if log_file_path_str:
          |         log_file_path = Path(log_file_path_str)
          |         if log_file_path.exists():
          |             try:
          |                 # Reading file is IO blocking, could use executor for very large previews or many calls
          |                 with open(log_file_path, 'r', encoding='utf-8', errors='ignore') as lf_read:
          |                     log_preview_list = [line.strip() for line in lf_read.readlines()[-20:]] # Last 20 lines
          |             except Exception as e_logread:
          |                 server.logger.warning(f"Could not read log preview for execution {execution_id}: {e_logread}")
          | 
          |     return {
          |         "execution_id": execution_id,
          |         "workflow_id": exec_data["workflow_id"],
          |         "status": exec_data["status"],
          |         "start_time": exec_data.get("start_time").isoformat() if exec_data.get("start_time") else None,
          |         "end_time": exec_data.get("end_time").isoformat() if exec_data.get("end_time") else None,
          |         "output": exec_data.get("output"),
          |         "error_message": exec_data.get("error_message"),
          |         "log_preview": log_preview_list
          |     }
          | 
          | @agent_server.register_method("mcp.agent.stopWorkflow")
          | async def handle_agent_stop_workflow(server: MCPServer, request_id: Optional[Union[str, int]], params: List[Any]):
          |     if not params or not isinstance(params[0], str):
          |         raise server.create_custom_error(request_id, -1, "Invalid params: execution_id (string) is required.")
          |     execution_id = params[0]
          | 
          |     if execution_id not in server.executions_state: # type: ignore
          |         raise server.create_custom_error(request_id, -3, f"Execution ID '{execution_id}' not found.")
          |     
          |     exec_data = server.executions_state[execution_id] # type: ignore
          |     current_status = exec_data.get("status")
          | 
          |     if current_status not in ["pending", "running"]:
          |         return {
          |             "execution_id": execution_id,
          |             "status": "already_completed_or_not_running",
          |             "message": f"Workflow execution {execution_id} is not in a stoppable state (current: {current_status})."
          |         }
          | 
          |     docker_container_id_to_stop = exec_data.get("docker_container_id")
          |     # cancellation_event_to_set = exec_data.get("cancellation_event") # For cooperative thread cancellation
          | 
          |     if docker_container_id_to_stop and server.docker_client: # type: ignore
          |         # Stopping Docker container is a blocking IO operation
          |         def stop_docker_container_blocking_sync():
          |             try:
          |                 container_to_stop = server.docker_client.containers.get(docker_container_id_to_stop) # type: ignore
          |                 server.logger.info(f"Exec {execution_id}: Sending stop signal to Docker container {docker_container_id_to_stop}")
          |                 container_to_stop.stop(timeout=10) # 10s graceful timeout
          |                 exec_data["status"] = "cancelling" # Workflow thread should update to "cancelled" or "failed"
          |                 return {"execution_id": execution_id, "status": "stop_requested", "message": "Stop signal sent to Docker container."}
          |             except docker.errors.NotFound: # type: ignore
          |                 # Container already gone
          |                 exec_data["status"] = "unknown_after_stop_notfound" # Or some other terminal state
          |                 return {"execution_id": execution_id, "status": "not_running", "message": "Docker container not found (already gone)."}
          |             except Exception as e_docker_stop:
          |                 server.logger.error(f"Exec {execution_id}: Error stopping Docker container {docker_container_id_to_stop}: {e_docker_stop}")
          |                 # Re-raise as a custom server error for the framework to handle
          |                 raise server.create_custom_error(request_id, -4, f"Failed to stop Docker container: {e_docker_stop}")
          |         
          |         return await server.run_in_executor(stop_docker_container_blocking_sync)
          |     
          |     # elif cancellation_event_to_set:
          |     #     server.logger.info(f"Exec {execution_id}: Setting cancellation event for Python thread.")
          |     #     cancellation_event_to_set.set()
          |     #     exec_data["status"] = "cancelling" # Thread should notice this and exit
          |     #     return {"execution_id": execution_id, "status": "stop_requested", "message": "Cancellation requested for workflow thread."}
          |     else:
          |         # If no specific stop mechanism is identified (e.g. simple sequential thread without event)
          |         return {
          |             "execution_id": execution_id,
          |             "status": "not_stoppable",
          |             "message": "Workflow type does not currently support external stopping or is not a stoppable resource."
          |         }
          | 
          | 
          | # --- Server Lifecycle Hooks (to be registered with MCPServer instance) ---
          | async def on_agent_server_startup(server: MCPServer):
          |     """Custom startup actions for the agent server."""
          |     server.logger.info(f"Agent Server '{server.server_name}' performing custom startup actions...")
          |     
          |     # Initialize server-specific state attributes
          |     server.workflow_definitions = {} # type: ignore
          |     server.executions_state = {}    # type: ignore
          |     server.docker_client = None     # type: ignore
          | 
          |     # Initialize Docker client (can be blocking, so use executor)
          |     try:
          |         def init_docker_client_sync(): # Blocking function
          |             return docker.from_env() # type: ignore
          |         
          |         server.docker_client = await server.run_in_executor(init_docker_client_sync) # type: ignore
          |         server.logger.info("Docker client initialized successfully via executor.")
          |     except Exception as docker_err_init:
          |         server.logger.warning(f"Failed to initialize Docker client: {docker_err_init}. Docker-based agents will be unavailable.", exc_info=True)
          |         # server.docker_client remains None
          |     
          |     # Load workflow definitions (blocking IO, use executor)
          |     await server.run_in_executor(_load_workflow_definitions_blocking, server)
          | 
          | async def on_agent_server_shutdown(server: MCPServer):
          |     """Custom shutdown actions for the agent server."""
          |     server.logger.info(f"Agent Server '{server.server_name}' performing custom shutdown actions...")
          |     
          |     # Attempt to gracefully stop/clean up any active workflow executions
          |     for exec_id, exec_data in list(server.executions_state.items()): # type: ignore # Iterate copy
          |         if exec_data.get("status") == "running":
          |             server.logger.info(f"Exec {exec_id}: Attempting cleanup/stop on server shutdown...")
          |             
          |             # Signal Python threads via cancellation event (if implemented)
          |             # cancellation_event = exec_data.get("cancellation_event")
          |             # if cancellation_event:
          |             #     server.logger.info(f"Exec {exec_id}: Setting cancellation event on shutdown.")
          |             #     cancellation_event.set()
          | 
          |             container_id_shutdown = exec_data.get("docker_container_id")
          |             if container_id_shutdown and server.docker_client: # type: ignore
          |                 def stop_docker_on_shutdown_sync_final():
          |                     try:
          |                         cont = server.docker_client.containers.get(container_id_shutdown) # type: ignore
          |                         server.logger.info(f"Exec {exec_id}: Requesting stop for Docker container {container_id_shutdown} on server shutdown.")
          |                         cont.stop(timeout=5) # Short timeout during shutdown
          |                         # Auto-remove logic from wf_config (might be complex to get here)
          |                         # For simplicity, just try to remove if it was not meant to be persistent.
          |                         # This part is tricky as wf_config might not be easily accessible here.
          |                         # Assume default auto-remove if container is stopped.
          |                         cont.reload()
          |                         if cont.status in ['exited', 'dead']:
          |                             server.logger.info(f"Exec {exec_id}: Removing container {container_id_shutdown} after stop.")
          |                             cont.remove(v=True)
          |                     except docker.errors.NotFound: # type: ignore
          |                         server.logger.info(f"Exec {exec_id}: Container {container_id_shutdown} already gone during shutdown stop.")
          |                     except Exception as e_stop_final:
          |                         server.logger.warning(f"Exec {exec_id}: Error during final stop/remove of Docker container {container_id_shutdown}: {e_stop_final}")
          |                 
          |                 # Run this blocking Docker stop in executor to not stall shutdown too much
          |                 try:
          |                     await server.run_in_executor(stop_docker_on_shutdown_sync_final)
          |                 except Exception as e_exec_shutdown:
          |                     server.logger.error(f"Error in executor for stopping container {container_id_shutdown} on shutdown: {e_exec_shutdown}")
          |             
          |             # Wait for Python threads if they were designed for clean exit
          |             # thread_obj_shutdown = exec_data.get("thread_obj")
          |             # if thread_obj_shutdown and thread_obj_shutdown.is_alive():
          |             #     server.logger.info(f"Exec {exec_id}: Waiting briefly for workflow thread to join on shutdown.")
          |             #     thread_obj_shutdown.join(timeout=5) # Brief wait for thread to finish
          |             #     if thread_obj_shutdown.is_alive():
          |             #         server.logger.warning(f"Exec {exec_id}: Workflow thread did not exit cleanly on shutdown.")
          | 
          | # Register lifecycle hooks with the server instance
          | agent_server.set_startup_hook(on_agent_server_startup)
          | agent_server.set_shutdown_hook(on_agent_server_shutdown)
          | 
          | # --- Main Entry Point (if running this server module directly) ---
          | if __name__ == "__main__":
          |     # The MCPServer class should handle its own logger setup for the main script execution if desired,
          |     # or we can add a specific logger here for the __main__ block.
          |     # For now, rely on MCPServer's internal logger or default Python logging for this block.
          |     # Example:
          |     # main_script_logger = logging.getLogger("llmbasedos.servers.agent_script_main")
          |     # main_script_logger.setLevel(os.getenv(f"LLMBDO_{SERVER_NAME.upper()}_LOG_LEVEL", "INFO").upper())
          |     # if not main_script_logger.hasHandlers():
          |     #    # Add basic handler for this script's direct execution logs
          |     #    # (MCPServer will have its own more structured logger for server operations)
          |     #    main_ch = logging.StreamHandler()
          |     #    main_ch.setFormatter(logging.Formatter(f"%(asctime)s - AGENT_SCRIPT_MAIN - %(levelname)s - %(message)s"))
          |     #    main_script_logger.addHandler(main_ch)
          | 
          |     # The MCPServer's start() method will run the asyncio event loop.
          |     try:
          |         asyncio.run(agent_server.start())
          |     except KeyboardInterrupt:
          |         # MCPServer.start() should handle KeyboardInterrupt gracefully for its cleanup.
          |         # Add a print here if specific message needed for direct run.
          |         print(f"\nAgent Server '{SERVER_NAME}' (direct run) stopped by KeyboardInterrupt.")
          |     except Exception as e_main_script:
          |         print(f"Agent Server '{SERVER_NAME}' (direct run) crashed: {e_main_script}", file=sys.stderr)
          |         # Optionally log to a file or more structured output if needed
          |         # main_script_logger.critical(f"Agent Server (main) crashed: {e_main_script}", exc_info=True)
          |     finally:
          |         # MCPServer.start() should also ensure its shutdown hook and executor cleanup occurs.
          |         print(f"Agent Server '{SERVER_NAME}' (direct run) fully shut down.")
          --- Fin Contenu ---

      Répertoire: ./llmbasedos_src/servers/fs
        Fichier: caps.json
          --- Début Contenu (ascii) ---
          | {
          |     "service_name": "fs",
          |     "description": "Provides capabilities for file system operations, including listing, reading, writing, deleting, embedding, and searching files.",
          |     "version": "0.1.1",
          |     "capabilities": [
          |         {
          |             "method": "mcp.fs.list",
          |             "description": "Lists files and directories in a given path. Path must be absolute or will be resolved against server's virtual root if not provided by a path-aware client (like the shell).",
          |             "params_schema": {
          |                 "type": "array",
          |                 "items": {"type": "string", "description": "The path to list. Should be absolute within the allowed virtual root."},
          |                 "minItems": 1,
          |                 "maxItems": 1
          |             },
          |             "result_schema": {
          |                 "type": "array",
          |                 "items": {
          |                     "type": "object",
          |                     "properties": {
          |                         "name": {"type": "string"},
          |                         "path": {"type": "string", "description": "Client-facing path, relative to virtual root, starting with /."},
          |                         "type": {"type": "string", "enum": ["file", "directory", "symlink", "other", "inaccessible"]},
          |                         "size": {"type": "integer", "description": "Size in bytes, -1 for directories or if not applicable."},
          |                         "modified_at": {"type": ["string", "null"], "format": "date-time", "description": "Last modification timestamp in ISO format, or null."}
          |                     },
          |                     "required": ["name", "path", "type", "size", "modified_at"]
          |                 }
          |             }
          |         },
          |         {
          |             "method": "mcp.fs.read",
          |             "description": "Reads file content. Path must be absolute. Returns content as base64 for binary, or text for text files.",
          |             "params_schema": {
          |                 "type": "array",
          |                 "items": [
          |                     {"type": "string", "description": "The absolute path to the file to read."},
          |                     {"type": "string", "enum": ["text", "base64"], "default": "text", "description": "Encoding. 'text' attempts UTF-8, 'base64' for binary."}
          |                 ],
          |                 "minItems": 1,
          |                 "maxItems": 2
          |             },
          |             "result_schema": {
          |                 "type": "object",
          |                 "properties": {
          |                     "path": {"type": "string"}, "content": {"type": "string"},
          |                     "encoding": {"type": "string", "enum": ["text", "base64"]},
          |                     "mime_type": {"type": "string"}
          |                 },
          |                 "required": ["path", "content", "encoding", "mime_type"]
          |             }
          |         },
          |         {
          |             "method": "mcp.fs.write",
          |             "description": "Writes content to a file. Path must be absolute. Content can be text or base64.",
          |             "params_schema": {
          |                 "type": "array",
          |                 "items": [
          |                     {"type": "string", "description": "Absolute path to the file to write."},
          |                     {"type": "string", "description": "Content to write."},
          |                     {"type": "string", "enum": ["text", "base64"], "default": "text"},
          |                     {"type": "boolean", "default": false, "description": "Append if true, overwrite otherwise."}
          |                 ],
          |                 "minItems": 2, "maxItems": 4
          |             },
          |             "result_schema": {
          |                 "type": "object", "properties": {"path": {"type": "string"}, "bytes_written": {"type": "integer"}, "status": {"type": "string"}},
          |                 "required": ["path", "bytes_written", "status"]
          |             }
          |         },
          |         {
          |             "method": "mcp.fs.delete",
          |             "description": "Deletes a file or an empty directory. Path must be absolute. Use 'recursive' for non-empty dirs.",
          |             "params_schema": {
          |                 "type": "array",
          |                 "items": [
          |                     {"type": "string", "description": "The absolute path to delete."},
          |                     {"type": "boolean", "default": false, "description": "Recursively delete if non-empty directory."}
          |                 ],
          |                 "minItems": 1, "maxItems": 2
          |             },
          |             "result_schema": {
          |                 "type": "object", "properties": {"path": {"type": "string"}, "status": {"type": "string"}},
          |                 "required": ["path", "status"]
          |             }
          |         },
          |         {
          |             "method": "mcp.fs.embed",
          |             "description": "Generates/stores embeddings for a file or directory contents. Path must be absolute.",
          |             "params_schema": {
          |                 "type": "array",
          |                 "items": [
          |                     {"type": "string", "description": "Absolute path to file/directory to embed."},
          |                     {"type": "boolean", "default": false, "description": "Recursively embed files in subdirectories."}
          |                 ],
          |                 "minItems": 1, "maxItems": 2
          |             },
          |             "result_schema": {
          |                 "type": "object", "properties": {"path_processed": {"type": "string"}, "files_embedded_this_run": {"type": "integer"}, "total_embeddings_in_index": {"type": "integer"}, "status": {"type": "string"}},
          |                 "required": ["path_processed", "files_embedded_this_run", "total_embeddings_in_index", "status"]
          |             }
          |         },
          |         {
          |             "method": "mcp.fs.search",
          |             "description": "Searches for files based on semantic similarity using stored embeddings.",
          |             "params_schema": {
          |                 "type": "array",
          |                 "items": [
          |                     {"type": "string", "description": "Query text for semantic search."},
          |                     {"type": "integer", "default": 5, "description": "Number of top results."},
          |                     {"type": "string", "description": "Optional absolute path to restrict search scope. Use null or omit if no scope."}
          |                 ],
          |                 "minItems": 1, 
          |                 "maxItems": 3 
          |             },
          |             "result_schema": {
          |                 "type": "array", "items": {"type": "object", "properties": {"path": {"type": "string"}, "score": {"type": "number"}, "preview": {"type": "string", "optional": true}}, "required": ["path", "score"]}
          |             }
          |         }
          |     ]
          | }
          --- Fin Contenu ---
        Fichier: requirements.txt
          --- Début Contenu (ascii) ---
          | # llmbasedos/servers/fs/requirements.txt
          | # watchdog is for gateway's dynamic discovery, not directly used by fs server unless it self-monitors
          | sentence-transformers>=2.2.0
          | faiss-cpu>=1.7.0 # or faiss-gpu
          | python-magic>=0.4.27
          | numpy>=1.20 # Often a dependency of sentence-transformers/faiss
          --- Fin Contenu ---
        Fichier: server.py
          --- Début Contenu (utf-8) ---
          | # llmbasedos_pkg/servers/fs/server.py
          | import asyncio
          | # logging sera géré par MCPServer, pas besoin d'importer directement ici.
          | import os
          | import shutil
          | from pathlib import Path
          | from datetime import datetime, timezone
          | import base64
          | import magic # For MIME types
          | from typing import Any, Dict, List, Optional, Tuple, Union
          | import json # For FAISS metadata
          | 
          | # Imports du projet
          | from llmbasedos.mcp_server_framework import MCPServer 
          | from llmbasedos.common_utils import validate_mcp_path_param, DEFAULT_VIRTUAL_ROOT_STR as COMMON_DEFAULT_VIRTUAL_ROOT_STR
          | 
          | # Embedding and Search related imports
          | try:
          |     from sentence_transformers import SentenceTransformer
          |     import faiss
          |     import numpy as np
          |     EMBEDDING_SYSTEM_AVAILABLE = True
          | except ImportError:
          |     EMBEDDING_SYSTEM_AVAILABLE = False
          |     SentenceTransformer = type(None) # type: ignore
          |     faiss = type(None) # type: ignore
          |     np = type(None) # type: ignore
          | 
          | # --- Server Specific Configuration ---
          | SERVER_NAME = "fs"
          | CAPS_FILE_PATH_STR = str(Path(__file__).parent / "caps.json")
          | FS_CUSTOM_ERROR_BASE = -32010 # Base for FS specific errors
          | 
          | # Embedding config: Read from ENV, provide defaults.
          | EMBEDDING_MODEL_NAME_CONF = os.getenv("LLMBDO_FS_EMBEDDING_MODEL", 'all-MiniLM-L6-v2' if EMBEDDING_SYSTEM_AVAILABLE else "disabled")
          | _faiss_dir_default_str = "/var/lib/llmbasedos/faiss_indices_fs" # Nom plus spécifique pour FS
          | FAISS_INDEX_DIR_STR = os.getenv("LLMBDO_FS_FAISS_DIR", _faiss_dir_default_str)
          | FAISS_INDEX_DIR_PATH_CONF = Path(FAISS_INDEX_DIR_STR).resolve()
          | 
          | FAISS_INDEX_FILE_PATH_CONF = FAISS_INDEX_DIR_PATH_CONF / "fs_index.faiss"
          | FAISS_METADATA_FILE_PATH_CONF = FAISS_INDEX_DIR_PATH_CONF / "fs_metadata.json"
          | 
          | # Virtual root for this FS server.
          | # Utilise LLMBDO_FS_VIRTUAL_ROOT si défini, sinon le DEFAULT_VIRTUAL_ROOT_STR de common_utils.
          | # LLMBDO_FS_DATA_ROOT est utilisé dans le Dockerfile/Compose pour le point de montage.
          | # Idéalement, LLMBDO_FS_VIRTUAL_ROOT devrait correspondre à LLMBDO_FS_DATA_ROOT.
          | FS_VIRTUAL_ROOT_STR = os.getenv(f"LLMBDO_{SERVER_NAME.upper()}_VIRTUAL_ROOT", os.getenv("LLMBDO_FS_DATA_ROOT", COMMON_DEFAULT_VIRTUAL_ROOT_STR))
          | 
          | # Initialize server instance
          | fs_server = MCPServer(SERVER_NAME, CAPS_FILE_PATH_STR, custom_error_code_base=FS_CUSTOM_ERROR_BASE)
          | 
          | # Attach embedding-specific state to the server instance (will be initialized in on_startup)
          | fs_server.embedding_enabled: bool = False # type: ignore
          | fs_server.embedding_model: Optional[SentenceTransformer] = None # type: ignore
          | fs_server.faiss_index: Optional[faiss.Index] = None # type: ignore
          | fs_server.faiss_index_metadata: List[Dict[str, Any]] = [] # type: ignore
          | fs_server.faiss_next_id: int = 0 # type: ignore
          | 
          | 
          | # --- Path Validation Helper for FS Server (using common_utils) ---
          | # Dans llmbasedos_pkg/servers/fs/server.py
          | # Dans llmbasedos_pkg/servers/fs/server.py
          | 
          | # Dans llmbasedos_pkg/servers/fs/server.py
          | 
          | # FS_VIRTUAL_ROOT_STR est défini au niveau du module, ex: "/mnt/user_data"
          | # (il est lu depuis os.getenv(..., os.getenv(..., COMMON_DEFAULT_VIRTUAL_ROOT_STR)))
          | 
          | # Dans llmbasedos_pkg/servers/fs/server.py
          | 
          | # FS_VIRTUAL_ROOT_STR est défini au niveau du module, ex: "/mnt/user_data"
          | # COMMON_DEFAULT_VIRTUAL_ROOT_STR est importé de common_utils
          | 
          | def _validate_fs_path(
          |         path_from_client: str, 
          |         check_exists: bool = False,
          |         must_be_dir: Optional[bool] = None,
          |         must_be_file: Optional[bool] = None
          |     ) -> Path: 
          |     
          |     fs_server.logger.debug(f"_validate_fs_path: Validating client_path='{path_from_client}', against FS_VIRTUAL_ROOT_STR='{FS_VIRTUAL_ROOT_STR}'")
          |     
          |     if not isinstance(path_from_client, str):
          |         raise ValueError(f"Path parameter must be a string, got {type(path_from_client)}")
          | 
          |     # Normaliser le chemin client pour qu'il soit relatif à la racine virtuelle du serveur FS
          |     # Le client peut envoyer :
          |     #  - "/" (signifiant la racine virtuelle du FS)
          |     #  - "/foo/bar.txt" (signifiant FS_VIRTUAL_ROOT/foo/bar.txt)
          |     #  - "foo/bar.txt" (si le client a une notion de CWD et l'a déjà résolu pour que ce soit relatif à la racine virtuelle)
          | 
          |     # Le `FS_VIRTUAL_ROOT_STR` est ce que le serveur considère comme sa racine sur le disque.
          |     # Les chemins du client sont relatifs à une "vue" de cette racine.
          |     # Si LLMBDO_FS_DATA_ROOT (utilisé pour FS_VIRTUAL_ROOT_STR) est /mnt/user_data,
          |     # et que le client envoie "/foo.txt", il veut /mnt/user_data/foo.txt.
          |     # Si le client envoie "/", il veut /mnt/user_data.
          | 
          |     path_relative_to_fs_root_for_common_util: str
          |     
          |     # Si le client envoie un chemin qui correspond exactement à ce que le shell considère comme sa racine
          |     # (par exemple, si le shell a un CWD de /mnt/user_data et que l'utilisateur tape "ls .",
          |     # le shell pourrait envoyer "/mnt/user_data" comme chemin à lister).
          |     # Ou si le client envoie explicitement le chemin de la racine virtuelle du FS.
          |     # On considère que le client envoie toujours des chemins "normalisés" où "/" est la racine de sa vue.
          |     
          |     if path_from_client == "/" or path_from_client == ".": # Le client demande la racine de ce qu'il voit
          |         path_relative_to_fs_root_for_common_util = ""
          |     else:
          |         path_relative_to_fs_root_for_common_util = path_from_client.lstrip('/\\')
          |     
          |     fs_server.logger.debug(f"_validate_fs_path: Passing to common_util: relative_path='{path_relative_to_fs_root_for_common_util}', virtual_root='{FS_VIRTUAL_ROOT_STR}'")
          | 
          |     resolved_disk_path, err_msg = validate_mcp_path_param(
          |         path_param_relative_to_root=path_relative_to_fs_root_for_common_util,
          |         virtual_root_str=FS_VIRTUAL_ROOT_STR,         
          |         check_exists=check_exists,
          |         must_be_dir=must_be_dir,
          |         must_be_file=must_be_file
          |     )
          | 
          |     if err_msg:
          |         final_error_message = f"Error for client path '{path_from_client}': {err_msg}"
          |         fs_server.logger.warning(final_error_message)
          |         raise ValueError(final_error_message) 
          |         
          |     if resolved_disk_path is None:
          |         raise ValueError(f"Path validation failed unexpectedly for client path '{path_from_client}'.")
          |             
          |     fs_server.logger.debug(f"_validate_fs_path: Client path '{path_from_client}' resolved to disk path '{resolved_disk_path}'")
          |     return resolved_disk_path
          | 
          | def _get_client_facing_path(abs_disk_path: Path) -> str:
          |     """Converts an absolute disk path back to a client-facing path (relative to virtual root, starts with /)."""
          |     virtual_root_path = Path(FS_VIRTUAL_ROOT_STR).resolve()
          |     try:
          |         relative_path = abs_disk_path.relative_to(virtual_root_path)
          |         return "/" + str(relative_path)
          |     except ValueError: # Path is not under virtual_root (should not happen if validation is correct)
          |         fs_server.logger.error(f"Cannot make client-facing path for {abs_disk_path}, not under {virtual_root_path}")
          |         return str(abs_disk_path) # Fallback, but indicates an issue
          | 
          | 
          | # --- Embedding and Search (Sync blocking functions for executor) ---
          | def _load_embedding_model_sync(server: MCPServer):
          |     if not server.embedding_enabled: return # type: ignore
          |     if server.embedding_model is None: # type: ignore
          |         server.logger.info(f"Loading sentence transformer model: {EMBEDDING_MODEL_NAME_CONF}")
          |         if not EMBEDDING_MODEL_NAME_CONF or EMBEDDING_MODEL_NAME_CONF == "disabled":
          |             server.logger.warning("Embedding model name not configured or disabled. Cannot load.")
          |             server.embedding_enabled = False; return # type: ignore
          |         try:
          |             server.embedding_model = SentenceTransformer(EMBEDDING_MODEL_NAME_CONF) # type: ignore
          |         except Exception as e:
          |             server.logger.error(f"Failed to load ST model '{EMBEDDING_MODEL_NAME_CONF}': {e}", exc_info=True)
          |             server.embedding_enabled = False; raise # type: ignore
          | 
          | def _load_faiss_index_sync(server: MCPServer):
          |     if not server.embedding_enabled: return # type: ignore
          |     if server.faiss_index is not None: return # type: ignore
          |     
          |     server.logger.info(f"Initializing/Loading FAISS index from: {FAISS_INDEX_FILE_PATH_CONF}")
          |     if FAISS_INDEX_FILE_PATH_CONF.exists() and FAISS_METADATA_FILE_PATH_CONF.exists():
          |         try:
          |             server.faiss_index = faiss.read_index(str(FAISS_INDEX_FILE_PATH_CONF)) # type: ignore
          |             with FAISS_METADATA_FILE_PATH_CONF.open('r') as f: server.faiss_index_metadata = json.load(f) # type: ignore
          |             if server.faiss_index_metadata: # type: ignore
          |                 server.faiss_next_id = max(item['id'] for item in server.faiss_index_metadata) + 1 if server.faiss_index_metadata else 0 # type: ignore
          |             server.logger.info(f"FAISS index loaded with {server.faiss_index.ntotal if server.faiss_index else 0} vectors.") # type: ignore
          |             return
          |         except Exception as e:
          |             server.logger.error(f"Failed to load FAISS index/metadata from {FAISS_INDEX_DIR_PATH_CONF}: {e}. Will create new.", exc_info=True)
          |             # Reset to ensure clean state for new index creation
          |             server.faiss_index = None; server.faiss_index_metadata = []; server.faiss_next_id = 0 # type: ignore
          |     
          |     # Create new index if loading failed or files don't exist
          |     if server.embedding_model is None: _load_embedding_model_sync(server) # Ensure model is loaded to get dim
          |     if not server.embedding_enabled: return # type: ignore
          | 
          |     embedding_dim = server.embedding_model.get_sentence_embedding_dimension() # type: ignore
          |     server.logger.info(f"Creating new FAISS index (dim: {embedding_dim}) at {FAISS_INDEX_DIR_PATH_CONF}.")
          |     server.faiss_index = faiss.IndexIDMap(faiss.IndexFlatL2(embedding_dim)) # type: ignore
          |     server.faiss_index_metadata = []; server.faiss_next_id = 0 # type: ignore
          |     _save_faiss_index_sync(server) # Save empty index and metadata
          | 
          | def _save_faiss_index_sync(server: MCPServer):
          |     if server.embedding_enabled and server.faiss_index is not None: # type: ignore
          |         try:
          |             FAISS_INDEX_DIR_PATH_CONF.mkdir(parents=True, exist_ok=True)
          |             server.logger.info(f"Saving FAISS index ({server.faiss_index.ntotal} vectors) & metadata to {FAISS_INDEX_DIR_PATH_CONF}...") # type: ignore
          |             faiss.write_index(server.faiss_index, str(FAISS_INDEX_FILE_PATH_CONF)) # type: ignore
          |             with FAISS_METADATA_FILE_PATH_CONF.open('w') as f: json.dump(server.faiss_index_metadata, f) # type: ignore
          |             server.logger.info("FAISS index and metadata saved.")
          |         except Exception as e: server.logger.error(f"Failed to save FAISS index/metadata: {e}", exc_info=True)
          | 
          | 
          | # --- Server Lifecycle Hooks ---
          | async def on_fs_server_startup(server: MCPServer):
          |     server.logger.info(f"FS Server '{server.server_name}' on_startup hook running...")
          |     
          |     # Initialize embedding_enabled state based on config and available libraries
          |     server.embedding_enabled = EMBEDDING_SYSTEM_AVAILABLE and (EMBEDDING_MODEL_NAME_CONF != "disabled") # type: ignore
          |     if not server.embedding_enabled: # type: ignore
          |         if not EMBEDDING_SYSTEM_AVAILABLE:
          |             server.logger.warning("Embedding system dependencies (sentence-transformers, faiss, numpy) missing. Embed/search capabilities disabled.")
          |         elif EMBEDDING_MODEL_NAME_CONF == "disabled":
          |             server.logger.info("Embedding model name configured as 'disabled'. Embed/search capabilities disabled.")
          |         return
          | 
          |     # Create FAISS directory if it doesn't exist
          |     try:
          |         FAISS_INDEX_DIR_PATH_CONF.mkdir(parents=True, exist_ok=True)
          |         server.logger.info(f"FAISS index directory ensured at: {FAISS_INDEX_DIR_PATH_CONF}")
          |     except OSError as e:
          |         server.logger.error(f"Could not create FAISS directory {FAISS_INDEX_DIR_PATH_CONF}: {e}. Embedding will be disabled.")
          |         server.embedding_enabled = False; return # type: ignore
          |         
          |     server.logger.info("Pre-loading embedding model and FAISS index in executor...")
          |     try:
          |         await server.run_in_executor(_load_embedding_model_sync, server)
          |         await server.run_in_executor(_load_faiss_index_sync, server)
          |         server.logger.info("Embedding model and FAISS index initialized successfully.")
          |     except Exception as e:
          |         server.logger.error(f"Error during startup initialization of embedding system: {e}", exc_info=True)
          |         server.embedding_enabled = False # type: ignore
          |         server.logger.warning("Embedding system has been disabled due to startup error.")
          | 
          | async def on_fs_server_shutdown(server: MCPServer):
          |     server.logger.info(f"FS Server '{server.server_name}' on_shutdown hook running...")
          |     if server.embedding_enabled and server.faiss_index is not None: # type: ignore
          |         server.logger.info("Attempting to save FAISS index on shutdown...")
          |         try:
          |             await server.run_in_executor(_save_faiss_index_sync, server)
          |         except Exception as e_save:
          |             server.logger.error(f"Failed to save FAISS index via executor during shutdown: {e_save}. Attempting sync save.", exc_info=True)
          |             try: _save_faiss_index_sync(server)
          |             except Exception as e_sync_save: server.logger.error(f"Synchronous FAISS save also failed: {e_sync_save}", exc_info=True)
          | 
          | fs_server.set_startup_hook(on_fs_server_startup)
          | fs_server.set_shutdown_hook(on_fs_server_shutdown)
          | 
          | 
          | # --- File System Capability Handlers ---
          | @fs_server.register_method("mcp.fs.list")
          | async def handle_fs_list(server: MCPServer, request_id: Optional[Union[str, int]], params: List[Any]):
          |     client_path_str = params[0]
          |     target_path_abs = _validate_fs_path(client_path_str, check_exists=True, must_be_dir=True)
          | 
          |     def list_dir_sync():
          |         items = []
          |         for item_abs_path in target_path_abs.iterdir():
          |             try:
          |                 stat_info = item_abs_path.stat()
          |                 item_type = "other"
          |                 if item_abs_path.is_file(): item_type = "file"
          |                 elif item_abs_path.is_dir(): item_type = "directory"
          |                 elif item_abs_path.is_symlink(): item_type = "symlink"
          |                 
          |                 items.append({
          |                     "name": item_abs_path.name,
          |                     "path": _get_client_facing_path(item_abs_path),
          |                     "type": item_type,
          |                     "size": stat_info.st_size if item_type != "directory" else -1,
          |                     "modified_at": datetime.fromtimestamp(stat_info.st_mtime, tz=timezone.utc).isoformat()
          |                 })
          |             except OSError as stat_err:
          |                 server.logger.warning(f"Could not stat {item_abs_path.name} in {target_path_abs}: {stat_err}")
          |                 items.append({"name": item_abs_path.name, "path": _get_client_facing_path(item_abs_path), 
          |                               "type": "inaccessible", "size": -1, "modified_at": None})
          |         return items
          |     
          |     try: return await server.run_in_executor(list_dir_sync)
          |     except ValueError as ve: raise # From _validate_fs_path
          |     except PermissionError as pe: # From iterdir or stat
          |         raise server.create_custom_error(request_id, 1, f"Permission denied for path '{client_path_str}'.", {"path": client_path_str}) from pe
          | 
          | @fs_server.register_method("mcp.fs.read")
          | async def handle_fs_read(server: MCPServer, request_id: Optional[Union[str, int]], params: List[Any]):
          |     client_path_str = params[0]
          |     encoding_type = params[1] if len(params) > 1 else "text"
          |     target_file_abs = _validate_fs_path(client_path_str, check_exists=True, must_be_file=True)
          | 
          |     def read_file_sync():
          |         mime_type_str = "application/octet-stream"
          |         try: mime_type_str = magic.from_file(str(target_file_abs), mime=True)
          |         except Exception as e_magic: server.logger.warning(f"Magic lib error for {target_file_abs}: {e_magic}")
          | 
          |         content_data: str
          |         if encoding_type == "text":
          |             try: content_data = target_file_abs.read_text(encoding="utf-8")
          |             except UnicodeDecodeError: raise ValueError(f"File '{client_path_str}' is not valid UTF-8. Try 'base64' encoding.")
          |         elif encoding_type == "base64":
          |             content_data = base64.b64encode(target_file_abs.read_bytes()).decode('ascii')
          |         else: raise ValueError(f"Unsupported encoding '{encoding_type}'.") # Should be caught by schema
          | 
          |         return {"path": _get_client_facing_path(target_file_abs), "content": content_data, 
          |                 "encoding": encoding_type, "mime_type": mime_type_str}
          | 
          |     try: return await server.run_in_executor(read_file_sync)
          |     except ValueError as ve: raise
          |     except PermissionError as pe:
          |         raise server.create_custom_error(request_id, 1, f"Permission denied reading '{client_path_str}'.", {"path": client_path_str}) from pe
          | 
          | 
          | @fs_server.register_method("mcp.fs.write")
          | async def handle_fs_write(server: MCPServer, request_id: Optional[Union[str, int]], params: List[Any]):
          |     client_path_str = params[0]
          |     content_to_write = params[1]
          |     encoding_type = params[2] if len(params) > 2 else "text"
          |     append_mode = params[3] if len(params) > 3 else False
          |     
          |     target_file_abs = _validate_fs_path(client_path_str, check_exists=False) # File may not exist
          | 
          |     if not target_file_abs.parent.is_dir():
          |          raise ValueError(f"Parent directory for '{client_path_str}' does not exist or is not a directory.")
          |     if target_file_abs.exists() and target_file_abs.is_dir():
          |         raise ValueError(f"Cannot write to '{client_path_str}', it is an existing directory.")
          | 
          |     def write_file_sync():
          |         bytes_to_write: bytes
          |         if encoding_type == "text": bytes_to_write = content_to_write.encode('utf-8')
          |         elif encoding_type == "base64":
          |             try: bytes_to_write = base64.b64decode(content_to_write)
          |             except Exception: raise ValueError("Invalid base64 content for writing.")
          |         else: raise ValueError(f"Unsupported encoding type '{encoding_type}'.")
          | 
          |         mode = 'ab' if append_mode else 'wb'
          |         with target_file_abs.open(mode) as f: num_bytes_written = f.write(bytes_to_write)
          |         return {"path": _get_client_facing_path(target_file_abs), "bytes_written": num_bytes_written, "status": "success"}
          | 
          |     try: return await server.run_in_executor(write_file_sync)
          |     except ValueError as ve: raise
          |     except PermissionError as pe:
          |         raise server.create_custom_error(request_id, 1, f"Permission denied writing to '{client_path_str}'.", {"path": client_path_str}) from pe
          | 
          | 
          | @fs_server.register_method("mcp.fs.delete")
          | async def handle_fs_delete(server: MCPServer, request_id: Optional[Union[str, int]], params: List[Any]):
          |     client_path_str = params[0]
          |     recursive = params[1] if len(params) > 1 else False
          |     target_path_abs = _validate_fs_path(client_path_str, check_exists=True)
          | 
          |     def delete_path_sync():
          |         if target_path_abs.is_file() or target_path_abs.is_symlink(): target_path_abs.unlink()
          |         elif target_path_abs.is_dir():
          |             if recursive: shutil.rmtree(target_path_abs)
          |             else:
          |                 try: target_path_abs.rmdir()
          |                 except OSError: raise ValueError(f"Directory '{client_path_str}' not empty. Use recursive=true to delete.")
          |         else: raise ValueError(f"Path '{client_path_str}' is an unknown type for deletion.")
          |         return {"path": _get_client_facing_path(target_path_abs), "status": "success"}
          | 
          |     try: return await server.run_in_executor(delete_path_sync)
          |     except ValueError as ve: raise
          |     except PermissionError as pe:
          |         raise server.create_custom_error(request_id, 1, f"Permission denied deleting '{client_path_str}'.", {"path": client_path_str}) from pe
          | 
          | 
          | @fs_server.register_method("mcp.fs.embed")
          | async def handle_fs_embed(server: MCPServer, request_id: Optional[Union[str, int]], params: List[Any]):
          |     if not server.embedding_enabled: # type: ignore
          |         raise RuntimeError("Embedding system is disabled for this server instance.")
          |     
          |     client_path_str = params[0]
          |     recursive = params[1] if len(params) > 1 else False
          |     target_path_abs = _validate_fs_path(client_path_str, check_exists=True)
          | 
          |     def embed_path_sync():
          |         embedding_model = _get_embedding_model_sync(server)
          |         faiss_idx = _get_faiss_index_sync(server)
          | 
          |         files_to_embed_abs: List[Path] = []
          |         if target_path_abs.is_file(): files_to_embed_abs.append(target_path_abs)
          |         elif target_path_abs.is_dir():
          |             glob_pattern = "**/*" if recursive else "*"
          |             for item_path_abs in target_path_abs.glob(glob_pattern):
          |                 if item_path_abs.is_file(): files_to_embed_abs.append(item_path_abs)
          |         
          |         processed_count = 0
          |         MAX_FILE_SIZE_BYTES = int(os.getenv("LLMBDO_FS_EMBED_MAX_SIZE_KB", "1024")) * 1024
          |         new_embeddings_data = []
          |         new_metadata_entries = []
          |         
          |         existing_client_paths = {item['path'] for item in server.faiss_index_metadata} # type: ignore
          | 
          |         for file_abs_path in files_to_embed_abs:
          |             client_facing_file_path = _get_client_facing_path(file_abs_path)
          |             if client_facing_file_path in existing_client_paths:
          |                 server.logger.debug(f"Skipping already embedded: {client_facing_file_path}")
          |                 continue
          |             try:
          |                 if file_abs_path.stat().st_size > MAX_FILE_SIZE_BYTES:
          |                     server.logger.warning(f"Skipping large file {file_abs_path} for embedding.")
          |                     continue
          |                 content = file_abs_path.read_text(encoding='utf-8', errors='ignore')
          |                 if not content.strip(): continue
          |                 
          |                 server.logger.debug(f"Embedding: {client_facing_file_path}")
          |                 embedding_vec = embedding_model.encode([content])[0]
          |                 new_embeddings_data.append(embedding_vec.astype('float32'))
          |                 new_metadata_entries.append({"id": server.faiss_next_id, "path": client_facing_file_path}) # type: ignore
          |                 server.faiss_next_id += 1 # type: ignore
          |                 processed_count += 1
          |             except Exception as e_single_embed:
          |                 server.logger.error(f"Error embedding file {file_abs_path}: {e_single_embed}", exc_info=True)
          |         
          |         if new_embeddings_data:
          |             try:
          |                 embeddings_np = np.array(new_embeddings_data) # type: ignore
          |                 ids_np = np.array([m['id'] for m in new_metadata_entries], dtype='int64') # type: ignore
          |                 faiss_idx.add_with_ids(embeddings_np, ids_np) # type: ignore
          |                 server.faiss_index_metadata.extend(new_metadata_entries) # type: ignore
          |                 _save_faiss_index_sync(server)
          |                 server.logger.info(f"Added {len(new_embeddings_data)} new embeddings. Total: {faiss_idx.ntotal}.") # type: ignore
          |             except Exception as e_faiss:
          |                 server.logger.error(f"Error adding embeddings to FAISS: {e_faiss}", exc_info=True)
          |                 raise RuntimeError(f"Failed to update search index: {e_faiss}")
          |         
          |         return {"path_processed": client_path_str, "files_embedded_this_run": processed_count,
          |                 "total_embeddings_in_index": faiss_idx.ntotal, "status": "success"} # type: ignore
          | 
          |     try: return await server.run_in_executor(embed_path_sync)
          |     except ValueError as ve: raise
          |     except RuntimeError as rte:
          |         raise server.create_custom_error(request_id, 2, str(rte), {"path": client_path_str}) from rte
          | 
          | 
          | @fs_server.register_method("mcp.fs.search")
          | async def handle_fs_search(server: MCPServer, request_id: Optional[Union[str, int]], params: List[Any]):
          |     if not server.embedding_enabled: # type: ignore
          |         raise RuntimeError("Search system is disabled.")
          | 
          |     def _ensure_search_ready_sync_local(): # Renamed to avoid conflict
          |         _load_embedding_model_sync(server)
          |         _load_faiss_index_sync(server)
          |         if server.faiss_index is None or server.faiss_index.ntotal == 0: # type: ignore
          |             raise RuntimeError("Search index not ready or empty.")
          |     await server.run_in_executor(_ensure_search_ready_sync_local)
          | 
          |     query_text = params[0]
          |     top_k = int(params[1]) if len(params) > 1 else 5
          |     scope_client_path_str = params[2] if len(params) > 2 else None
          |     
          |     # Resolve virtual root once for path operations
          |     _fs_virtual_root_resolved = Path(FS_VIRTUAL_ROOT_STR).resolve()
          |     
          |     scope_filter_prefix: Optional[str] = None
          |     if scope_client_path_str:
          |         # Validate the scope path and convert to a client-facing prefix
          |         scope_abs_path = _validate_fs_path(scope_client_path_str, check_exists=True, must_be_dir=True)
          |         scope_filter_prefix = _get_client_facing_path(scope_abs_path)
          |         if not scope_filter_prefix.endswith('/'): scope_filter_prefix += '/' # Ensure it's a dir prefix
          | 
          |     def search_sync():
          |         embedding_model = server.embedding_model # type: ignore
          |         faiss_idx: faiss.Index = server.faiss_index # type: ignore
          | 
          |         query_embedding = embedding_model.encode([query_text])[0].astype('float32').reshape(1, -1)
          |         
          |         # Fetch more results if filtering by scope, then narrow down
          |         k_to_fetch_faiss = max(top_k * 5, 20) if scope_filter_prefix else top_k
          |         k_to_fetch_faiss = min(k_to_fetch_faiss, faiss_idx.ntotal)
          |         if k_to_fetch_faiss == 0: return []
          | 
          |         distances, faiss_ids_array = faiss_idx.search(query_embedding, k=k_to_fetch_faiss)
          |         
          |         search_results = []
          |         for i in range(len(faiss_ids_array[0])):
          |             faiss_id_val = faiss_ids_array[0][i]
          |             if faiss_id_val == -1: continue
          |             
          |             meta = next((m for m in server.faiss_index_metadata if m['id'] == faiss_id_val), None) # type: ignore
          |             if not meta: continue
          |             
          |             client_path_found = meta['path']
          |             if scope_filter_prefix and not client_path_found.startswith(scope_filter_prefix):
          |                 continue
          | 
          |             similarity_score = float(1.0 / (1.0 + distances[0][i]))
          |             preview = ""
          |             try:
          |                 # Convert client path back to absolute disk path for preview reading
          |                 abs_path_for_preview = (_fs_virtual_root_resolved / client_path_found.lstrip('/')).resolve()
          |                 # Security check: ensure preview path is still within the virtual root (after resolving symlinks etc.)
          |                 if abs_path_for_preview.is_file() and validate_mcp_path_param(str(abs_path_for_preview), virtual_root_str=FS_VIRTUAL_ROOT_STR)[1] is None:
          |                     with open(abs_path_for_preview, 'r', encoding='utf-8', errors='ignore') as pf:
          |                         preview_content = pf.read(250)
          |                         preview = preview_content.strip() + ("..." if len(preview_content) == 250 else "")
          |             except Exception as e_prev: server.logger.debug(f"Could not get preview for {client_path_found}: {e_prev}")
          | 
          |             search_results.append({"path": client_path_found, "score": round(similarity_score, 4), "preview": preview})
          |             if len(search_results) >= top_k: break 
          |         
          |         search_results.sort(key=lambda x: x['score'], reverse=True)
          |         return search_results[:top_k]
          | 
          |     try: return await server.run_in_executor(search_sync)
          |     except ValueError as ve: raise # From _validate_fs_path on scope_path
          |     except RuntimeError as rte:
          |         raise server.create_custom_error(request_id, 3, str(rte), {"query": query_text}) from rte
          | 
          | 
          | # --- Main Entry Point ---
          | # llmbasedos_pkg/servers/fs/server.py
          | # ... (tous les imports, définitions de constantes, instance fs_server, handlers, hooks) ...
          | 
          | # Le if __name__ == "__main__": doit uniquement contenir l'appel pour démarrer le serveur
          | if __name__ == "__main__":
          |     # Initialisation du logging spécifique pour le script si pas déjà fait par MCPServer au niveau module
          |     if not fs_server.logger.hasHandlers(): # Vérifier si le logger de l'instance a déjà des handlers
          |         _fallback_lvl = logging.INFO
          |         _log_lvl_main = logging.getLevelName(os.getenv(f"LLMBDO_{SERVER_NAME.upper()}_LOG_LEVEL", "INFO").upper())
          |         if not isinstance(_log_lvl_main, int): _log_lvl_main = _fallback_lvl
          |         logging.basicConfig(level=_log_lvl_main, format=f"%(asctime)s - FS_MAIN - %(levelname)s - %(message)s")
          |         # Ou configurez fs_server.logger ici plus spécifiquement si MCPServer ne le fait pas assez tôt.
          | 
          |     if not FS_VIRTUAL_ROOT_STR: # Vérification critique
          |         fs_server.logger.critical(f"FS Server CRITICAL: FS_VIRTUAL_ROOT_STR not defined!")
          |         sys.exit(1) # Utiliser sys.exit
          |     
          |     _final_root_to_check = Path(FS_VIRTUAL_ROOT_STR).resolve()
          |     if not _final_root_to_check.is_dir():
          |         fs_server.logger.critical(f"FS Server CRITICAL: Virtual root '{_final_root_to_check}' is not an existing directory.")
          |         sys.exit(1)
          | 
          |     fs_server.logger.info(f"FS Server '{SERVER_NAME}' starting with effective virtual root: {FS_VIRTUAL_ROOT_STR}")
          |     try:
          |         asyncio.run(fs_server.start())
          |     except KeyboardInterrupt:
          |         fs_server.logger.info(f"FS Server '{SERVER_NAME}' (main) stopped by KeyboardInterrupt.")
          |     # ... (le reste de votre bloc __main__ pour le cleanup) ...
          | 
          |     try:
          |         asyncio.run(fs_server.start())
          |     except KeyboardInterrupt:
          |         fs_server.logger.info(f"FS Server '{SERVER_NAME}' (main) stopped by KeyboardInterrupt.")
          |     except Exception as e_main_fs:
          |         fs_server.logger.critical(f"FS Server '{SERVER_NAME}' (main) crashed: {e_main_fs}", exc_info=True)
          |     finally:
          |         fs_server.logger.info(f"FS Server '{SERVER_NAME}' (main) exiting.")
          --- Fin Contenu ---

      Répertoire: ./llmbasedos_src/servers/sync
        Fichier: caps.json
          --- Début Contenu (ascii) ---
          | {
          |     "service_name": "sync",
          |     "description": "Manages and executes rclone-based synchronization tasks.",
          |     "version": "0.1.1",
          |     "capabilities": [
          |         {
          |             "method": "mcp.sync.listRemotes",
          |             "description": "Lists configured rclone remotes.",
          |             "params_schema": { "type": "array", "maxItems": 0 },
          |             "result_schema": {"type": "array", "items": {"type": "string"}}
          |         },
          |         {
          |             "method": "mcp.sync.listJobs",
          |             "description": "Lists currently defined/known sync jobs.",
          |             "params_schema": { "type": "array", "maxItems": 0 },
          |             "result_schema": {
          |                 "type": "array", "items": {
          |                     "type": "object", "properties": {
          |                         "job_id": {"type": "string"},
          |                         "description": {"type": "string", "optional": true},
          |                         "source": {"type": "string"}, "destination": {"type": "string"},
          |                         "status": {"type": "string", "enum": ["idle", "running", "completed", "failed", "stopping", "unknown"]},
          |                         "is_running": {"type": "boolean"},
          |                         "start_time": {"type": ["string", "null"], "format": "date-time"},
          |                         "end_time": {"type": ["string", "null"], "format": "date-time"},
          |                         "pid": {"type": ["integer", "null"]}
          |                     }, "required": ["job_id", "source", "destination", "status", "is_running"]
          |                 }
          |             }
          |         },
          |         {
          |             "method": "mcp.sync.runJob",
          |             "description": "Manually triggers a sync operation.",
          |             "params_schema": {
          |                 "type": "array", "minItems": 1, "maxItems": 1, "items": [{
          |                     "type": "object", "properties": {
          |                         "job_id_prefix": {"type": "string", "optional": true, "description": "Optional prefix for ad-hoc job ID."},
          |                         "source": {"type": "string", "description": "Source path (e.g., 'local:/path' or 'myremote:bucket')."},
          |                         "destination": {"type": "string", "description": "Destination path."},
          |                         "rclone_args": {"type": "array", "items": {"type": "string"}, "optional": true, "description": "Additional rclone arguments."}
          |                     }, "required": ["source", "destination"]
          |                 }]
          |             },
          |             "result_schema": {
          |                 "type": "object", "properties": { "job_id": {"type": "string"}, "status": {"type": "string"}, "message": {"type": "string", "optional": true}, "pid": {"type": ["integer", "null"]}},
          |                 "required": ["job_id", "status"]
          |             }
          |         },
          |         {
          |             "method": "mcp.sync.getJobStatus",
          |             "description": "Gets status and logs of a sync job.",
          |             "params_schema": { "type": "array", "minItems": 1, "maxItems": 1, "items": [{"type": "string", "description": "The job_id."}]},
          |             "result_schema": {
          |                 "type": "object", "properties": {
          |                     "job_id": {"type": "string"}, "is_running": {"type": "boolean"},
          |                     "status_message": {"type": "string"},
          |                     "start_time": {"type": ["string", "null"], "format": "date-time"},
          |                     "end_time": {"type": ["string", "null"], "format": "date-time"},
          |                     "return_code": {"type": ["integer", "null"]},
          |                     "log_preview": {"type": "array", "items": {"type": "string"}, "optional": true}
          |                 }, "required": ["job_id", "is_running", "status_message"]
          |             }
          |         },
          |         {
          |             "method": "mcp.sync.stopJob",
          |             "description": "Stops a running sync job.",
          |             "params_schema": { "type": "array", "minItems": 1, "maxItems": 1, "items": [{"type": "string", "description": "The job_id to stop."}]},
          |             "result_schema": {
          |                 "type": "object", "properties": {"job_id": {"type": "string"}, "status": {"type": "string"}, "message": {"type": "string", "optional": true}},
          |                 "required": ["job_id", "status"]
          |             }
          |         }
          |     ]
          | }
          --- Fin Contenu ---
        Fichier: requirements.txt
          --- Début Contenu (ascii) ---
          | # llmbasedos/servers/sync/requirements.txt
          | # rclone should be installed as a system binary.
          | pyyaml>=6.0 # For potential future job definitions from YAML
          | # schedule library was removed as job checking is now simpler thread
          --- Fin Contenu ---
        Fichier: server.py
          --- Début Contenu (ascii) ---
          | # llmbasedos/servers/sync/server.py
          | import asyncio
          | import logging # Logger obtained from MCPServer
          | import os
          | from pathlib import Path
          | import uuid
          | import subprocess
          | import signal
          | import threading # For background job process checker
          | import time
          | from datetime import datetime, timezone
          | from typing import Any, Dict, List, Optional, Tuple, Union
          | 
          | # --- Import Framework ---
          | from llmbasedos.mcp_server_framework import MCPServer 
          | from llmbasedos.common_utils import validate_mcp_path_param # Assurez-vous que fs_server en a besoin
          | 
          | # --- Server Specific Configuration ---
          | SERVER_NAME = "sync"
          | CAPS_FILE_PATH_STR = str(Path(__file__).parent / "caps.json")
          | SYNC_CUSTOM_ERROR_BASE = -32020
          | 
          | RCLONE_CONFIG_PATH_CONF = Path(os.getenv("LLMBDO_RCLONE_CONFIG_PATH", os.path.expanduser("~/.config/rclone/rclone.conf"))).resolve()
          | RCLONE_EXECUTABLE_CONF = os.getenv("LLMBDO_RCLONE_EXECUTABLE", "rclone")
          | SYNC_JOB_LOG_DIR_CONF = Path(os.getenv("LLMBDO_SYNC_JOB_LOG_DIR", f"/var/log/llmbasedos/{SERVER_NAME}"))
          | SYNC_JOB_LOG_DIR_CONF.mkdir(parents=True, exist_ok=True) # Ensure log dir exists
          | 
          | # In-memory stores, managed by the server instance
          | # SYNC_JOBS_STATE: Dict[str, Dict[str, Any]] = {} # job_id -> job_data (moved to server instance)
          | # RCLONE_PROCESSES_STATE: Dict[str, subprocess.Popen] = {} # job_id -> Popen (moved to server instance)
          | 
          | # Initialize server instance
          | sync_server = MCPServer(SERVER_NAME, CAPS_FILE_PATH_STR, custom_error_code_base=SYNC_CUSTOM_ERROR_BASE)
          | 
          | # Attach server-specific state to the instance
          | sync_server.sync_jobs_state: Dict[str, Dict[str, Any]] = {} # type: ignore
          | sync_server.rclone_processes_state: Dict[str, subprocess.Popen] = {} # type: ignore
          | sync_server.job_check_thread_stop_event = threading.Event() # type: ignore
          | sync_server.job_check_thread: Optional[threading.Thread] = None # type: ignore
          | 
          | 
          | # --- Rclone Utilities (Blocking, for executor) ---
          | def _run_rclone_cmd_blocking(server: MCPServer, args: List[str], job_info_context: str) -> Tuple[int, str, str]:
          |     cmd = [RCLONE_EXECUTABLE_CONF, f"--config={RCLONE_CONFIG_PATH_CONF}"] + args
          |     server.logger.info(f"Rclone (ctx: {job_info_context}): Executing {' '.join(cmd)}")
          |     try:
          |         # Increased timeout for potentially slower remote operations like listremotes
          |         proc = subprocess.run(cmd, capture_output=True, text=True, check=False, timeout=120)
          |         server.logger.info(f"Rclone (ctx: {job_info_context}) finished with code {proc.returncode}")
          |         return proc.returncode, proc.stdout, proc.stderr
          |     except FileNotFoundError: msg = f"rclone executable '{RCLONE_EXECUTABLE_CONF}' not found."; server.logger.error(msg); return -1, "", msg
          |     except subprocess.TimeoutExpired: msg = f"rclone cmd (ctx: {job_info_context}) timed out."; server.logger.error(msg); return -2, "", msg
          |     except Exception as e: server.logger.error(f"Rclone cmd error (ctx: {job_info_context}): {e}", exc_info=True); return -3, "", str(e)
          | 
          | def _start_rclone_sync_proc_blocking(
          |     server: MCPServer, job_id: str, source: str, destination: str, extra_args: Optional[List[str]] = None
          | ) -> Tuple[Optional[int], str]: # Returns PID or None, and error_message_str
          |     
          |     if job_id in server.rclone_processes_state and server.rclone_processes_state[job_id].poll() is None: # type: ignore
          |         return None, "Job is already running."
          | 
          |     # Using "copy" for safety by default. Can be overridden by rclone_args if user passes "sync" command.
          |     # Or, make the command (copy/sync) a parameter.
          |     rclone_command_verb = "copy" 
          |     # Check if user provided a verb in extra_args (e.g. "sync", "move")
          |     # This is a bit naive; a full rclone command parser would be better.
          |     if extra_args and extra_args[0] in ["sync", "move", "check", "copyto", "moveto", "copy"]:
          |         rclone_command_verb = extra_args.pop(0) # Use user's verb and remove from args
          | 
          |     cmd = [RCLONE_EXECUTABLE_CONF, f"--config={RCLONE_CONFIG_PATH_CONF}", rclone_command_verb,
          |            source, destination, "--progress", "-v", "--log-level", "INFO"] # Default log level for rclone
          |     if extra_args: cmd.extend(extra_args)
          | 
          |     log_file = SYNC_JOB_LOG_DIR_CONF / f"{job_id}.log"
          |     server.logger.info(f"Job {job_id}: Starting rclone: {' '.join(cmd)}. Log: {log_file}")
          |     
          |     try:
          |         with open(log_file, 'ab') as lf: # Append binary for robustness
          |             lf.write(f"\n--- Job '{job_id}' started at {datetime.now(timezone.utc).isoformat()} ---\n".encode())
          |             lf.write(f"Command: {' '.join(cmd)}\n---\n".encode())
          |             lf.flush()
          |             # Use os.setsid for process group management on POSIX for reliable termination
          |             proc = subprocess.Popen(cmd, stdout=lf, stderr=subprocess.STDOUT, text=False, 
          |                                     preexec_fn=os.setsid if os.name != 'nt' else None)
          |         
          |         server.rclone_processes_state[job_id] = proc # type: ignore
          |         job_entry = server.sync_jobs_state.get(job_id, {"job_id": job_id, "is_adhoc": True}) # type: ignore
          |         job_entry.update({
          |             "source": source, "destination": destination, "rclone_args": extra_args or [],
          |             "process_pid": proc.pid, "status": "running", 
          |             "start_time": datetime.now(timezone.utc), "log_file": str(log_file) # Store as string
          |         })
          |         server.sync_jobs_state[job_id] = job_entry # type: ignore
          |         return proc.pid, ""
          |     except FileNotFoundError: msg = f"rclone executable '{RCLONE_EXECUTABLE_CONF}' not found."; server.logger.error(msg); return None, msg
          |     except Exception as e: server.logger.error(f"Job {job_id}: Failed to start rclone: {e}", exc_info=True); return None, str(e)
          | 
          | # --- Background Job Process Checker Thread ---
          | def _job_process_checker_thread_target(server: MCPServer):
          |     server.logger.info("Rclone job process checker thread started.")
          |     while not server.job_check_thread_stop_event.is_set(): # type: ignore
          |         for job_id, process in list(server.rclone_processes_state.items()): # type: ignore # Iterate copy
          |             if process.poll() is not None: # Process finished
          |                 server.logger.info(f"Job {job_id} (PID {process.pid}) process finished with code {process.returncode}.")
          |                 if job_id in server.sync_jobs_state: # type: ignore
          |                     job_data = server.sync_jobs_state[job_id] # type: ignore
          |                     job_data["status"] = "completed" if process.returncode == 0 else "failed"
          |                     job_data["end_time"] = datetime.now(timezone.utc)
          |                     job_data["return_code"] = process.returncode
          |                     job_data["process_pid"] = None # Clear PID as process is gone
          |                 server.rclone_processes_state.pop(job_id, None) # type: ignore # Remove from active
          |         
          |         # Wait for a bit or until stop event is set
          |         server.job_check_thread_stop_event.wait(timeout=5) # Check every 5 seconds # type: ignore
          |     server.logger.info("Rclone job process checker thread stopped.")
          | 
          | 
          | # --- Sync Capability Handlers (decorated) ---
          | @sync_server.register_method("mcp.sync.listRemotes")
          | async def handle_sync_list_remotes(server: MCPServer, request_id: str, params: List[Any]):
          |     ret_code, stdout, stderr = await server.run_in_executor(
          |         _run_rclone_cmd_blocking, server, ["listremotes"], "mcp.sync.listRemotes"
          |     )
          |     if ret_code != 0: raise RuntimeError(f"Failed to list rclone remotes: {stderr or 'Unknown rclone error'}")
          |     return [line.strip().rstrip(':') for line in stdout.splitlines() if line.strip()]
          | 
          | @sync_server.register_method("mcp.sync.listJobs")
          | async def handle_sync_list_jobs(server: MCPServer, request_id: str, params: List[Any]):
          |     # Checker thread updates statuses, this just reads from server.sync_jobs_state
          |     response = []
          |     for job_id, job_data in server.sync_jobs_state.items(): # type: ignore
          |         is_running = (job_id in server.rclone_processes_state and server.rclone_processes_state[job_id].poll() is None) # type: ignore
          |         entry = {
          |             "job_id": job_id,
          |             "description": job_data.get("description", "Ad-hoc job" if job_data.get("is_adhoc") else "N/A"),
          |             "source": job_data.get("source"), "destination": job_data.get("destination"),
          |             "status": "running" if is_running else job_data.get("status", "unknown"),
          |             "is_running": is_running,
          |             "start_time": job_data.get("start_time").isoformat() if job_data.get("start_time") else None,
          |             "end_time": job_data.get("end_time").isoformat() if job_data.get("end_time") else None,
          |             "pid": job_data.get("process_pid") # PID is present if running
          |         }
          |         response.append(entry)
          |     return response
          | 
          | @sync_server.register_method("mcp.sync.runJob")
          | async def handle_sync_run_job(server: MCPServer, request_id: str, params: List[Any]):
          |     job_spec = params[0] # Validated by schema
          |     job_id_prefix = job_spec.get("job_id_prefix", "adhoc")
          |     source = job_spec["source"]; destination = job_spec["destination"]
          |     rclone_args = job_spec.get("rclone_args", [])
          | 
          |     exec_job_id = f"{job_id_prefix}_{uuid.uuid4().hex[:8]}"
          |     
          |     pid, err_msg = await server.run_in_executor(
          |         _start_rclone_sync_proc_blocking, server, exec_job_id, source, destination, rclone_args
          |     )
          |     if pid is None: raise RuntimeError(f"Failed to start rclone job '{exec_job_id}': {err_msg}")
          |     return {"job_id": exec_job_id, "status": "started", "pid": pid, "message": f"Job '{exec_job_id}' started."}
          | 
          | @sync_server.register_method("mcp.sync.getJobStatus")
          | async def handle_sync_get_job_status(server: MCPServer, request_id: str, params: List[Any]):
          |     job_id = params[0]
          |     if job_id not in server.sync_jobs_state: raise ValueError(f"Job ID '{job_id}' not found.") # type: ignore
          | 
          |     job_data = server.sync_jobs_state[job_id] # type: ignore
          |     is_running = (job_id in server.rclone_processes_state and server.rclone_processes_state[job_id].poll() is None) # type: ignore
          |     status_msg = "running" if is_running else job_data.get("status", "unknown")
          | 
          |     log_preview = []
          |     log_file_path_str = job_data.get("log_file")
          |     if log_file_path_str and Path(log_file_path_str).exists():
          |         try: # Small IO, can be sync here or executor for extreme robustness
          |             with open(log_file_path_str, 'r', errors='ignore') as lf:
          |                 log_preview = [line.strip() for line in lf.readlines()[-20:]] # Last 20 lines
          |         except Exception as e: server.logger.warning(f"Could not read log for job {job_id}: {e}")
          |     
          |     return {"job_id": job_id, "is_running": is_running, "status_message": status_msg,
          |             "start_time": job_data.get("start_time").isoformat() if job_data.get("start_time") else None,
          |             "end_time": job_data.get("end_time").isoformat() if job_data.get("end_time") else None,
          |             "return_code": job_data.get("return_code"), "log_preview": log_preview}
          | 
          | @sync_server.register_method("mcp.sync.stopJob")
          | async def handle_sync_stop_job(server: MCPServer, request_id: str, params: List[Any]):
          |     job_id = params[0]
          |     if job_id not in server.rclone_processes_state or server.rclone_processes_state[job_id].poll() is not None: # type: ignore
          |         msg = f"Job '{job_id}' not running or not found."
          |         if job_id in server.sync_jobs_state: server.sync_jobs_state[job_id]["status"] = "unknown" # type: ignore # Or "not_running"
          |         return {"job_id": job_id, "status": "not_running", "message": msg}
          | 
          |     process_to_stop = server.rclone_processes_state[job_id] # type: ignore
          |     server.logger.info(f"Job {job_id}: Attempting to stop rclone process PID {process_to_stop.pid}.")
          |     try:
          |         # Sending signal is quick. The process termination is async.
          |         if os.name != 'nt': os.killpg(os.getpgid(process_to_stop.pid), signal.SIGTERM)
          |         else: process_to_stop.terminate()
          |         
          |         if job_id in server.sync_jobs_state: server.sync_jobs_state[job_id]["status"] = "stopping" # type: ignore
          |         # The checker thread will eventually update to completed/failed after process exits.
          |         return {"job_id": job_id, "status": "stopping_signal_sent",
          |                 "message": f"Sent SIGTERM to rclone job {job_id} (PID {process_to_stop.pid})."}
          |     except Exception as e: # ProcessLookupError if PID no longer exists
          |         server.logger.error(f"Job {job_id}: Failed to send stop signal (PID {process_to_stop.pid}): {e}", exc_info=True)
          |         raise RuntimeError(f"Failed to stop job {job_id}: {e}")
          | 
          | 
          | # --- Server Lifecycle Hooks for MCPServer ---
          | async def on_sync_server_startup_hook(server: MCPServer): # Renamed to avoid conflict if MCPServer has same name
          |     server.logger.info(f"Sync Server '{server.server_name}' custom startup actions...")
          |     server.job_check_thread_stop_event.clear() # type: ignore
          |     server.job_check_thread = threading.Thread( # type: ignore
          |         target=_job_process_checker_thread_target, args=(server,), daemon=True)
          |     server.job_check_thread.start() # type: ignore
          | 
          | async def on_sync_server_shutdown_hook(server: MCPServer):
          |     server.logger.info(f"Sync Server '{server.server_name}' custom shutdown actions...")
          |     server.job_check_thread_stop_event.set() # type: ignore
          |     if server.job_check_thread and server.job_check_thread.is_alive(): # type: ignore
          |         server.logger.info("Waiting for job checker thread to stop...")
          |         server.job_check_thread.join(timeout=7) # Give it a bit more time # type: ignore
          |         if server.job_check_thread.is_alive(): # type: ignore
          |             server.logger.warning("Job checker thread did not stop in time.")
          |     
          |     # Terminate any remaining rclone processes forcefully
          |     for job_id, process in list(server.rclone_processes_state.items()): # type: ignore
          |         if process.poll() is None: # If still running
          |             server.logger.warning(f"Job {job_id} (PID {process.pid}): Force terminating rclone process on shutdown.")
          |             try:
          |                 if os.name != 'nt': os.killpg(os.getpgid(process.pid), signal.SIGKILL)
          |                 else: process.kill()
          |                 process.wait(timeout=3) # Brief wait
          |             except Exception as e_term:
          |                 server.logger.error(f"Job {job_id}: Error force terminating rclone PID {process.pid}: {e_term}")
          | 
          | # Assign hooks to the server instance
          | sync_server.on_startup = on_sync_server_startup_hook # type: ignore
          | sync_server.on_shutdown = on_sync_server_shutdown_hook # type: ignore
          | 
          | 
          | # --- Main Entry Point ---
          | if __name__ == "__main__":
          |     script_logger = logging.getLogger("llmbasedos.servers.sync_script_main")
          |     # Basic config for this script's logger, MCPServer instance handles its own.
          |     log_level_main = os.getenv(f"LLMBDO_{SERVER_NAME.upper()}_LOG_LEVEL", "INFO").upper()
          |     script_logger.setLevel(log_level_main)
          |     if not script_logger.hasHandlers():
          |         ch = logging.StreamHandler()
          |         ch.setFormatter(logging.Formatter(f"%(asctime)s - SYNC MAIN - %(levelname)s - %(message)s"))
          |         script_logger.addHandler(ch)
          | 
          |     try:
          |         # MCPServer's start method will call on_startup and on_shutdown if they are set
          |         asyncio.run(sync_server.start())
          |     except KeyboardInterrupt:
          |         script_logger.info(f"Server '{SERVER_NAME}' (main) stopped by KeyboardInterrupt.")
          |     except Exception as e_main_sync:
          |         script_logger.critical(f"Sync Server (main) crashed: {e_main_sync}", exc_info=True)
          |     finally:
          |         script_logger.info(f"Sync Server (main) is shutting down...")
          |         # If asyncio.run() completed or was interrupted, the loop is no longer running.
          |         # MCPServer's own finally block in start() handles executor shutdown and socket cleanup.
          |         # on_shutdown hook for this server was already called by MCPServer.start()'s finally block
          |         # if it was successfully started and then shutdown (e.g. by CancelledError).
          |         # If startup itself failed, on_shutdown might not have run.
          |         # For robustness, ensure critical cleanup if thread was started but server.start() didn't run full cycle.
          |         if sync_server.job_check_thread and sync_server.job_check_thread.is_alive(): # type: ignore
          |             script_logger.warning("Job check thread still alive after server stop, attempting to stop it now.")
          |             sync_server.job_check_thread_stop_event.set() # type: ignore
          |             sync_server.job_check_thread.join(timeout=5) # type: ignore
          |         script_logger.info(f"Sync Server (main) fully shut down.")
          --- Fin Contenu ---

      Répertoire: ./llmbasedos_src/servers/mail
        Fichier: caps.json
          --- Début Contenu (ascii) ---
          | {
          |     "service_name": "mail",
          |     "description": "Interacts with email accounts via IMAP and iCalendar.",
          |     "version": "0.1.1",
          |     "capabilities": [
          |         {
          |             "method": "mcp.mail.listAccounts",
          |             "description": "Lists configured email accounts.",
          |             "params_schema": { "type": "array", "maxItems": 0 },
          |             "result_schema": {
          |                 "type": "array", "items": {
          |                     "type": "object", "properties": {
          |                         "account_id": {"type": "string"}, "email_address": {"type": "string"},
          |                         "type": {"type": "string", "enum": ["imap"]}
          |                     }, "required": ["account_id", "email_address", "type"]
          |                 }
          |             }
          |         },
          |         {
          |             "method": "mcp.mail.listFolders",
          |             "description": "Lists mail folders for an account.",
          |             "params_schema": { "type": "array", "minItems": 1, "maxItems": 1, "items": [{"type": "string", "description": "account_id"}]},
          |             "result_schema": {
          |                 "type": "array", "items": {
          |                     "type": "object", "properties": {
          |                         "name": {"type": "string"}, "path": {"type": "string"},
          |                         "flags": {"type": "array", "items": {"type": "string"}}
          |                     }, "required": ["name", "path", "flags"]
          |                 }
          |             }
          |         },
          |         {
          |             "method": "mcp.mail.listMessages",
          |             "description": "Lists messages in a folder.",
          |             "params_schema": {
          |                 "type": "array", "minItems": 2, "maxItems": 3, "items": [
          |                     {"type": "string", "description": "account_id"},
          |                     {"type": "string", "description": "folder_name/path"},
          |                     {"type": "object", "optional": true, "properties": {
          |                         "limit": {"type": "integer", "default": 25},
          |                         "search_criteria": {"type": "string", "default": "ALL", "description": "IMAP search criteria string."}
          |                     }}
          |                 ]
          |             },
          |             "result_schema": {
          |                 "type": "array", "items": {
          |                     "type": "object", "properties": {
          |                         "uid": {"type": "integer"}, "subject": {"type": "string"},
          |                         "from": {"type": "array", "items": {"type": "string"}},
          |                         "to": {"type": "array", "items": {"type": "string"}},
          |                         "date": {"type": ["string", "null"], "format": "date-time"},
          |                         "seen": {"type": "boolean"}, "has_attachments": {"type": "boolean"}
          |                     }, "required": ["uid", "subject", "from", "date", "seen", "has_attachments"]
          |                 }
          |             }
          |         },
          |         {
          |             "method": "mcp.mail.getMessage",
          |             "description": "Retrieves a specific message.",
          |             "params_schema": {
          |                 "type": "array", "minItems": 3, "maxItems": 4, "items": [
          |                     {"type": "string", "description": "account_id"},
          |                     {"type": "string", "description": "folder_name/path"},
          |                     {"type": "integer", "description": "message UID"},
          |                     {"type": "object", "optional": true, "properties": {
          |                         "body_preference": {"type": "array", "items": {"type": "string", "enum": ["text/plain", "text/html"]}, "default": ["text/plain", "text/html"]},
          |                         "fetch_attachments": {"type": "boolean", "default": false},
          |                         "max_attachment_size_inline_kb": {"type": "integer", "default": 1024, "description": "Max attachment size (KB) to include base64 encoded in response."}
          |                     }}
          |                 ]
          |             },
          |             "result_schema": {
          |                 "type": "object", "properties": {
          |                     "uid": {"type": "integer"}, "subject": {"type": "string"},
          |                     "from": {"type": "array", "items": {"type": "string"}},
          |                     "to": {"type": "array", "items": {"type": "string"}},
          |                     "cc": {"type": "array", "items": {"type": "string"}, "optional": true},
          |                     "date": {"type": ["string", "null"], "format": "date-time"},
          |                     "headers": {"type": "object", "additionalProperties": {"type": "string"}},
          |                     "body_plain": {"type": ["string", "null"]}, "body_html": {"type": ["string", "null"]},
          |                     "attachments": {"type": "array", "optional": true, "items": {
          |                         "type": "object", "properties": {
          |                             "filename": {"type": "string"}, "mime_type": {"type": "string"},
          |                             "size": {"type": "integer"}, "content_id": {"type": ["string", "null"]},
          |                             "content_base64": {"type": ["string", "null"], "description": "Base64 content or 'CONTENT_TOO_LARGE_OR_NOT_FETCHED'."}
          |                         }, "required": ["filename", "mime_type", "size"]
          |                     }}
          |                 }, "required": ["uid", "subject", "from", "to", "date", "headers"]
          |             }
          |         },
          |         {
          |             "method": "mcp.mail.parseIcalendar",
          |             "description": "Parses iCalendar data string.",
          |             "params_schema": { "type": "array", "minItems": 1, "maxItems": 1, "items": [{"type": "string", "description": "iCalendar data string."}]},
          |             "result_schema": {
          |                 "type": "array", "items": {
          |                     "type": "object", "description": "Calendar component details.",
          |                     "properties": { /* Same as before, detailed fields for VEVENT etc. */
          |                         "type": {"type": "string"}, "summary": {"type": ["string", "null"]},
          |                         "dtstart": {"type": ["string", "null"], "format": "date-time"},
          |                         "dtend": {"type": ["string", "null"], "format": "date-time"}
          |                         /* Add more relevant iCal fields as needed */
          |                     }
          |                 }
          |             }
          |         }
          |     ]
          | }
          --- Fin Contenu ---
        Fichier: mail_accounts.yaml
          --- Début Contenu (utf-8) ---
          | # llmbasedos/servers/mail/mail_accounts.yaml
          | # Chemin par défaut pour le développement si LLMBDO_MAIL_ACCOUNTS_CONFIG_PATH n'est pas défini.
          | # Pour Docker, ce fichier serait monté à /etc/llmbasedos/mail_accounts.yaml (ou autre chemin ENV).
          | 
          | accounts:
          |   perso_gmail:
          |     email: "mon.adresse@gmail.com"
          |     host: "imap.gmail.com"
          |     port: 993
          |     user: "mon.adresse@gmail.com"
          |     password: "VOTRE_MOT_DE_PASSE_APPLICATION_GMAIL" # Important: utiliser un mot de passe d'application pour Gmail
          |     ssl: true
          |     starttls: false
          |     auth_type: "password" # Gmail avec mot de passe d'application
          | 
          |   pro_outlook:
          |     email: "mon.adresse.pro@outlook.com"
          |     host: "outlook.office365.com"
          |     port: 993
          |     user: "mon.adresse.pro@outlook.com"
          |     password: "VOTRE_MOT_DE_PASSE_PRO"
          |     ssl: true
          |     starttls: false
          |     auth_type: "password"
          |     # Pour OAuth2 avec Microsoft, il faudrait ajouter:
          |     # auth_type: "oauth2"
          |     # client_id: "..."
          |     # tenant_id: "..."
          |     # etc. (la logique OAuth2 n'est pas implémentée dans le code Python fourni)
          --- Fin Contenu ---
        Fichier: requirements.txt
          --- Début Contenu (ascii) ---
          | # llmbasedos/servers/mail/requirements.txt
          | imapclient>=2.1.0
          | icalendar>=5.0.0
          | PyYAML>=6.0 # For loading account configurations from YAML
          | # google-auth-oauthlib>=0.4.0 (if using Google OAuth2)
          | # msal>=1.10.0 (if using Microsoft OAuth2)
          --- Fin Contenu ---
        Fichier: server.py
          --- Début Contenu (utf-8) ---
          | # llmbasedos/servers/mail/server.py
          | import asyncio
          | import logging # Logger obtained from MCPServer
          | import os
          | from pathlib import Path
          | from typing import Any, Dict, List, Optional, Tuple, Union
          | from email.parser import BytesParser # Standard library
          | from email.header import decode_header, make_header # Standard library
          | from email.utils import parseaddr, parsedate_to_datetime, getaddresses # Standard library
          | from datetime import datetime, timezone # Standard library
          | import base64 # Standard library
          | 
          | from imapclient import IMAPClient # External dep
          | from imapclient.exceptions import IMAPClientError, LoginError # External dep
          | from icalendar import Calendar # External dep
          | import yaml # External dep for account config
          | from llmbasedos.mcp_server_framework import MCPServer 
          | from llmbasedos.common_utils import validate_mcp_path_param # Assurez-vous que fs_server en a besoin
          | 
          | # --- Import Framework ---
          | # Supposons que ce framework est dans le PYTHONPATH
          | # from llmbasedos.mcp_server_framework import MCPServer
          | # Pour le rendre exécutable en standalone pour le moment, je vais simuler MCPServer
          | # Dans votre projet réel, vous utiliserez votre import.
          | if __name__ == '__main__': # Simulation pour exécution directe
          |     class MCPServer: # Minimal mock for standalone testing
          |         def __init__(self, server_name, caps_file_path_str, custom_error_code_base=0):
          |             self.server_name = server_name
          |             self.caps_file_path = Path(caps_file_path_str)
          |             self.custom_error_code_base = custom_error_code_base
          |             self.logger = logging.getLogger(f"llmbasedos.servers.{server_name}") # Mock logger
          |             self.logger.setLevel(os.getenv(f"LLMBDO_{server_name.upper()}_LOG_LEVEL", "INFO").upper())
          |             if not self.logger.hasHandlers():
          |                 ch = logging.StreamHandler()
          |                 ch.setFormatter(logging.Formatter(f"%(asctime)s - %(name)s - %(levelname)s - %(message)s"))
          |                 self.logger.addHandler(ch)
          |             
          |             self.executor = None # Placeholder
          |             self.on_startup = None
          |             self.on_shutdown = None
          |             self._handlers = {}
          | 
          |         def register(self, method_name):
          |             def decorator(func):
          |                 self._handlers[method_name] = func
          |                 return func
          |             return decorator
          | 
          |         async def run_in_executor(self, func, *args):
          |             # In a real scenario, this would use a ThreadPoolExecutor
          |             loop = asyncio.get_running_loop()
          |             return await loop.run_in_executor(self.executor, func, *args) # executor would be properly initialized
          | 
          |         async def start(self):
          |             # Mock start method
          |             if self.on_startup: await self.on_startup(self)
          |             self.logger.info(f"Mock MCPServer '{self.server_name}' started. Socket path would be used here.")
          |             # Simulate running forever (e.g. handling connections)
          |             try:
          |                 while True: await asyncio.sleep(3600) # Sleep for a long time
          |             except asyncio.CancelledError:
          |                 self.logger.info(f"Mock MCPServer '{self.server_name}' cancelled.")
          |             finally:
          |                 if self.on_shutdown: await self.on_shutdown(self)
          |                 self.logger.info(f"Mock MCPServer '{self.server_name}' shut down.")
          |     # Fin de la simulation MCPServer
          | else:
          |     from llmbasedos.mcp_server_framework import MCPServer
          | 
          | 
          | # --- Server Specific Configuration ---
          | SERVER_NAME = "mail"
          | CAPS_FILE_PATH_STR = str(Path(__file__).parent / "caps.json") # Relative to this file
          | 
          | # --- Configuration via Environment Variables with Defaults ---
          | # Chemin du fichier de configuration des comptes mail
          | MAIL_ACCOUNTS_CONFIG_FILE_STR: str = os.getenv(
          |     "LLMBDO_MAIL_ACCOUNTS_CONFIG_PATH",  # Variable d'environnement pour Docker/ déploiement
          |     str(Path(__file__).parent / "mail_accounts.yaml") # Fallback local pour dev (à côté de server.py)
          | )
          | MAIL_ACCOUNTS_CONFIG_FILE_PATH = Path(MAIL_ACCOUNTS_CONFIG_FILE_STR)
          | 
          | # Codes d'erreur personnalisés (pas besoin de modifier si MCPServer les gère)
          | MAIL_CUSTOM_ERROR_BASE = -32030
          | MAIL_AUTH_ERROR_CODE = MAIL_CUSTOM_ERROR_BASE - 1
          | 
          | 
          | # Initialize server instance (utilisera le logger configuré par MCPServer)
          | mail_server = MCPServer(SERVER_NAME, CAPS_FILE_PATH_STR, custom_error_code_base=MAIL_CUSTOM_ERROR_BASE)
          | 
          | # Attach server-specific state (sera peuplé par on_mail_server_startup_hook)
          | mail_server.mail_accounts: Dict[str, Dict[str, Any]] = {} # type: ignore
          | 
          | 
          | # --- Helper functions (utilisent mail_server.logger) ---
          | def _decode_email_header_str(header_value: Union[str, bytes, None]) -> str:
          |     if header_value is None: return ""
          |     try:
          |         decoded_header = make_header(decode_header(header_value))
          |         return str(decoded_header)
          |     except Exception as e:
          |         mail_server.logger.warning(f"Could not fully decode header: '{str(header_value)[:50]}...': {e}")
          |         if isinstance(header_value, bytes): return header_value.decode('latin-1', errors='replace')
          |         return str(header_value)
          | 
          | def _parse_address_list_str(header_value: str) -> List[str]:
          |     parsed_addrs = []
          |     for realname, email_address in getaddresses([header_value]):
          |         if email_address:
          |             if realname: parsed_addrs.append(f"{_decode_email_header_str(realname)} <{email_address}>")
          |             else: parsed_addrs.append(email_address)
          |     return parsed_addrs
          | 
          | 
          | # --- IMAP Client Context Manager ---
          | class IMAPConnection:
          |     def __init__(self, server: MCPServer, account_id: str):
          |         self.server = server
          |         self.account_id = account_id
          |         self.client: Optional[IMAPClient] = None
          |         self.acc_conf = server.mail_accounts.get(account_id) # type: ignore
          | 
          |     def __enter__(self) -> IMAPClient: # Renvoie IMAPClient, pas self
          |         if not self.acc_conf:
          |             self.server.logger.error(f"IMAP config not found for account ID '{self.account_id}'.")
          |             raise ValueError(f"Account ID '{self.account_id}' configuration not found.") # Internal error or bad param
          |         
          |         host = self.acc_conf.get("host")
          |         port = int(self.acc_conf.get("port", 993 if self.acc_conf.get("ssl", True) else 143))
          |         user = self.acc_conf.get("user")
          |         password = self.acc_conf.get("password") # WARNING: Still plaintext password handling
          |         use_ssl = self.acc_conf.get("ssl", True)
          |         use_starttls = self.acc_conf.get("starttls", False)
          |         # Timeout pour les opérations socket IMAP, configurable via ENV
          |         imap_timeout_sec = int(os.getenv("LLMBDO_MAIL_IMAP_TIMEOUT_SEC", "30"))
          | 
          | 
          |         if not all([host, user, password]):
          |             self.server.logger.error(f"Incomplete IMAP config for account '{self.account_id}'. Missing host, user, or password.")
          |             raise ValueError(f"Incomplete IMAP config for account '{self.account_id}'.")
          | 
          |         try:
          |             self.server.logger.debug(f"IMAP: Connecting to {host}:{port} for {self.account_id} (SSL: {use_ssl}, STARTTLS: {use_starttls}, Timeout: {imap_timeout_sec}s)")
          |             self.client = IMAPClient(host=host, port=port, ssl=use_ssl, timeout=imap_timeout_sec)
          |             
          |             # STARTTLS should only be attempted if not already using SSL and if enabled
          |             if use_starttls and not use_ssl:
          |                 self.server.logger.debug(f"IMAP: Attempting STARTTLS for {self.account_id}")
          |                 self.client.starttls() # This upgrades the connection to TLS
          | 
          |             # TODO: Implement OAuth2 logic based on self.acc_conf.get("auth_type")
          |             # if self.acc_conf.get("auth_type") == "oauth2":
          |             #     # token = get_oauth_token_for_account(self.account_id)
          |             #     # self.client.oauth2_login(user, token)
          |             #     pass
          |             # else:
          |             self.server.logger.debug(f"IMAP: Logging in as '{user}' for account '{self.account_id}'")
          |             self.client.login(user, password)
          |             
          |             self.server.logger.info(f"IMAP: Login successful for account '{self.account_id}'.")
          |             return self.client
          |         except LoginError as e:
          |             self.server.logger.error(f"IMAP login failed for {self.account_id} on {host}: {e}")
          |             # Important: Ne pas exposer les détails de l'erreur de login au client final pour la sécurité.
          |             raise ConnectionRefusedError(f"Authentication failed for mail account '{self.account_id}'.")
          |         except IMAPClientError as e: # Includes socket errors, timeouts during connection
          |             self.server.logger.error(f"IMAP client error for {self.account_id} on {host}: {e}", exc_info=True)
          |             raise ConnectionError(f"IMAP connection error for account '{self.account_id}'. Details: {type(e).__name__}")
          |         except Exception as e:
          |             self.server.logger.error(f"Unexpected error during IMAP connect/login for {self.account_id}: {e}", exc_info=True)
          |             raise ConnectionError(f"Unexpected IMAP error for account '{self.account_id}'.")
          | 
          |     def __exit__(self, exc_type, exc_val, exc_tb):
          |         if self.client:
          |             try:
          |                 self.server.logger.debug(f"IMAP: Attempting logout for {self.account_id}")
          |                 self.client.logout()
          |                 self.server.logger.info(f"IMAP: Logged out successfully for {self.account_id}")
          |             except IMAPClientError as e:
          |                 self.server.logger.warning(f"IMAP error during logout for {self.account_id}: {e}. Connection might have been already closed.")
          |             except Exception as e_logout: # Other unexpected errors
          |                  self.server.logger.error(f"Unexpected error during IMAP logout for {self.account_id}: {e_logout}", exc_info=True)
          |             finally: # Ensure client is None regardless of logout success/failure
          |                 self.client = None
          | 
          | # --- Mail Capability Handlers ---
          | @mail_server.register("mcp.mail.listAccounts")
          | async def handle_mail_list_accounts(server: MCPServer, request_id: str, params: List[Any]):
          |     # Validation des paramètres (si nécessaire) est gérée par le framework MCPServer via caps.json
          |     if not server.mail_accounts: # type: ignore
          |         server.logger.info("mcp.mail.listAccounts: No mail accounts configured or loaded.")
          |         return []
          |     
          |     return [
          |         {"account_id": acc_id, "email_address": conf.get("email", conf.get("user")), "type": "imap"}
          |         for acc_id, conf in server.mail_accounts.items() # type: ignore
          |     ]
          | 
          | @mail_server.register("mcp.mail.listFolders")
          | async def handle_mail_list_folders(server: MCPServer, request_id: str, params: List[Any]):
          |     # `params` est déjà validé par le framework MCPServer contre le `params_schema` de caps.json
          |     account_id = params[0]
          |     if account_id not in server.mail_accounts: # type: ignore
          |         server.logger.warning(f"mcp.mail.listFolders: Account ID '{account_id}' not found in server configuration.")
          |         # Le framework devrait retourner une erreur basée sur le schema ou nous levons une ValueError
          |         raise ValueError(f"Account ID '{account_id}' not configured on this server.")
          | 
          |     def list_folders_sync_blocking_op(): # Renommé pour clarifier que c'est l'opération bloquante
          |         with IMAPConnection(server, account_id) as client:
          |             folders_raw = client.list_folders()
          |             return [{"name": name_bytes.decode('utf-8', 'surrogateescape'), 
          |                      "path": name_bytes.decode('utf-8', 'surrogateescape'),
          |                      "flags": [f.decode('ascii', 'ignore') for f in flags_tuple]} 
          |                     for flags_tuple, _, name_bytes in folders_raw]
          |     try:
          |         return await server.run_in_executor(list_folders_sync_blocking_op)
          |     except (ConnectionRefusedError, ConnectionError) as conn_e: # Erreurs levées par IMAPConnection
          |         # Ces erreurs sont spécifiques à la connexion et à l'authentification.
          |         # Elles devraient être mappées à une erreur MCP appropriée.
          |         # Le framework MCPServer pourrait avoir un mécanisme pour cela, ou nous utilisons MAIL_AUTH_ERROR_CODE.
          |         raise mail_server.create_custom_error(
          |             MAIL_AUTH_ERROR_CODE,
          |             str(conn_e), # Message de l'erreur de connexion
          |             data={"account_id": account_id}
          |         ) from conn_e
          |     except IMAPClientError as imap_e: # Autres erreurs IMAP après connexion
          |         server.logger.error(f"IMAPClientError in listFolders for {account_id}: {imap_e}", exc_info=True)
          |         raise RuntimeError(f"Server error while listing folders for account '{account_id}'.") from imap_e
          | 
          | 
          | @mail_server.register("mcp.mail.listMessages")
          | async def handle_mail_list_messages(server: MCPServer, request_id: str, params: List[Any]):
          |     account_id, folder_name_str = params[0], params[1]
          |     options = params[2] if len(params) > 2 else {}
          |     if account_id not in server.mail_accounts: raise ValueError(f"Account ID '{account_id}' not found.") # type: ignore
          |     
          |     limit = options.get("limit", 25)
          |     search_criteria_str = options.get("search_criteria", "ALL")
          | 
          |     def list_messages_sync_blocking_op():
          |         with IMAPConnection(server, account_id) as client:
          |             # IMAPClient recommande d'utiliser des chaînes str pour select_folder, il gère l'encodage.
          |             server.logger.debug(f"IMAP: Selecting folder '{folder_name_str}' for account {account_id}")
          |             select_info = client.select_folder(folder_name_str, readonly=True) # Readonly pour list
          |             server.logger.debug(f"IMAP: Folder select_info: {select_info}")
          | 
          |             # Utiliser UID SEARCH. La criteria doit être en bytes pour certains serveurs, ou str avec charset.
          |             # IMAPClient essaie de gérer cela. Si erreur, essayer .encode('utf-8').
          |             server.logger.debug(f"IMAP: Searching messages with criteria '{search_criteria_str}' (UID mode)")
          |             try:
          |                 message_uids_bytes_list = client.search(criteria=search_criteria_str, charset='UTF-8', uid=True)
          |             except IMAPClientError as search_err: # Si le charset pose problème
          |                 server.logger.warning(f"IMAP search with UTF-8 charset failed for '{search_criteria_str}', trying raw bytes: {search_err}")
          |                 message_uids_bytes_list = client.search(criteria=search_criteria_str.encode('utf-8', 'surrogateescape'), uid=True)
          | 
          |             message_uids = sorted([int(uid_b) for uid_b in message_uids_bytes_list], reverse=True)
          |             server.logger.debug(f"IMAP: Found {len(message_uids)} UIDs, fetching first {limit}")
          |             
          |             uids_to_fetch = message_uids[:limit]
          |             if not uids_to_fetch: return []
          | 
          |             fetch_items = ['UID', 'ENVELOPE', 'FLAGS', 'BODYSTRUCTURE', 'INTERNALDATE']
          |             raw_fetch_data = client.fetch(uids_to_fetch, fetch_items, uid=True)
          |             
          |             messages_result_list = []
          |             for uid_bytes_key, data_dict in raw_fetch_data.items():
          |                 uid = int(uid_bytes_key)
          |                 env = data_dict.get(b'ENVELOPE')
          |                 flags_bytes_tuple = data_dict.get(b'FLAGS', tuple()) # FLAGS est un tuple de bytes
          |                 bs = data_dict.get(b'BODYSTRUCTURE')
          |                 internal_date_dt = data_dict.get(b'INTERNALDATE')
          |                 if not env: continue
          |                 
          |                 has_attach = False # Simplification
          |                 if bs:
          |                     try: # Check for common indicators of attachments in BODYSTRUCTURE
          |                         bs_str_repr = str(bs).lower() # Convert complex structure to string for simple check
          |                         if '("attachment"' in bs_str_repr or '("filename"' in bs_str_repr or \
          |                            '("name"' in bs_str_repr and 'text/' not in bs_str_repr: # Avoid matching name on text parts
          |                             has_attach = True
          |                     except: pass
          | 
          |                 msg_date_to_use = env.date or internal_date_dt
          |                 
          |                 # From/To can be None or empty list in ENVELOPE
          |                 from_addrs = env.from_ if env.from_ else []
          |                 to_addrs = env.to if env.to else []
          | 
          |                 messages_result_list.append({
          |                     "uid": uid,
          |                     "subject": _decode_email_header_str(env.subject),
          |                     "from": [_decode_email_header_str(addr.addr_spec) for addr in from_addrs if addr and hasattr(addr, 'addr_spec')],
          |                     "to": [_decode_email_header_str(addr.addr_spec) for addr in to_addrs if addr and hasattr(addr, 'addr_spec')],
          |                     "date": msg_date_to_use.astimezone(timezone.utc).isoformat() if msg_date_to_use else None,
          |                     "seen": b'\\Seen' in flags_bytes_tuple,
          |                     "has_attachments": has_attach
          |                 })
          |             messages_result_list.sort(key=lambda m: m['uid'], reverse=True)
          |             return messages_result_list
          | 
          |     try: return await server.run_in_executor(list_messages_sync_blocking_op)
          |     except (ConnectionRefusedError, ConnectionError) as conn_e:
          |         raise mail_server.create_custom_error(MAIL_AUTH_ERROR_CODE, str(conn_e), data={"account_id": account_id})
          |     except ValueError as ve: # e.g. account not found, folder not found (from select_folder)
          |         server.logger.warning(f"ValueError in listMessages for {account_id}/{folder_name_str}: {ve}")
          |         raise mail_server.create_custom_error(server.ERROR_INVALID_PARAMS, str(ve), data={"folder": folder_name_str})
          |     except IMAPClientError as imap_e:
          |         server.logger.error(f"IMAPClientError in listMessages for {account_id}/{folder_name_str}: {imap_e}", exc_info=True)
          |         raise RuntimeError(f"Server error listing messages for '{folder_name_str}'.")
          | 
          | 
          | @mail_server.register("mcp.mail.getMessage")
          | async def handle_mail_get_message(server: MCPServer, request_id: str, params: List[Any]):
          |     account_id, folder_name_str, msg_uid_int = params[0], params[1], params[2]
          |     options = params[3] if len(params) > 3 else {}
          |     if account_id not in server.mail_accounts: raise ValueError(f"Account ID '{account_id}' not found.") # type: ignore
          | 
          |     body_pref_list = options.get("body_preference", ["text/plain", "text/html"])
          |     fetch_attach_flag = options.get("fetch_attachments", False)
          |     max_attach_kb = int(os.getenv("LLMBDO_MAIL_MAX_ATTACH_INLINE_KB", "1024")) # Configurable max size for inline base64 attachment
          |     max_attach_bytes = max_attach_kb * 1024
          | 
          |     def get_message_sync_blocking_op():
          |         with IMAPConnection(server, account_id) as client:
          |             server.logger.debug(f"IMAP: Selecting folder '{folder_name_str}' for getMessage UID {msg_uid_int}")
          |             client.select_folder(folder_name_str) # No readonly=True for get, might involve implicit \Seen flag set by server
          |             
          |             raw_msg_fetch_data = client.fetch([msg_uid_int], ['UID', b'RFC822'], uid=True)
          |             
          |             message_content_bytes = None
          |             for key_uid_bytes, data_dict in raw_msg_fetch_data.items():
          |                 if int(key_uid_bytes) == msg_uid_int:
          |                     message_content_bytes = data_dict.get(b'RFC822'); break
          |             if not message_content_bytes:
          |                 raise ValueError(f"Message UID {msg_uid_int} not found or content missing in '{folder_name_str}'.")
          | 
          |             email_msg_obj = BytesParser().parsebytes(message_content_bytes)
          |             headers_dict = {key: _decode_email_header_str(value) for key, value in email_msg_obj.items()}
          |             body_plain_str, body_html_str, attachments_list_res = None, None, []
          | 
          |             # Iterate MIME parts: first collect preferred, then fallbacks
          |             # Pass 1: Collect preferred body types
          |             for part in email_msg_obj.walk():
          |                 if part.is_multipart(): continue # Skip multipart containers themselves
          |                 
          |                 content_type_main = part.get_content_type().lower()
          |                 content_disposition_str = str(part.get("Content-Disposition", "")).lower()
          | 
          |                 is_attachment_like = "attachment" in content_disposition_str or part.get_filename() is not None
          | 
          |                 if not is_attachment_like: # Potential body part
          |                     if content_type_main == "text/plain" and "text/plain" in body_pref_list and body_plain_str is None:
          |                         payload_b = part.get_payload(decode=True) or b''
          |                         charset = part.get_content_charset() or 'utf-8'
          |                         body_plain_str = payload_b.decode(charset, errors='replace')
          |                     elif content_type_main == "text/html" and "text/html" in body_pref_list and body_html_str is None:
          |                         payload_b = part.get_payload(decode=True) or b''
          |                         charset = part.get_content_charset() or 'utf-8'
          |                         body_html_str = payload_b.decode(charset, errors='replace')
          |             
          |             # Pass 2: Collect attachments and fallback body types if preferred not found
          |             for part in email_msg_obj.walk():
          |                 if part.is_multipart(): continue
          | 
          |                 content_type_main = part.get_content_type().lower()
          |                 content_disposition_str = str(part.get("Content-Disposition", "")).lower()
          |                 is_attachment_like = "attachment" in content_disposition_str or part.get_filename() is not None
          | 
          |                 if is_attachment_like:
          |                     filename_decoded = _decode_email_header_str(part.get_filename() or f"attachment_{len(attachments_list_res)+1}")
          |                     payload_bytes_att = part.get_payload(decode=True) or b''
          |                     att_data = {"filename": filename_decoded, "mime_type": content_type_main,
          |                                 "size": len(payload_bytes_att), "content_id": part.get("Content-ID")}
          |                     if fetch_attach_flag:
          |                         if len(payload_bytes_att) <= max_attach_bytes:
          |                             att_data["content_base64"] = base64.b64encode(payload_bytes_att).decode('ascii')
          |                         else: att_data["content_base64"] = f"CONTENT_SKIPPED_TOO_LARGE (>{max_attach_kb}KB)"
          |                     attachments_list_res.append(att_data)
          |                 else: # Fallback for body parts if not found in pass 1
          |                     if content_type_main == "text/plain" and body_plain_str is None:
          |                          body_plain_str = (part.get_payload(decode=True)or b'').decode(part.get_content_charset() or 'utf-8', 'replace')
          |                     elif content_type_main == "text/html" and body_html_str is None:
          |                          body_html_str = (part.get_payload(decode=True)or b'').decode(part.get_content_charset() or 'utf-8', 'replace')
          |             
          |             msg_date_hdr = headers_dict.get("Date"); parsed_dt_iso = None
          |             if msg_date_hdr:
          |                 try: parsed_dt_iso = parsedate_to_datetime(msg_date_hdr).astimezone(timezone.utc).isoformat()
          |                 except: server.logger.debug(f"Could not parse date header '{msg_date_hdr}' for UID {msg_uid_int}")
          |             
          |             return {
          |                 "uid": msg_uid_int, "subject": headers_dict.get("Subject", ""),
          |                 "from": _parse_address_list_str(headers_dict.get("From", "")),
          |                 "to": _parse_address_list_str(headers_dict.get("To", "")),
          |                 "cc": _parse_address_list_str(headers_dict.get("Cc", "")),
          |                 "date": parsed_dt_iso, "headers": headers_dict,
          |                 "body_plain": body_plain_str, "body_html": body_html_str,
          |                 "attachments": attachments_list_res if attachments_list_res else None
          |             }
          |     try: return await server.run_in_executor(get_message_sync_blocking_op)
          |     except (ConnectionRefusedError, ConnectionError) as conn_e:
          |         raise mail_server.create_custom_error(MAIL_AUTH_ERROR_CODE, str(conn_e), data={"account_id": account_id})
          |     except ValueError as ve: # e.g. UID not found by handler
          |         server.logger.warning(f"ValueError in getMessage for {account_id}/{folder_name_str}/{msg_uid_int}: {ve}")
          |         raise mail_server.create_custom_error(server.ERROR_RESOURCE_NOT_FOUND, str(ve), data={"uid": msg_uid_int})
          |     except IMAPClientError as imap_e:
          |         server.logger.error(f"IMAPClientError in getMessage for {account_id}/{folder_name_str}/{msg_uid_int}: {imap_e}", exc_info=True)
          |         raise RuntimeError(f"Server error getting message UID {msg_uid_int}.")
          | 
          | 
          | @mail_server.register("mcp.mail.parseIcalendar")
          | async def handle_mail_parse_icalendar(server: MCPServer, request_id: str, params: List[Any]):
          |     ical_data_str = params[0]
          |     # Optional context params: account_id, message_uid, attachment_filename (params[1] to params[3])
          |     # Not used in this parser, but could be logged or used for context if needed.
          | 
          |     def parse_ical_sync_blocking_op():
          |         try:
          |             cal: Calendar = Calendar.from_ical(ical_data_str) # type: ignore
          |             parsed_components = []
          |             for component in cal.walk():
          |                 comp_name_str = str(component.name) # Ensure it's a string
          |                 comp_data: Dict[str, Any] = {"type": comp_name_str}
          |                 if comp_name_str in ["VEVENT", "VTODO", "VJOURNAL"]:
          |                     for prop_name_bstr in component: # Iterate over property names (often bytes in icalendar)
          |                         prop_name_str = str(prop_name_bstr).lower() # Normalize to lower string
          |                         prop_val = component.get(prop_name_bstr)
          |                         
          |                         if prop_val is None: continue
          | 
          |                         if prop_name_str in ["summary", "location", "description", "uid", "status", "priority", "url", "categories", "class"]:
          |                             comp_data[prop_name_str] = str(prop_val) if not isinstance(prop_val, list) else [_decode_email_header_str(v) for v in prop_val]
          |                         elif prop_name_str in ["dtstart", "dtend", "created", "last-modified", "dtstamp", "due", "completed", "exdate", "rdate"]:
          |                             # Handle VDDDTypes (can be date or datetime)
          |                             if hasattr(prop_val, 'dt'): # It's a vDDDLists or vDDDTypes object
          |                                 dt_obj = prop_val.dt
          |                                 if isinstance(dt_obj, list): # For EXDATE/RDATE which can have multiple values
          |                                     comp_data[prop_name_str] = []
          |                                     for d_item in dt_obj:
          |                                         if isinstance(d_item, datetime) and d_item.tzinfo is None:
          |                                             d_item = d_item.replace(tzinfo=timezone.utc)
          |                                         comp_data[prop_name_str].append(d_item.isoformat())
          |                                 else: # Single datetime/date
          |                                     if isinstance(dt_obj, datetime) and dt_obj.tzinfo is None:
          |                                         dt_obj = dt_obj.replace(tzinfo=timezone.utc)
          |                                     comp_data[prop_name_str] = dt_obj.isoformat()
          |                             else: # If it's already a string or other simple type
          |                                 comp_data[prop_name_str] = str(prop_val)
          |                         elif prop_name_str == "duration":
          |                             comp_data["duration"] = str(prop_val.to_ical().decode())
          |                         elif prop_name_str == "organizer":
          |                             comp_data["organizer"] = prop_val.params.get('CN', prop_val.to_ical().decode().replace('MAILTO:', ''))
          |                         elif prop_name_str == "attendee":
          |                             if "attendees" not in comp_data: comp_data["attendees"] = []
          |                             attendee_str = prop_val.params.get('CN', prop_val.to_ical().decode().replace('MAILTO:', ''))
          |                             comp_data["attendees"].append(attendee_str)
          |                         # Add more specific property parsers as needed (RRULE, GEO, etc.)
          | 
          |                 if comp_name_str != "VCALENDAR":
          |                     parsed_components.append(comp_data)
          |             return parsed_components
          |         except Exception as e_ical:
          |             server.logger.error(f"Error parsing iCalendar data: {e_ical}", exc_info=True)
          |             # Raise specific error type that framework can map to MCP error
          |             raise ValueError(f"Failed to parse iCalendar data: {str(e_ical)[:100]}")
          | 
          |     return await server.run_in_executor(parse_ical_sync_blocking_op)
          | 
          | 
          | # --- Server Lifecycle Hooks ---
          | async def on_mail_server_startup_hook(server: MCPServer):
          |     server.logger.info(f"Mail Server '{server.server_name}' custom startup: Loading mail accounts...")
          |     
          |     # This function is sync, so needs to be run in executor if it does I/O
          |     # For YAML loading, it's usually fast enough, but good practice for consistency
          |     def _load_config_sync():
          |         # Clear previous configs if any (e.g., on reload if framework supports it)
          |         server.mail_accounts.clear() # type: ignore
          | 
          |         if not MAIL_ACCOUNTS_CONFIG_FILE_PATH.exists():
          |             server.logger.warning(f"Mail accounts config file not found: {MAIL_ACCOUNTS_CONFIG_FILE_PATH}. No accounts will be available.")
          |             return
          | 
          |         try:
          |             with MAIL_ACCOUNTS_CONFIG_FILE_PATH.open('r') as f:
          |                 loaded_config_yaml = yaml.safe_load(f)
          |             
          |             if not isinstance(loaded_config_yaml, dict) or "accounts" not in loaded_config_yaml:
          |                 server.logger.error(f"Invalid format in {MAIL_ACCOUNTS_CONFIG_FILE_PATH}: Must be a dictionary with a top-level 'accounts' key.")
          |                 return
          | 
          |             accounts_dict_from_yaml = loaded_config_yaml["accounts"]
          |             if not isinstance(accounts_dict_from_yaml, dict):
          |                 server.logger.error(f"'accounts' key in {MAIL_ACCOUNTS_CONFIG_FILE_PATH} does not contain a dictionary.")
          |                 return
          | 
          |             # Basic validation and storing
          |             valid_accounts_loaded = 0
          |             for acc_id, conf_dict in accounts_dict_from_yaml.items():
          |                 if not isinstance(conf_dict, dict):
          |                     server.logger.warning(f"Account '{acc_id}' in config has invalid format (not a dict). Skipping.")
          |                     continue
          |                 if not all(k in conf_dict for k in ["host", "user", "password"]):
          |                     server.logger.warning(f"Account '{acc_id}' is missing required fields (host, user, password). Skipping.")
          |                     continue
          |                 
          |                 # Set defaults if not present
          |                 conf_dict.setdefault("ssl", True) # Default to SSL
          |                 conf_dict.setdefault("port", 993 if conf_dict["ssl"] else 143)
          |                 conf_dict.setdefault("email", conf_dict["user"]) # Email defaults to username
          |                 conf_dict.setdefault("starttls", False)
          |                 conf_dict.setdefault("auth_type", "password") # Default auth type
          | 
          |                 server.mail_accounts[acc_id] = conf_dict # type: ignore
          |                 valid_accounts_loaded += 1
          |             
          |             server.logger.info(f"Successfully loaded {valid_accounts_loaded} mail account(s) from {MAIL_ACCOUNTS_CONFIG_FILE_PATH}.")
          | 
          |         except yaml.YAMLError as ye:
          |             server.logger.error(f"Error parsing YAML from {MAIL_ACCOUNTS_CONFIG_FILE_PATH}: {ye}", exc_info=True)
          |         except Exception as e:
          |             server.logger.error(f"Unexpected error loading mail accounts config: {e}", exc_info=True)
          | 
          |     await server.run_in_executor(_load_config_sync)
          | 
          | 
          | async def on_mail_server_shutdown_hook(server: MCPServer):
          |     server.logger.info(f"Mail Server '{server.server_name}' custom shutdown hook called.")
          |     # No specific global resources to release for IMAP usually, connections are per-request.
          | 
          | mail_server.on_startup = on_mail_server_startup_hook # type: ignore
          | mail_server.on_shutdown = on_mail_server_shutdown_hook # type: ignore
          | 
          | # --- Main Entry Point (if run directly, for MCPServer framework) ---
          | if __name__ == "__main__":
          |     # Setup basic logging if MCPServer doesn't do it early enough for this script's messages
          |     if not mail_server.logger.hasHandlers():
          |         _sh = logging.StreamHandler()
          |         _sh.setFormatter(logging.Formatter(f"%(asctime)s - {mail_server.server_name} (main) - %(levelname)s - %(message)s"))
          |         mail_server.logger.addHandler(_sh)
          |         mail_server.logger.setLevel(os.getenv(f"LLMBDO_{SERVER_NAME.upper()}_LOG_LEVEL", "INFO").upper())
          |     
          |     mail_server.logger.info(f"Starting Mail Server '{SERVER_NAME}' directly via __main__...")
          |     try:
          |         asyncio.run(mail_server.start()) # MCPServer should handle its own socket creation and loop
          |     except KeyboardInterrupt:
          |         mail_server.logger.info(f"Mail Server '{SERVER_NAME}' (main) stopped by KeyboardInterrupt.")
          |     except Exception as e_main:
          |         mail_server.logger.critical(f"Mail Server '{SERVER_NAME}' (main) crashed: {e_main}", exc_info=True)
          |     finally:
          |         mail_server.logger.info(f"Mail Server '{SERVER_NAME}' (main) exiting.")
          --- Fin Contenu ---

  Répertoire: ./iso
    Fichier: build.sh
      --- Début Contenu (ascii) ---
      | #!/usr/bin/env bash
      | 
      | set -eo pipefail # Exit on error, treat unset variables as an error, and propagate exit status
      | 
      | # --- Configuration ---
      | PROFILE_DIR_ABS="$(cd "$(dirname "${BASH_SOURCE[0]}")" &>/dev/null && pwd)" # Absolute path to this script's dir (iso/)
      | WORK_DIR_ABS="${PROFILE_DIR_ABS}/work"   # Archiso working directory
      | OUT_DIR_ABS="${PROFILE_DIR_ABS}/out"    # Output directory for ISO
      | REPO_ROOT_ABS="$(cd "${PROFILE_DIR_ABS}/../.." &>/dev/null && pwd)" # Root of llmbasedos repo
      | 
      | LLMBasedOS_APP_TARGET_DIR_NAME="llmbasedos" # Name of the app dir inside /opt
      | 
      | # Verbosity for mkarchiso (-v)
      | MKARCHISO_OPTS="-v" # Add other mkarchiso options if needed
      | 
      | # Clean up previous build
      | cleanup() {
      |     echo "--- Cleaning up work directory: ${WORK_DIR_ABS} ---"
      |     if [ -d "${WORK_DIR_ABS}" ]; then
      |         if [[ "${WORK_DIR_ABS}" == *"/iso/work" ]]; then # Safety check path
      |             sudo rm -rf "${WORK_DIR_ABS}"
      |             echo "Work directory cleaned."
      |         else
      |             echo "Error: WORK_DIR_ABS path '${WORK_DIR_ABS}' seems unsafe. Aborting cleanup." >&2; exit 1;
      |         fi
      |     fi
      | }
      | 
      | # Prepare the airootfs for customization
      | prepare_airootfs_customizations() {
      |     local airootfs_mnt_path="${1}" # Path to the mounted airootfs (e.g., ${WORK_DIR_ABS}/x86_64/airootfs)
      |     echo "--- Preparing airootfs customizations in ${airootfs_mnt_path} ---"
      | 
      |     # 1. Copy the llmbasedos application repository
      |     local app_target_path="${airootfs_mnt_path}/opt/${LLMBasedOS_APP_TARGET_DIR_NAME}"
      |     echo "Copying llmbasedos repository to ${app_target_path}..."
      |     mkdir -p "${app_target_path}"
      |     rsync -a --delete --checksum \
      |         --exclude ".git" --exclude ".github" --exclude "iso/work" --exclude "iso/out" \
      |         --exclude "*.pyc" --exclude "__pycache__" --exclude ".DS_Store" \
      |         --exclude "venv" --exclude ".venv" --exclude "docs/_build" \
      |         "${REPO_ROOT_ABS}/" "${app_target_path}/"
      | 
      |     # 2. Install Python dependencies (system-wide pip install)
      |     echo "Installing Python dependencies system-wide via pip..."
      |     PIP_REQ_FILES=(
      |         "${app_target_path}/gateway/requirements.txt"
      |         "${app_target_path}/servers/fs/requirements.txt"
      |         "${app_target_path}/servers/sync/requirements.txt"
      |         "${app_target_path}/servers/mail/requirements.txt"
      |         "${app_target_path}/servers/agent/requirements.txt"
      |         "${app_target_path}/shell/requirements.txt"
      |     )
      |     for req_file in "${PIP_REQ_FILES[@]}"; do
      |         if [ -f "${req_file}" ]; then
      |             echo "Installing from ${req_file}..."
      |             # Using --break-system-packages if pip version is new and system python
      |             # Or, better, ensure python-pip is from system and compatible.
      |             # Or, use a venv within /opt/llmbasedos_env and adjust ExecStart in services.
      |             # For simplicity in minimal OS, system-wide for now.
      |             sudo arch-chroot "${airootfs_mnt_path}" pip install --no-cache-dir -r "${req_file}"
      |         else
      |             echo "Warning: Requirements file not found: ${req_file}"
      |         fi
      |     done
      |     echo "Cleaning up pip cache inside chroot..."
      |     sudo arch-chroot "${airootfs_mnt_path}" rm -rf /root/.cache/pip # Pip cache for root user
      | 
      |     # 3. Copy systemd service units
      |     local systemd_units_source_dir_iso="${PROFILE_DIR_ABS}/systemd_units" # Units stored in iso/systemd_units/
      |     local systemd_units_target_dir_airootfs="${airootfs_mnt_path}/etc/systemd/system"
      |     echo "Copying systemd service units to ${systemd_units_target_dir_airootfs}..."
      |     mkdir -p "${systemd_units_target_dir_airootfs}"
      |     if [ -d "${systemd_units_source_dir_iso}" ]; then
      |         sudo cp -v "${systemd_units_source_dir_iso}/"*.service "${systemd_units_target_dir_airootfs}/"
      |         sudo chmod 644 "${systemd_units_target_dir_airootfs}/"*.service # Ensure correct permissions
      |     else
      |         echo "Warning: Systemd units source directory ${systemd_units_source_dir_iso} not found."
      |     fi
      | 
      |     # 4. Copy post-installation script
      |     local postinstall_script_target_path="${airootfs_mnt_path}/root/llmbasedos_postinstall.sh" # Place in root for installer
      |     echo "Copying postinstall.sh to ${postinstall_script_target_path}..."
      |     sudo cp "${PROFILE_DIR_ABS}/postinstall.sh" "${postinstall_script_target_path}"
      |     sudo chmod 755 "${postinstall_script_target_path}"
      | 
      |     # 5. Create dummy/default configuration files and directories
      |     echo "Creating default configuration files/directories..."
      |     sudo mkdir -p "${airootfs_mnt_path}/etc/llmbasedos"
      |     sudo mkdir -p "${airootfs_mnt_path}/etc/llmbasedos/workflows" # For agent
      |     # Dummy licence key (FREE tier)
      |     if [ ! -f "${airootfs_mnt_path}/etc/llmbasedos/lic.key" ]; then
      |         sudo tee "${airootfs_mnt_path}/etc/llmbasedos/lic.key" > /dev/null <<LIC_EOF
      | # Default FREE tier licence. Replace with a valid key.
      | tier: FREE
      | user_id: default_live_user
      | expires_at: "$(date -d "+1 year" +%Y-%m-%dT%H:%M:%SZ)" # Example expiry
      | LIC_EOF
      |         sudo chmod 640 "${airootfs_mnt_path}/etc/llmbasedos/lic.key" # Readable by llmgroup
      |     fi
      |     # Dummy licence tiers config (gateway will use defaults if this is missing/empty)
      |     if [ ! -f "${airootfs_mnt_path}/etc/llmbasedos/licence_tiers.yaml" ]; then
      |         sudo tee "${airootfs_mnt_path}/etc/llmbasedos/licence_tiers.yaml" > /dev/null <<TIERS_EOF
      | # Empty: Gateway will use its internal DEFAULT_LICENCE_TIERS.
      | # Add custom tier definitions here if needed.
      | TIERS_EOF
      |         sudo chmod 644 "${airootfs_mnt_path}/etc/llmbasedos/licence_tiers.yaml"
      |     fi
      |     # Dummy mail accounts config
      |     if [ ! -f "${airootfs_mnt_path}/etc/llmbasedos/mail_accounts.yaml" ]; then
      |         sudo tee "${airootfs_mnt_path}/etc/llmbasedos/mail_accounts.yaml" > /dev/null <<MAIL_EOF
      | # Example mail account configuration (replace with real details)
      | # my_gmail_account:
      | #   email: "myemail@gmail.com"
      | #   host: "imap.gmail.com"
      | #   port: 993
      | #   user: "myemail@gmail.com"
      | #   password: "YOUR_APP_PASSWORD_HERE" # Use App Password for Gmail
      | #   ssl: true
      | MAIL_EOF
      |         sudo chmod 640 "${airootfs_mnt_path}/etc/llmbasedos/mail_accounts.yaml" # Readable by llmgroup
      |     fi
      | 
      |     # 6. Set hostname for the live ISO environment
      |     echo "Setting hostname for live system..."
      |     sudo tee "${airootfs_mnt_path}/etc/hostname" > /dev/null <<< "llmbasedos-live"
      | 
      |     # 7. Create user/group and enable services for the LIVE ISO environment
      |     # This is separate from postinstall.sh which is for the INSTALLED system.
      |     echo "Configuring live environment user and services..."
      |     # Create user llmuser and group llmgroup inside chroot
      |     sudo arch-chroot "${airootfs_mnt_path}" groupadd llmgroup --gid 1001 || echo "Group llmgroup may already exist."
      |     sudo arch-chroot "${airootfs_mnt_path}" useradd llmuser --uid 1001 --gid llmgroup -m -s /bin/bash -c "LLMBasedOS Service User" --groups wheel,tty,docker || echo "User llmuser may already exist."
      |     # Set a default password for live user (optional, can also use autologin)
      |     echo 'llmuser:livepass' | sudo arch-chroot "${airootfs_mnt_path}" chpasswd
      |     echo "Created live user 'llmuser' (pass: livepass) and group 'llmgroup'."
      |     # Add llmuser to docker group if docker package was installed
      |     if sudo arch-chroot "${airootfs_mnt_path}" pacman -Q docker &>/dev/null; then
      |       sudo arch-chroot "${airootfs_mnt_path}" usermod -aG docker llmuser
      |       echo "Added llmuser to docker group for live environment."
      |     fi
      | 
      | 
      |     # Enable services for multi-user.target in live environment
      |     # This creates symlinks in /etc/systemd/system/multi-user.target.wants/
      |     LIVE_SERVICES_TO_ENABLE=(
      |         "NetworkManager.service" # Essential for network
      |         "sshd.service"           # Optional for remote access
      |         "docker.service"         # If docker is used by agent server
      |         "mcp-gateway.service"
      |         "mcp-fs.service"
      |         "mcp-sync.service"
      |         "mcp-mail.service"
      |         "mcp-agent.service"
      |     )
      |     for service_live in "${LIVE_SERVICES_TO_ENABLE[@]}"; do
      |         if [ -f "${airootfs_mnt_path}/etc/systemd/system/${service_live}" ] || \
      |            [ -f "${airootfs_mnt_path}/usr/lib/systemd/system/${service_live}" ]; then
      |             echo "Enabling ${service_live} for live environment..."
      |             sudo arch-chroot "${airootfs_mnt_path}" systemctl enable "${service_live}"
      |         else
      |             echo "Warning: Service ${service_live} not found, cannot enable for live env."
      |         fi
      |     done
      | 
      |     # Configure TTY1 for luca-shell in live environment (autologin as llmuser)
      |     # This uses a drop-in for getty@tty1 to autologin llmuser,
      |     # then llmuser's .bash_profile/.zprofile should start luca-shell.
      |     echo "Configuring TTY1 for autologin and luca-shell in live environment..."
      |     local getty_autologin_dir="${airootfs_mnt_path}/etc/systemd/system/getty@tty1.service.d"
      |     sudo mkdir -p "${getty_autologin_dir}"
      |     sudo tee "${getty_autologin_dir}/autologin.conf" > /dev/null << AUTOLOGIN_EOF
      | [Service]
      | ExecStart=
      | ExecStart=-/sbin/agetty --autologin llmuser --noclear %I \$TERM
      | AUTOLOGIN_EOF
      |     # Add luca-shell startup to llmuser's .bash_profile
      |     local llmuser_bash_profile="${airootfs_mnt_path}/home/llmuser/.bash_profile"
      |     sudo tee -a "${llmuser_bash_profile}" > /dev/null << PROFILE_EOF
      | 
      | # Start luca-shell if on TTY1 and no X server
      | if [ "\$(tty)" = "/dev/tty1" ] && [ -z "\$DISPLAY" ]; then
      |     echo "Starting luca-shell on TTY1..."
      |     # Ensure PATH includes /usr/local/bin if python is there, or use /usr/bin/python
      |     # Make sure PYTHONPATH is set if llmbasedos is not installed in standard site-packages
      |     export PYTHONPATH=/opt # Crucial for finding llmbasedos package
      |     exec /usr/bin/python -m llmbasedos.shell.luca
      | fi
      | PROFILE_EOF
      |     sudo chown 1001:1001 "${llmuser_bash_profile}" # llmuser:llmgroup
      |     sudo chmod 644 "${llmuser_bash_profile}"
      |     
      |     # Ensure /run/mcp and log/lib dirs exist with correct perms for live boot
      |     sudo mkdir -p "${airootfs_mnt_path}/run/mcp"
      |     sudo chown 1001:1001 "${airootfs_mnt_path}/run/mcp" # llmuser:llmgroup
      |     sudo chmod 1770 "${airootfs_mnt_path}/run/mcp" # Sticky, rwxrwx---
      | 
      |     sudo mkdir -p "${airootfs_mnt_path}/var/log/llmbasedos"
      |     sudo chown 1001:1001 "${airootfs_mnt_path}/var/log/llmbasedos"
      |     sudo chmod 770 "${airootfs_mnt_path}/var/log/llmbasedos"
      | 
      |     sudo mkdir -p "${airootfs_mnt_path}/var/lib/llmbasedos"
      |     sudo chown 1001:1001 "${airootfs_mnt_path}/var/lib/llmbasedos"
      |     sudo chmod 770 "${airootfs_mnt_path}/var/lib/llmbasedos"
      | 
      | 
      |     echo "Airootfs customizations complete."
      | }
      | 
      | # --- Main Build Process ---
      | trap cleanup EXIT ERR INT TERM # Cleanup on exit or error
      | 
      | # Check for root/sudo privileges (mkarchiso needs it)
      | if [ "$(id -u)" -ne 0 ]; then
      |     echo "This script uses 'sudo' for mkarchiso and chroot. Ensure sudo is configured."
      |     # No exit needed, sudo will prompt if required.
      | fi
      | 
      | # Ensure mkarchiso is installed
      | if ! command -v mkarchiso &> /dev/null; then
      |     echo "mkarchiso command not found. Please install 'archiso' package." >&2; exit 1;
      | fi
      | 
      | echo "--- Starting llmbasedos ISO build process ---"
      | # 1. Initial cleanup and directory creation
      | cleanup
      | mkdir -p "${WORK_DIR_ABS}" "${OUT_DIR_ABS}"
      | 
      | # 2. Prepare profile directory for mkarchiso (copy essential configs)
      | #    profiledef.sh is already in ${PROFILE_DIR_ABS}
      | #    Copy pacman.conf and mirrorlist to where mkarchiso expects them for the profile.
      | echo "Copying pacman.conf and mirrorlist to profile directory..."
      | cp "${PROFILE_DIR_ABS}/pacman.conf" "${PROFILE_DIR_ABS}/pacman.conf" # It's already there, just ensures it's named correctly
      | cp "${PROFILE_DIR_ABS}/mirrorlist-llmbasedos-build" "${PROFILE_DIR_ABS}/pacman.d/mirrorlist" # mkarchiso looks in pacman.d/ relative to profile dir
      |                                                                                             # Or, /etc/pacman.d/mirrorlist inside chroot if pacman.conf uses that.
      |                                                                                             # The profiledef.sh pacman_conf points to ${PWD}/pacman.conf.
      |                                                                                             # This pacman.conf includes /etc/pacman.d/mirrorlist-llmbasedos-build
      |                                                                                             # So, we need to place it there inside the chroot structure later.
      |                                                                                             # Or, simpler: make profiledef.sh's pacman.conf use a path relative to profile dir.
      |                                                                                             # The `${PWD}` in profiledef.sh refers to the workdir when mkarchiso runs.
      |                                                                                             # Let's ensure it's copied to workdir too.
      | # Simpler way for mirrorlist with `pacman_conf` in `profiledef.sh`:
      | # The `pacman.conf` in `profiledef.sh` should point to a mirrorlist file that will exist *inside the chroot*.
      | # So, copy our build mirrorlist to the location inside airootfs that pacman.conf will reference.
      | # We'll do this during airootfs customization.
      | 
      | # 3. Build the ISO using mkarchiso.
      | #    mkarchiso will:
      | #    a. Create a base chroot environment using pacstrap (using pkg_list from profiledef.sh).
      | #    b. Execute customization hooks if defined (we do it manually after base chroot is made).
      | #    c. Package everything into an ISO.
      | 
      | # Using mkarchiso's `-C <config_dir_override_for_airootfs_copy_commands>` could be an option
      | # or `-r <run_script_in_chroot_after_pacstrap>` for some customizations.
      | # Standard flow:
      | #  mkarchiso -w work -o out -v prepare ... (does pacstrap, makes airootfs base)
      | #  (manual customization of work/arch/airootfs)
      | #  mkarchiso -w work -o out -v build ... (takes the customized airootfs and makes ISO)
      | # Or, just `mkarchiso -w work -o out -v profile_dir` and it does all if hooks are set up.
      | # Let's use a two-step approach for clarity: prepare, customize, then build.
      | 
      | echo "Running mkarchiso prepare step (pacstrap)..."
      | # This command structure might vary based on exact mkarchiso version and desired control.
      | # The `-p` option is not standard for "prepare only".
      | # A common way is to use a minimal profile and then customize.
      | # For full control, use arch-install-scripts' pacstrap directly if needed,
      | # or structure `profiledef.sh` with `init_airootfs` and `customize_airootfs` functions.
      | 
      | # Let's assume `mkarchiso ... <profile_dir>` does a full build, and we need to hook customizations.
      | # Archiso's `customize_airootfs.sh` script (if placed in profile dir) is run after pacstrap.
      | # So, let's put our `prepare_airootfs_customizations` logic into such a script.
      | 
      | # Create customize_airootfs.sh that mkarchiso will run
      | cat > "${PROFILE_DIR_ABS}/customize_airootfs.sh" << CUSTOMIZE_SCRIPT_EOF
      | #!/usr/bin/env bash
      | set -eo pipefail
      | echo "--- Running customize_airootfs.sh from mkarchiso hook ---"
      | 
      | # The script needs access to functions and variables from build.sh
      | # This is tricky. Let's redefine or source.
      | # For simplicity, redefine the core logic here or make build.sh functions exportable.
      | # Easier: Call prepare_airootfs_customizations directly with '/' as airootfs_mnt_path,
      | # because this script runs *inside* the chroot.
      | 
      | # Path to the app code, now relative to chroot's /
      | APP_SOURCE_ON_HOST_FOR_COPY="/ PaisibleAI_Outside_Chroot_Repo_Path_Placeholder /llmbasedos" # This path needs to be accessible from chroot or copied earlier by mkarchiso's generic copy
      |                                                               # mkarchiso copies contents of profile dir/airootfs/ into chroot /
      |                                                               # So, if llmbasedos repo is in profile_dir/airootfs_overlay/opt/llmbasedos, it's copied to /opt/llmbasedos
      |                                                               # Let's assume mkarchiso's copy mechanism handles this via airootfs directory in profile.
      | 
      | # We need to make `build.sh` place the repo into `iso/airootfs/opt/llmbasedos`
      | # so mkarchiso copies it into the chroot's `/opt/llmbasedos`.
      | # Then this script can work on `/opt/llmbasedos`.
      | 
      | # Call the customization function (ensure it's defined or sourced if in separate file)
      | # For this script, the function is not available. We need to inline its logic or source.
      | # Let's inline the core logic that needs to run *inside* the chroot.
      | 
      | echo "Customizing airootfs from within chroot..."
      | 
      | # Paths are now relative to chroot's /
      | CHROOT_APP_DIR="/opt/${LLMBasedOS_APP_TARGET_DIR_NAME}" # Should match build.sh's var
      | 
      | echo "Installing Python dependencies system-wide via pip (from chroot)..."
      | PIP_REQ_FILES_CHROOT=(
      |     "\${CHROOT_APP_DIR}/gateway/requirements.txt"
      |     "\${CHROOT_APP_DIR}/servers/fs/requirements.txt"
      |     "\${CHROOT_APP_DIR}/servers/sync/requirements.txt"
      |     "\${CHROOT_APP_DIR}/servers/mail/requirements.txt"
      |     "\${CHROOT_APP_DIR}/servers/agent/requirements.txt"
      |     "\${CHROOT_APP_DIR}/shell/requirements.txt"
      | )
      | for req_file_chroot in "\${PIP_REQ_FILES_CHROOT[@]}"; do
      |     if [ -f "\${req_file_chroot}" ]; then
      |         echo "Installing from \${req_file_chroot}..."
      |         pip install --no-cache-dir -r "\${req_file_chroot}"
      |     else
      |         echo "Warning (chroot): Requirements file not found: \${req_file_chroot}"
      |     fi
      | done
      | echo "Cleaning up pip cache (from chroot)..."
      | rm -rf /root/.cache/pip
      | 
      | echo "Copying systemd service units (from chroot perspective)..."
      | # Units are already copied to /etc/systemd/system by mkarchiso if they were in profile_dir/airootfs/etc/systemd/system
      | # Or, if they are in profile_dir/systemd_units, we copy them here if this script is run by -r option
      | # Assuming units are in CHROOT_APP_DIR/iso/systemd_units and need to be copied/linked
      | SYSTEMD_UNITS_SOURCE_IN_CHROOT="\${CHROOT_APP_DIR}/iso/systemd_units"
      | SYSTEMD_TARGET_DIR="/etc/systemd/system"
      | if [ -d "\${SYSTEMD_UNITS_SOURCE_IN_CHROOT}" ]; then
      |     cp -v "\${SYSTEMD_UNITS_SOURCE_IN_CHROOT}/"*.service "\${SYSTEMD_TARGET_DIR}/"
      |     chmod 644 "\${SYSTEMD_TARGET_DIR}/"*.service
      | else
      |     echo "Warning (chroot): Systemd units source \${SYSTEMD_UNITS_SOURCE_IN_CHROOT} not found."
      | fi
      | 
      | echo "Creating default configuration files/directories (from chroot)..."
      | mkdir -p /etc/llmbasedos/workflows
      | # Dummy licence key
      | if [ ! -f "/etc/llmbasedos/lic.key" ]; then
      |     tee "/etc/llmbasedos/lic.key" > /dev/null <<LIC_EOF_CHROOT
      | tier: FREE
      | user_id: default_chroot_user
      | expires_at: "$(date -d "+1 year" +%Y-%m-%dT%H:%M:%SZ)"
      | LIC_EOF_CHROOT
      |     chmod 640 "/etc/llmbasedos/lic.key"
      | fi
      | # Other dummy configs (tiers, mail) similar to prepare_airootfs_customizations
      | 
      | echo "Setting hostname (from chroot)..."
      | tee "/etc/hostname" > /dev/null <<< "llmbasedos-live"
      | 
      | echo "Configuring live environment user and services (from chroot)..."
      | groupadd llmgroup --gid 1001 || echo "Group llmgroup may already exist."
      | useradd llmuser --uid 1001 --gid llmgroup -m -s /bin/bash -c "LLMBasedOS Service User" --groups wheel,tty,docker || echo "User llmuser may already exist."
      | echo 'llmuser:livepass' | chpasswd
      | echo "Created live user 'llmuser' (pass: livepass)."
      | if pacman -Q docker &>/dev/null; then usermod -aG docker llmuser; fi
      | 
      | LIVE_SERVICES_TO_ENABLE_CHROOT=(
      |     "NetworkManager.service" "sshd.service" "docker.service"
      |     "mcp-gateway.service" "mcp-fs.service" "mcp-sync.service"
      |     "mcp-mail.service" "mcp-agent.service"
      | )
      | for service_chroot in "\${LIVE_SERVICES_TO_ENABLE_CHROOT[@]}"; do
      |     if [ -f "/etc/systemd/system/\${service_chroot}" ] || [ -f "/usr/lib/systemd/system/\${service_chroot}" ]; then
      |         echo "Enabling \${service_chroot} for live env (chroot)..."
      |         systemctl enable "\${service_chroot}"
      |     else echo "Warning (chroot): Service \${service_chroot} not found."; fi
      | done
      | 
      | echo "Configuring TTY1 autologin for llmuser and luca-shell (from chroot)..."
      | GETTY_AUTOLOGIN_DIR="/etc/systemd/system/getty@tty1.service.d"
      | mkdir -p "\${GETTY_AUTOLOGIN_DIR}"
      | tee "\${GETTY_AUTOLOGIN_DIR}/autologin.conf" > /dev/null << AUTOLOGIN_EOF_CHROOT
      | [Service]
      | ExecStart=
      | ExecStart=-/sbin/agetty --autologin llmuser --noclear %I \$TERM
      | AUTOLOGIN_EOF_CHROOT
      | 
      | LLMUSER_BASH_PROFILE="/home/llmuser/.bash_profile"
      | tee -a "\${LLMUSER_BASH_PROFILE}" > /dev/null << PROFILE_EOF_CHROOT
      | if [ "\\\$(tty)" = "/dev/tty1" ] && [ -z "\\\$DISPLAY" ]; then
      |     echo "Starting luca-shell on TTY1 (from .bash_profile)..."
      |     export PYTHONPATH=/opt # Crucial
      |     exec /usr/bin/python -m llmbasedos.shell.luca
      | fi
      | PROFILE_EOF_CHROOT
      | chown 1001:1001 "\${LLMUSER_BASH_PROFILE}"; chmod 644 "\${LLMUSER_BASH_PROFILE}"
      | 
      | # Ensure /run/mcp and log/lib dirs exist with correct perms for live boot
      | mkdir -p /run/mcp; chown 1001:1001 /run/mcp; chmod 1770 /run/mcp
      | mkdir -p /var/log/llmbasedos; chown 1001:1001 /var/log/llmbasedos; chmod 770 /var/log/llmbasedos
      | mkdir -p /var/lib/llmbasedos; chown 1001:1001 /var/lib/llmbasedos; chmod 770 /var/lib/llmbasedos
      | 
      | # Copy postinstall script to /root for installer access
      | cp "\${CHROOT_APP_DIR}/iso/postinstall.sh" "/root/llmbasedos_postinstall.sh"
      | chmod 755 "/root/llmbasedos_postinstall.sh"
      | 
      | echo "--- customize_airootfs.sh COMPLETED ---"
      | CUSTOMIZE_SCRIPT_EOF
      | chmod +x "${PROFILE_DIR_ABS}/customize_airootfs.sh"
      | 
      | # Now, prepare the airootfs overlay that mkarchiso will use.
      | # mkarchiso copies contents of `profile_dir/airootfs/` into the chroot's `/`.
      | AIROOTFS_OVERLAY_DIR="${PROFILE_DIR_ABS}/airootfs"
      | mkdir -p "${AIROOTFS_OVERLAY_DIR}/opt/${LLMBasedOS_APP_TARGET_DIR_NAME}"
      | mkdir -p "${AIROOTFS_OVERLAY_DIR}/etc/pacman.d" # For mirrorlist
      | 
      | echo "Copying application code to airootfs overlay for mkarchiso..."
      | rsync -a --delete --checksum \
      |     --exclude ".git" --exclude ".github" --exclude "iso/work" --exclude "iso/out" \
      |     --exclude "*.pyc" --exclude "__pycache__" --exclude ".DS_Store" \
      |     --exclude "venv" --exclude ".venv" --exclude "docs/_build" \
      |     "${REPO_ROOT_ABS}/" "${AIROOTFS_OVERLAY_DIR}/opt/${LLMBasedOS_APP_TARGET_DIR_NAME}/"
      | 
      | echo "Copying build-time mirrorlist for use inside chroot by pacman..."
      | cp "${PROFILE_DIR_ABS}/mirrorlist-llmbasedos-build" "${AIROOTFS_OVERLAY_DIR}/etc/pacman.d/mirrorlist-llmbasedos-build"
      | 
      | 
      | # Finally, run mkarchiso. It will use profiledef.sh and execute customize_airootfs.sh.
      | echo "Building the ISO image with mkarchiso..."
      | # The profile directory is PROFILE_DIR_ABS
      | sudo mkarchiso ${MKARCHISO_OPTS} -w "${WORK_DIR_ABS}" -o "${OUT_DIR_ABS}" "${PROFILE_DIR_ABS}"
      | 
      | FINAL_ISO_PATH_FOUND=$(find "${OUT_DIR_ABS}" -name "llmbasedos*.iso" -print -quit)
      | 
      | if [ -f "${FINAL_ISO_PATH_FOUND}" ]; then
      |     echo "--- ISO Build Successful! ---"
      |     echo "ISO created at: ${FINAL_ISO_PATH_FOUND}"
      |     ls -lh "${FINAL_ISO_PATH_FOUND}"
      | else
      |     echo "--- ISO Build Failed. Check logs in ${WORK_DIR_ABS}. ---" >&2; exit 1;
      | fi
      | 
      | # Cleanup customize_airootfs.sh and airootfs overlay dir from profile after build
      | rm -f "${PROFILE_DIR_ABS}/customize_airootfs.sh"
      | # sudo rm -rf "${AIROOTFS_OVERLAY_DIR}" # Optional: clean up overlay
      | 
      | exit 0
      --- Fin Contenu ---
    Fichier: mirrorlist-llmbasedos-build
    Fichier: pacman.conf
      --- Début Contenu (ascii) ---
      | # llmbasedos/iso/pacman.conf
      | # Minimal pacman.conf for ISO build. Should point to valid mirrors.
      | # Usually, you copy /etc/pacman.conf and /etc/pacman.d/mirrorlist from host.
      | 
      | [options]
      | HoldPkg     = pacman glibc
      | Architecture = auto
      | SigLevel    = Required DatabaseOptional
      | LocalFileSigLevel = Optional
      | 
      | [core]
      | Include = /etc/pacman.d/mirrorlist-llmbasedos-build # Use a build-specific mirrorlist
      | 
      | [extra]
      | Include = /etc/pacman.d/mirrorlist-llmbasedos-build
      | 
      | # [community] # Merged into extra in Arch
      | # Include = /etc/pacman.d/mirrorlist-llmbasedos-build
      | 
      | # Consider adding [multilib] if any 32-bit compatibility is needed (unlikely for this project)
      --- Fin Contenu ---
    Fichier: postinstall.sh
      --- Début Contenu (ascii) ---
      | #!/usr/bin/env bash
      | set -eo pipefail
      | 
      | # llmbasedos/iso/postinstall.sh
      | # Run on the *target system* after base Arch install to configure llmbasedos.
      | 
      | echo "--- Starting LLMBasedOS Post-Installation Configuration ---"
      | 
      | LLMBasedOS_APP_DIR_INSTALLED="/opt/llmbasedos" # Where app code resides on installed system
      | USERNAME_INSTALLED="llmuser"
      | USERHOME_INSTALLED="/home/${USERNAME_INSTALLED}"
      | MCP_GROUP_INSTALLED="llmgroup" # Dedicated group for MCP services and resources
      | 
      | # 1. Create User and Group if they don't exist
      | if ! getent group "${MCP_GROUP_INSTALLED}" > /dev/null; then
      |     echo "Creating group ${MCP_GROUP_INSTALLED}..."
      |     groupadd --system "${MCP_GROUP_INSTALLED}" # System group
      | else echo "Group ${MCP_GROUP_INSTALLED} already exists."; fi
      | 
      | if ! id "${USERNAME_INSTALLED}" > /dev/null; then
      |     echo "Creating user ${USERNAME_INSTALLED}..."
      |     useradd --system -g "${MCP_GROUP_INSTALLED}" -d "${USERHOME_INSTALLED}" -m \
      |             -s /usr/sbin/nologin -c "LLMBasedOS Service User" "${USERNAME_INSTALLED}"
      |     # For shell access, change -s /usr/sbin/nologin to /bin/bash and set a password.
      |     # If luca-shell runs as this user on TTY, shell should be /bin/bash.
      |     # Let's assume llmuser needs a login shell for luca-shell.
      |     usermod -s /bin/bash "${USERNAME_INSTALLED}"
      |     echo "Setting default password for ${USERNAME_INSTALLED}. User MUST change this."
      |     echo "${USERNAME_INSTALLED}:changeme" | chpasswd # Highly insecure, for demo only!
      |     echo "User ${USERNAME_INSTALLED} created. Login shell: /bin/bash."
      | else
      |     echo "User ${USERNAME_INSTALLED} already exists. Ensuring group membership."
      |     usermod -aG "${MCP_GROUP_INSTALLED}" "${USERNAME_INSTALLED}"
      | fi
      | # Add to tty group for TTY access by luca-shell, and docker group for agent server
      | usermod -aG tty "${USERNAME_INSTALLED}"
      | if pacman -Q docker &>/dev/null; then # If docker is installed
      |     usermod -aG docker "${USERNAME_INSTALLED}"
      |     echo "Added ${USERNAME_INSTALLED} to docker group. Re-login required for effect."
      | fi
      | 
      | 
      | # 2. Permissions for application and configuration directories
      | echo "Setting permissions for application and config directories..."
      | if [ -d "${LLMBasedOS_APP_DIR_INSTALLED}" ]; then
      |     chown -R "${USERNAME_INSTALLED}:${MCP_GROUP_INSTALLED}" "${LLMBasedOS_APP_DIR_INSTALLED}"
      |     find "${LLMBasedOS_APP_DIR_INSTALLED}" -type d -exec chmod 750 {} \; # u=rwx, g=rx, o=
      |     find "${LLMBasedOS_APP_DIR_INSTALLED}" -type f -exec chmod 640 {} \; # u=rw, g=r, o=
      |     find "${LLMBasedOS_APP_DIR_INSTALLED}" -type f -name "*.sh" -exec chmod 750 {} \; # Executable scripts
      | else echo "Warning: ${LLMBasedOS_APP_DIR_INSTALLED} not found. Skipping permissions."; fi
      | 
      | mkdir -p /etc/llmbasedos/workflows
      | chown -R "${USERNAME_INSTALLED}:${MCP_GROUP_INSTALLED}" /etc/llmbasedos
      | chmod 770 /etc/llmbasedos # Dir: u=rwx, g=rwx (for config file updates by services if needed)
      | chmod 750 /etc/llmbasedos/workflows # u=rwx, g=rx
      | # Config files within /etc/llmbasedos should be u=rw, g=r (640)
      | find /etc/llmbasedos -type f -exec chmod 640 {} \;
      | # Ensure lic.key has strict permissions if it contains sensitive data
      | if [ -f "/etc/llmbasedos/lic.key" ]; then chmod 640 "/etc/llmbasedos/lic.key"; fi
      | 
      | 
      | # 3. /run/mcp for sockets
      | MCP_RUN_DIR_INSTALLED="/run/mcp" # systemd's RuntimeDirectory= in service files is better
      | # If not using RuntimeDirectory=, create and set perms here.
      | # mkdir -p "${MCP_RUN_DIR_INSTALLED}"
      | # chown "${USERNAME_INSTALLED}:${MCP_GROUP_INSTALLED}" "${MCP_RUN_DIR_INSTALLED}"
      | # chmod 2770 "${MCP_RUN_DIR_INSTALLED}" # rwxrws--- (setgid bit so sockets inherit group)
      | # For systemd >v247, RuntimeDirectoryDefaultMode=0755, and RuntimeDirectoryPreserve=
      | # Sockets created by services running as llmuser:llmgroup will have that ownership.
      | # Socket permissions (0660) are set by MCPServer.start().
      | # Ensure parent /run/mcp is writable by llmuser or llmgroup if RuntimeDirectory not used.
      | # Best: define RuntimeDirectory=mcp in service files, systemd handles creation & perms.
      | # Let's assume service files will handle /run/mcp creation.
      | 
      | # 4. Log and lib directories
      | LOG_DIR_INSTALLED="/var/log/llmbasedos"
      | LIB_DIR_INSTALLED="/var/lib/llmbasedos" # For FAISS index, etc.
      | mkdir -p "${LOG_DIR_INSTALLED}" "${LIB_DIR_INSTALLED}"
      | chown -R "${USERNAME_INSTALLED}:${MCP_GROUP_INSTALLED}" "${LOG_DIR_INSTALLED}" "${LIB_DIR_INSTALLED}"
      | chmod -R 770 "${LOG_DIR_INSTALLED}" # u=rwx, g=rwx for log writing
      | chmod -R 770 "${LIB_DIR_INSTALLED}" # u=rwx, g=rwx for lib data (FAISS index)
      | 
      | # 5. Enable Systemd Services
      | echo "Enabling and starting systemd services for llmbasedos..."
      | systemctl daemon-reload # Refresh systemd manager configuration
      | 
      | SERVICES_TO_ENABLE_INSTALLED=(
      |     "mcp-gateway.service" "mcp-fs.service" "mcp-sync.service"
      |     "mcp-mail.service" "mcp-agent.service"
      | )
      | # Also enable NetworkManager, sshd, docker if they were chosen during install
      | if pacman -Q networkmanager &>/dev/null; then systemctl enable NetworkManager.service; fi
      | if pacman -Q openssh &>/dev/null; then systemctl enable sshd.service; fi
      | if pacman -Q docker &>/dev/null; then systemctl enable docker.service; fi
      | 
      | 
      | for service_inst in "\${SERVICES_TO_ENABLE_INSTALLED[@]}"; do
      |     if systemctl list-unit-files | grep -q "^\${service_inst}"; then
      |         echo "Enabling \${service_inst}..."
      |         systemctl enable "\${service_inst}"
      |         # Optionally start them now, or let reboot handle it
      |         # systemctl start "\${service_inst}" || echo "Warning: Failed to start \${service_inst} immediately."
      |     else echo "Warning: Service file \${service_inst} not found. Cannot enable."; fi
      | done
      | 
      | # 6. Configure TTY1 for luca-shell (for installed system)
      | echo "Configuring TTY1 for luca-shell..."
      | if systemctl list-unit-files | grep -q "^luca-shell@.service"; then
      |     if systemctl list-unit-files | grep -q "^getty@tty1.service"; then
      |         echo "Disabling getty@tty1.service..."
      |         systemctl disable getty@tty1.service # Standard getty on TTY1
      |         echo "Masking getty@tty1.service to prevent it from being started by getty.target..."
      |         systemctl mask getty@tty1.service # Prevent getty.target from pulling it in
      |     fi
      |     echo "Enabling luca-shell@tty1.service..."
      |     systemctl enable luca-shell@tty1.service # Our custom shell service for TTY1
      | else echo "Warning: luca-shell@.service not found. TTY1 setup skipped."; fi
      | 
      | # 7. Final messages
      | echo "--- LLMBasedOS Post-Installation Complete ---"
      | echo "User '${USERNAME_INSTALLED}' created/configured (password: changeme - CHANGE IMMEDIATELY!)."
      | echo "Services enabled. A reboot is recommended for all changes to take full effect."
      | echo "After reboot, luca-shell should be on TTY1, and MCP gateway accessible."
      | echo "Remember to configure API keys in /etc/llmbasedos/gateway.env (or similar) if needed."
      --- Fin Contenu ---
    Fichier: profiledef.sh
      --- Début Contenu (ascii) ---
      | #!/usr/bin/env bash
      | # shellcheck disable=SC2034
      | 
      | # llmbasedos/iso/profiledef.sh (Archiso profile definition)
      | 
      | iso_name="llmbasedos"
      | iso_label="LLMBasedOS_$(date +%Y%m)"
      | iso_publisher="llmbasedos Project <your_email_or_site>" # Customize
      | iso_application="LLMBasedOS Minimal Linux with MCP"
      | iso_version="$(date +%Y.%m.%d)"
      | install_dir="arch" 
      | buildmodes=('iso')
      | bootmodes=('bios.syslinux.mbr' 'bios.syslinux.eltorito'
      |            'uefi-x64.systemd-boot.esp' 'uefi-x64.systemd-boot.eltorito')
      | arch="x86_64"
      | pacman_conf="${PWD}/pacman.conf" # Use pacman.conf from this profile directory
      | airootfs_image_type="squashfs"
      | airootfs_image_tool_options=('-comp' 'xz' '-Xbcj' 'x86' '-b' '1M' '-noappend') # Standard compression
      | 
      | # File permissions (adjust as needed, especially for /etc/llmbasedos files)
      | file_permissions=(
      |   ["/etc/shadow"]="0:0:400"
      |   ["/root"]="0:0:750"
      |   ["/etc/sudoers.d"]="0:0:755"
      |   ["/opt/llmbasedos"]="0:0:755" # Base app dir
      |   ["/opt/llmbasedos/**/*.py"]="0:0:644"
      |   ["/opt/llmbasedos/**/*.sh"]="0:0:755"
      |   ["/etc/llmbasedos"]="0:0:750" # Root owned, llmuser/llmgroup readable (or more restrictive)
      |   ["/etc/llmbasedos/lic.key"]="0:42:640" # Example: root:shadow readable by shadow group (if sensitive)
      |                                        # Or root:llmgroup 640 if llmuser/llmgroup needs to read it
      |                                        # For now, assume gateway (llmuser) reads it, so 0:llmgroup 640
      |   ["/etc/llmbasedos/mail_accounts.yaml"]="0:llmgroup:640" # Readable by llmgroup
      |   ["/etc/llmbasedos/workflows"]="0:llmgroup:750" # Workflows dir
      |   ["/etc/llmbasedos/licence_tiers.yaml"]="0:0:644" # Globally readable for tiers info
      |   ["/etc/systemd/system/*"]="0:0:644"
      |   ["/run/mcp"]="llmuser:llmgroup:1770" # Sticky bit, owned by llmuser:llmgroup, rwxrwx--T
      |                                       # Services create their own sockets here.
      |   ["/var/log/llmbasedos"]="llmuser:llmgroup:770" # Writable by services
      |   ["/var/lib/llmbasedos"]="llmuser:llmgroup:770" # For FAISS index etc.
      | )
      | 
      | # System packages needed for llmbasedos and its dependencies
      | # Python dependencies will be installed via pip in build.sh's airootfs customization
      | pkg_list=(
      |     "base" "linux" "linux-firmware" "systemd" "systemd-sysvcompat" "mkinitcpio" "pacman"
      |     "sudo" "networkmanager" "openssh" "git" # Core utilities
      |     "python" "python-pip" # Python runtime and pip
      |     # System libraries needed by Python packages (examples, verify specific needs)
      |     "gcc" "make" "rust" # For compiling some Python packages if installed from source by pip
      |     "blas" "lapack" # For numpy/scipy often needed by ML libs
      |     "tk" # Sometimes needed by matplotlib, indirectly by sentence-transformers vis
      |     # llmbasedos direct system dependencies
      |     "rclone"                            # For sync server
      |     "docker"                            # For agent server (Docker workflows)
      |     "libmagic"                          # For python-magic (fs server)
      |     "faiss-cpu"                         # If FAISS is from system repo (ensure name matches)
      |     # Other useful tools for a minimal system
      |     "vim" "curl" "wget" "htop" "man-db" "man-pages" "less" "tree" "tmux"
      |     "archiso" # If building on the ISO itself, for dev
      | )
      | 
      | # Remove unnecessary files from ISO (customize heavily for minimal size)
      | rm_iso=(
      |     "usr/lib/systemd/system/multi-user.target.wants/{graphical.target,plymouth*,rescue*,emergency*}"
      |     "etc/systemd/system/*.wants/{plymouth*,rescue*,emergency*}"
      |     "var/log/journal/*" "var/cache/pacman/pkg/*"
      |     "usr/share/man/*" "!usr/share/man/man1" "!usr/share/man/man5" "!usr/share/man/man8" # Keep some man pages
      |     "usr/share/doc/*" "!usr/share/doc/llmbasedos*" # Keep our own docs if any
      |     "usr/share/locale/*" "!usr/share/locale/en_US" "!usr/share/locale/locale.alias" # Keep English only
      |     "usr/include" # Development headers (unless building on ISO)
      |     "usr/lib/python*/test" "usr/lib/python*/unittest" # Python test files
      |     "usr/lib/python*/site-packages/pip*" # Pip itself after installs (optional)
      |     "usr/lib/python*/site-packages/setuptools*" # Setuptools after installs (optional)
      |     "usr/lib/python*/site-packages/wheel*" # Wheel after installs (optional)
      |     "opt/llmbasedos/**/*.pyc" "opt/llmbasedos/**/__pycache__" # Clean Python bytecode
      | )
      --- Fin Contenu ---

    Répertoire: ./iso/systemd_units
      Fichier: luca-shell@.service
        --- Début Contenu (ascii) ---
        | # llmbasedos/iso/systemd_units/luca-shell@.service
        | [Unit]
        | Description=Luca Shell on %I
        | Documentation=man:agetty(8) man:systemd-getty-generator(8)
        | After=systemd-user-sessions.service plymouth-quit-wait.service systemd-logind.service mcp-gateway.service
        | Before=getty.target
        | IgnoreOnIsolate=yes
        | ConditionPathExists=/dev/%I
        | 
        | [Service]
        | User=llmuser
        | Group=tty # For TTY access, llmuser must be in tty group
        | WorkingDirectory=/home/llmuser # Shell starts in user's home
        | ExecStart=-/usr/bin/python -m llmbasedos.shell.luca
        | Type=idle
        | Restart=always # Keep shell running on TTY
        | RestartSec=1
        | UtmpIdentifier=%I
        | TTYPath=/dev/%I
        | TTYReset=yes
        | TTYVHangup=yes
        | TTYVTDisallocate=yes
        | KillMode=process-group # Kill shell and its children if service stops
        | IgnoreSIGPIPE=no
        | SendSIGHUP=yes
        | StandardInput=tty
        | StandardOutput=tty
        | # StandardError=journal # Errors from shell REPL can go to journal
        | StandardError=tty # Or directly to TTY for immediate visibility
        | Environment=PYTHONUNBUFFERED=1
        | Environment="PYTHONPATH=/opt" # So 'llmbasedos.shell.luca' is found
        | 
        | [Install]
        | WantedBy=getty.target # This allows 'systemctl enable luca-shell@tty1.service'
        --- Fin Contenu ---
      Fichier: mcp-agent.service
        --- Début Contenu (ascii) ---
        | # llmbasedos/iso/systemd_units/mcp-agent.service
        | [Unit]
        | Description=LLMBasedOS MCP Agent Server
        | After=network-online.target mcp-gateway.service docker.service
        | Wants=mcp-gateway.service docker.service
        | 
        | [Service]
        | Type=simple
        | User=llmuser
        | Group=llmgroup # Ensure llmuser is also in 'docker' group if using Docker socket directly
        | ExecStart=/usr/bin/python -m llmbasedos.servers.agent.server
        | WorkingDirectory=/opt/llmbasedos
        | Restart=on-failure
        | StandardOutput=journal
        | StandardError=journal
        | Environment=PYTHONUNBUFFERED=1
        | Environment="PYTHONPATH=/opt"
        | Environment="LLMBDO_AGENT_WORKFLOWS_DIR=/etc/llmbasedos/workflows"
        | # Ensure Docker socket is accessible or configure DOCKER_HOST if needed
        | 
        | [Install]
        | WantedBy=multi-user.target
        --- Fin Contenu ---
      Fichier: mcp-fs.service
        --- Début Contenu (ascii) ---
        | # llmbasedos/iso/systemd_units/mcp-fs.service
        | [Unit]
        | Description=LLMBasedOS MCP File System Server
        | After=network-online.target mcp-gateway.service
        | Wants=mcp-gateway.service
        | 
        | [Service]
        | Type=simple
        | User=llmuser
        | Group=llmgroup
        | ExecStart=/usr/bin/python -m llmbasedos.servers.fs.server
        | WorkingDirectory=/opt/llmbasedos # Python -m resolves from here if PYTHONPATH includes /opt
        | Restart=on-failure
        | StandardOutput=journal
        | StandardError=journal
        | Environment=PYTHONUNBUFFERED=1
        | Environment="PYTHONPATH=/opt"
        | # Environment="LLMBDO_FS_VIRTUAL_ROOT=/home/llmuser/shared_fs" # Example custom root for FS server
        | 
        | [Install]
        | WantedBy=multi-user.target
        --- Fin Contenu ---
      Fichier: mcp-gateway.service
        --- Début Contenu (ascii) ---
        | # llmbasedos/iso/systemd_units/mcp-gateway.service
        | [Unit]
        | Description=LLMBasedOS MCP Gateway
        | After=network-online.target
        | Wants=network-online.target
        | 
        | [Service]
        | Type=simple
        | User=llmuser
        | Group=llmgroup # Ensure this group exists and llmuser is a member
        | ExecStart=/usr/bin/python -m llmbasedos.gateway.main
        | WorkingDirectory=/opt/llmbasedos # All modules are relative to this if PYTHONPATH is set, or CWD is /opt/llmbasedos/gateway
        | Restart=on-failure
        | StandardOutput=journal
        | StandardError=journal
        | Environment=PYTHONUNBUFFERED=1
        | # Set PYTHONPATH so that 'llmbasedos' package is found from /opt
        | Environment="PYTHONPATH=/opt"
        | # EnvironmentFile=/etc/llmbasedos/gateway.env # For secrets like API keys
        | 
        | [Install]
        | WantedBy=multi-user.target
        --- Fin Contenu ---
      Fichier: mcp-mail.service
        --- Début Contenu (ascii) ---
        | # llmbasedos/iso/systemd_units/mcp-mail.service
        | [Unit]
        | Description=LLMBasedOS MCP Mail Server
        | After=network-online.target mcp-gateway.service
        | Wants=mcp-gateway.service
        | 
        | [Service]
        | Type=simple
        | User=llmuser
        | Group=llmgroup
        | ExecStart=/usr/bin/python -m llmbasedos.servers.mail.server
        | WorkingDirectory=/opt/llmbasedos
        | Restart=on-failure
        | StandardOutput=journal
        | StandardError=journal
        | Environment=PYTHONUNBUFFERED=1
        | Environment="PYTHONPATH=/opt"
        | Environment="LLMBDO_MAIL_ACCOUNTS_CONFIG=/etc/llmbasedos/mail_accounts.yaml"
        | 
        | [Install]
        | WantedBy=multi-user.target
        --- Fin Contenu ---
      Fichier: mcp-sync.service
        --- Début Contenu (ascii) ---
        | # llmbasedos/iso/systemd_units/mcp-sync.service
        | [Unit]
        | Description=LLMBasedOS MCP Sync Server
        | After=network-online.target mcp-gateway.service
        | Wants=mcp-gateway.service
        | 
        | [Service]
        | Type=simple
        | User=llmuser
        | Group=llmgroup
        | ExecStart=/usr/bin/python -m llmbasedos.servers.sync.server
        | WorkingDirectory=/opt/llmbasedos
        | Restart=on-failure
        | StandardOutput=journal
        | StandardError=journal
        | Environment=PYTHONUNBUFFERED=1
        | Environment="PYTHONPATH=/opt"
        | Environment="LLMBDO_RCLONE_CONFIG_PATH=/home/llmuser/.config/rclone/rclone.conf"
        | 
        | [Install]
        | WantedBy=multi-user.target
        --- Fin Contenu ---
